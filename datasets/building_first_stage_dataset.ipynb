{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import orjson\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MAX_QUESTION_LENGTH = 2000\n",
    "MAX_ANSWER_LENGTH = 500\n",
    "\n",
    "AVAILABLE_TAGS = [\n",
    "    \"multiple_choice\",\n",
    "    \"context_based\",\n",
    "    \"knowledge_based\",\n",
    "    \"human_eval\",\n",
    "    \"code\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_example(example):\n",
    "    example = example.copy()\n",
    "    limit_fields = ['input', 'context', 'answers', 'final_answer']\n",
    "    for field in limit_fields:\n",
    "        if field in example:\n",
    "            if isinstance(example[field], str):\n",
    "                if len(example[field]) > 200:\n",
    "                    example[field] = example[field][:200] + '...' + example[field][-50:]\n",
    "            elif isinstance(example[field], list):\n",
    "                # Process each answer in the list\n",
    "                limited_answers = []\n",
    "                for answer in example[field]:\n",
    "                    if isinstance(answer, str) and len(answer) > 200:\n",
    "                        limited_answers.append(answer[:200] + '...' + answer[-50:])\n",
    "                    else:\n",
    "                        limited_answers.append(answer)\n",
    "                example[field] = limited_answers\n",
    "    \n",
    "    # Prepare the output string\n",
    "    output = \"{\\n\"\n",
    "    for key, value in example.items():\n",
    "        # Format the value for display\n",
    "        if isinstance(value, str):\n",
    "            formatted_value = f'\"{value}\"'\n",
    "        elif isinstance(value, list):\n",
    "            formatted_value = '[' + ', '.join([f'\"{v}\"' if isinstance(v, str) else str(v) for v in value]) + ']'\n",
    "        else:\n",
    "            formatted_value = str(value)\n",
    "        \n",
    "        output += f\"\\t{key}: {formatted_value},\\n\"\n",
    "    \n",
    "    output = output.rstrip(\",\\n\") + \"\\n}\"\n",
    "    \n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential sources\n",
    "### Candidates\n",
    "\n",
    "> HellaSwag (?)\n",
    "> PIQA ++\n",
    "> TruthfulQA ++\n",
    "> Mt-bench ++\n",
    "> MMLU (?)\n",
    "\n",
    "---\n",
    "\n",
    "Language Modeling\n",
    "- CHID\n",
    "- Winogrande\n",
    "\n",
    "Understanding\n",
    "- RACE\n",
    "- LAMBADA\n",
    "\n",
    "Reasoning\n",
    "- SIQA\n",
    "- PIQA +! \n",
    "\n",
    "In-Context Learning\n",
    "- MMLU\n",
    "- CEval\n",
    "\n",
    "Multi-Step Reasoning\n",
    "- GSM8K\n",
    "- StrategyQA\n",
    "\n",
    "Instruction-Following\n",
    "- Hellaswag\n",
    "- ARC\n",
    "\n",
    "Self-Calibration\n",
    "- MMLU\n",
    "\n",
    "Hallucination\n",
    "- TruthfulQA +! \n",
    "\n",
    "Robustness\n",
    "- AdvGLUE\n",
    "\n",
    "Dialogue\n",
    "- Mt-bench +! \n",
    "\n",
    "Long-Context\n",
    "- Longeval\n",
    "- Multi-Doc QA\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qa Multi Hop qa [Knowledge-based] [Difficult]\n",
    "https://huggingface.co/datasets/pszemraj/qmsum-cleaned/viewer/default/\n",
    "\n",
    "This dataset has questions that may require multistep thinking or testing various knowledge.\n",
    "The answer can be either small or difficult. When it requires multistep summary + answer will be supported.\n",
    "Obviously I'll keep only the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------======== DATASET: khaimaitien/qa-expert-multi-hop-qa-V1.0 ========--------\n",
      "\n",
      "=== Example 1 ===\n",
      "{\n",
      "\tsub_questions: [{'answer': 'Egypt', 'extra_info': None, 'long_answer': 'The knowledge mentions Gamal Abdel Nasser as one of the leaders that Josip Broz Tito met during his rule.\\nAnswer: Based on the provided knowledge, Nasser was a leader of Egypt.', 'paragraph': \"Josip Broz Tito. Yugoslavia had a liberal travel policy permitting foreigners to freely travel through the country and its citizens to travel worldwide, whereas it was limited by most Communist countries. A number[quantify] of Yugoslav citizens worked throughout Western Europe. Tito met many world leaders during his rule, such as Soviet rulers Joseph Stalin, Nikita Khrushchev and Leonid Brezhnev; Egypt's Gamal Abdel Nasser, Indian politicians Jawaharlal Nehru and Indira Gandhi; British Prime Ministers Winston Churchill, James Callaghan and Margaret Thatcher; U.S. Presidents Dwight D. Eisenhower, John F. Kennedy, Richard Nixon, Gerald Ford and Jimmy Carter; other political leaders, dignitaries and heads of state that Tito met at least once in his lifetime included Che Guevara, Fidel Castro, Yasser Arafat, Willy Brandt, Helmut Schmidt, Georges Pompidou, Queen Elizabeth II, Hua Guofeng, Kim Il Sung, Sukarno, Sheikh Mujibur Rahman, Suharto, Idi Amin, Haile Selassie, Kenneth Kaunda, Gaddafi, Erich Honecker, Nicolae Ceaușescu, János Kádár and Urho Kekkonen. He also met numerous celebrities.\", 'question': 'Nasser was a leader of what country?', 'unanswerable': None}, {'answer': 'the Roman Empire', 'extra_info': None, 'long_answer': 'The knowledge states that Egypt became one of the provinces of the Roman Empire after the death of Cleopatra in the 1st century BC.\\nAnswer: Egypt became a colony of the Roman Empire in the 1st century BC after the death of Cleopatra.', 'paragraph': \"History of Egypt. In 332 BC, Macedonian ruler Alexander the Great conquered Egypt as he toppled the Achaemenids and established the Hellenistic Ptolemaic Kingdom, whose first ruler was one of Alexander's former generals, Ptolemy I Soter. The Ptolemies had to fight native rebellions and were involved in foreign and civil wars that led to the decline of the kingdom and its final annexation by Rome. The death of Cleopatra ended the nominal independence of Egypt resulting in Egypt becoming one of the provinces of the Roman Empire.\", 'question': 'in the 1st century bc Egypt became a colony of', 'unanswerable': None}, {'answer': 'in 376', 'extra_info': None, 'long_answer': 'The knowledge provides several relevant dates and events related to the Fall of the Western Roman Empire. The Empire was at its greatest territorial extent in 117 CE, and the accession of Diocletian took place in 284. However, the major territorial loss began in 376 with a large-scale irruption of Goths and others.\\nAnswer: Based on the provided knowledge, the Roman Empire began to fall in 376 with the large-scale irruption of Goths and others, marking the onset of its irreversible major territorial loss.', 'paragraph': 'Fall of the Western Roman Empire. Relevant dates include 117 CE, when the Empire was at its greatest territorial extent, and the accession of Diocletian in 284. Irreversible major territorial loss, however, began in 376 with a large - scale irruption of Goths and others. In 395, after winning two destructive civil wars, Theodosius I died, leaving a collapsing field army and the Empire, still plagued by Goths, divided between his two incapable sons. By 476 when Odoacer deposed the Emperor Romulus, the Western Roman Emperor wielded negligible military, political, or financial power and had no effective control over the scattered Western domains that could still be described as Roman. Invading barbarians had established their own power in most of the area of the Western Empire. While its legitimacy lasted for centuries longer and its cultural influence remains today, the Western Empire never had the strength to rise again.', 'question': 'when did the Roman Empire began to fall', 'unanswerable': None}],\n",
      "\ttag: \"musique-valid.json\",\n",
      "\tanswer: \"in 376\",\n",
      "\tquestion: \"Nasser was the leader of the country that in the 1st century BC become a colony of an empire that began to fall in what year?\",\n",
      "\tmeta_info: {'attribute_1': None, 'attribute_2': None, 'category': None, 'comparison_attribute': None, 'entity': None, 'entity_1': None, 'entity_2': None, 'index': None, 'list_of_attributes': None, 'llm': 'gpt-3.5-turbo-instruct', 'negatives': None, 'selected': None, 'slot_dic': None, 'src': 'musique'},\n",
      "\tfinal_answer: \"Summary: Nasser was a leader of Egypt and Egypt became a colony of the Roman Empire in the 1st century BC after the death of Cleopatra. The Roman Empire began to fall in 376 with the largescale irruption of Goths and others, marking the onset of its irreversible major territorial loss. Nasser was the leader of Egypt, which became a colony of the Roman Empire in the 1st century BC after the death of Cleopatra. The Roman Empire began to fall in 376, which was marked by the largescale irruption of Goths and others.\n",
      "Answer: Nasser was the leader of Egypt, which became a colony of the Roman Empire in the 1st century BC after the death of Cleopatra. The Roman Empire began to fall in 376, which was marked by the largescale irruption of Goths and others. Therefore, Nasser was the leader of the country that in the 1st century BC became a colony of an empire that began to fall in 376.\",\n",
      "\tmultihop: True\n",
      "}\n",
      "\n",
      "=== Example 2 ===\n",
      "{\n",
      "\tsub_questions: [{'answer': None, 'extra_info': None, 'long_answer': \"The provided knowledge mentions that the number of visitors to Canada's Wonderland has been steadily increasing each year with an average annual growth rate of 1.5%. However, it does not provide a specific number for the total visitors that Canada's Wonderland receives annually.\\nAnswer: I'm sorry, but based on the provided knowledge, I cannot give a specific number for the total visitors that Canada's Wonderland receives annually.\", 'paragraph': \"The number of visitors to Canada's Wonderland has been steadily increasing each year, with an average annual growth rate of 1.5%. The park has seen a significant increase in international visitors, with tourists from the United States, Europe, and Asia making up a large portion of the attendance. Canada's Wonderland also holds the record for the most number of rides in a single amusement park, surpassing its own record of 16 roller coasters in 2008. The park continues to expand and add new rides and attractions, with the most recent addition being the Yukon Striker roller coaster in 2019. The park is also known for its annual Halloween event, Halloween Haunt, which draws in thousands of visitors each year.\", 'question': \"How many visitors does Canada's Wonderland receive annually?\", 'unanswerable': None}],\n",
      "\ttag: \"attributes-single_valid.json\",\n",
      "\tanswer: None,\n",
      "\tquestion: \"How many visitors does Canada's Wonderland receive annually?\",\n",
      "\tmeta_info: {'attribute_1': 'Number of Rides', 'attribute_2': 'Number of Visitors', 'category': 'Amusement Park in Canada', 'comparison_attribute': None, 'entity': \"Canada's Wonderland\", 'entity_1': None, 'entity_2': None, 'index': None, 'list_of_attributes': 'Size, Number of Rides, Number of Visitors, Location, Year Opened, Owner, Number of Employees, Admission Price, Number of Food Options', 'llm': 'wizard_lm', 'negatives': None, 'selected': {'answerable': True, 'item_index': 132, 'q_index': 1, 'src': 'attributes_valid'}, 'slot_dic': {'category': 'Amusement Park in Canada', 'popularity': '3', 'popularity_1': '4', 'popularity_2': '2', 'question_type': 'wh question'}, 'src': 'gen_qa'},\n",
      "\tfinal_answer: \"The provided knowledge mentions that the number of visitors to Canada's Wonderland has been steadily increasing each year with an average annual growth rate of 1.5%. However, it does not provide a specific number for the total visitors that Canada's Wonderland receives annually.\n",
      "Answer: I'm sorry, but based on the provided knowledge, I cannot give a specific number for the total visitors that Canada's Wonderland receives annually.\",\n",
      "\tmultihop: False\n",
      "}\n",
      "\n",
      "=== Example 3 ===\n",
      "{\n",
      "\tsub_questions: [{'answer': 'yak meat', 'extra_info': None, 'long_answer': 'The provided knowledge mentions that the staple foods in Tibet are roasted barley, yak meat, and butter tea. These foods are essential for the people of Tibet to maintain their diet and nutrition in the high-altitude region.\\nAnswer: A staple food in Tibet is roasted barley, yak meat, and butter tea.', 'paragraph': 'Tibet. The economy of Tibet is dominated by subsistence agriculture, though tourism has become a growing industry in recent decades. The dominant religion in Tibet is Tibetan Buddhism; in addition there is Bön, which is similar to Tibetan Buddhism, and there are also Tibetan Muslims and Christian minorities. Tibetan Buddhism is a primary influence on the art, music, and festivals of the region. Tibetan architecture reflects Chinese and Indian influences. Staple foods in Tibet are roasted barley, yak meat, and butter tea.', 'question': 'What is a staple food in Tibet ?', 'unanswerable': None}],\n",
      "\ttag: \"musique-single_valid.json\",\n",
      "\tanswer: None,\n",
      "\tquestion: \"What is a staple food in Tibet ?\",\n",
      "\tmeta_info: {'attribute_1': None, 'attribute_2': None, 'category': None, 'comparison_attribute': None, 'entity': None, 'entity_1': None, 'entity_2': None, 'index': None, 'list_of_attributes': None, 'llm': 'wizard_lm', 'negatives': None, 'selected': {'answerable': True, 'item_index': 79, 'q_index': 1, 'src': 'musique_valid'}, 'slot_dic': None, 'src': 'musique'},\n",
      "\tfinal_answer: \"The provided knowledge mentions that the staple foods in Tibet are roasted barley, yak meat, and butter tea. These foods are essential for the people of Tibet to maintain their diet and nutrition in the high-altitude region.\n",
      "Answer: A staple food in Tibet is roasted barley, yak meat, and butter tea.\",\n",
      "\tmultihop: False\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataset = \"khaimaitien/qa-expert-multi-hop-qa-V1.0\"\n",
    "data = load_dataset(dataset, split='validation', streaming=True)\n",
    "sample = list(data.take(3))\n",
    "\n",
    "print(F\"--------======== DATASET: {dataset} ========--------\")\n",
    "for i, example in enumerate(sample):\n",
    "    print(f\"\\n=== Example {i+1} ===\")\n",
    "    print_example(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing khaimaitien/qa-expert-multi-hop-qa-V1.0, split: train...: 100%|██████████| 25547/25547 [00:02<00:00, 9236.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished! 11322 records saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing khaimaitien/qa-expert-multi-hop-qa-V1.0, split: validation...: 100%|██████████| 3186/3186 [00:00<00:00, 8594.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished! 1552 records saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def load_qa_expert_multi_hopdataset(dataset_split, output_file):\n",
    "    dataset_name = \"khaimaitien/qa-expert-multi-hop-qa-V1.0\"\n",
    "    data = load_dataset(dataset_name)\n",
    "    dataset = data[dataset_split]\n",
    "\n",
    "    def cutout_summary(final_answer):\n",
    "        if \"Answer:\" in final_answer:\n",
    "            final_answer = final_answer[final_answer.find(\"Answer: \") + len(\"Answer: \"):]\n",
    "        return final_answer.strip()\n",
    "\n",
    "    processed_data = []\n",
    "    for item in tqdm(dataset, desc = f\"Processing {dataset_name}, split: {dataset_split}...\"):\n",
    "        if len(item[\"question\"]) < MAX_QUESTION_LENGTH and len(item[\"final_answer\"]) < MAX_ANSWER_LENGTH*2 and \\\n",
    "            ((\"I'm sorry\" not in item[\"final_answer\"]) and (\"I can't\" not in item[\"final_answer\"]) and (\"I cannot\" not in item[\"final_answer\"])):\n",
    "            conversation = [\n",
    "                {\"content\": item[\"question\"], \"role\": \"user\"},\n",
    "                {\"content\": cutout_summary(item[\"final_answer\"]), \"role\": \"assistant\"}\n",
    "            ]\n",
    "\n",
    "            if len(conversation[1][\"content\"]) > MAX_ANSWER_LENGTH:\n",
    "                continue\n",
    "\n",
    "            processed_row = {\n",
    "                \"question_id\": str(hash(item[\"question\"])),\n",
    "                \"tstamp\": str(datetime.now().timestamp()),\n",
    "                \"conversation_a\": conversation,\n",
    "                \"source\": dataset_name,\n",
    "                \"meta_info\": {\"tag\": item[\"tag\"], \"src\": item[\"meta_info\"].get(\"src\")},\n",
    "                \"tag\": \"knowledge_based\",\n",
    "            }\n",
    "            processed_data.append(processed_row)\n",
    "\n",
    "    df = pd.DataFrame(processed_data)\n",
    "    with open(output_file, 'wb') as f:\n",
    "        f.write(orjson.dumps(json.loads(df.to_json(orient='records'))))\n",
    "    print(f\"Finished! {len(df)} records saved\")\n",
    "\n",
    "load_qa_expert_multi_hopdataset(\"train\", \"./qa-expert-multi-hop-qa-train.json\")\n",
    "load_qa_expert_multi_hopdataset(\"validation\", \"./qa-expert-multi-hop-qa-valid.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qasper [Context based] [Papers]\n",
    "\n",
    "https://huggingface.co/datasets/allenai/qasper/viewer\n",
    "\n",
    "It's a dataset about questions about papers. Instead of taking full text I only have taken abstract and futher will filter out using special flag that the context is enough to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------======== DATASET: allenai/qasper ========--------\n",
      "\n",
      "=== Example 1 ===\n",
      "{\n",
      "\tid: \"1912.01214\",\n",
      "\ttitle: \"Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine Translation\",\n",
      "\tabstract: \"Transfer learning between different language pairs has shown its effectiveness for Neural Machine Translation (NMT) in low-resource scenario. However, existing transfer methods involving a common target language are far from success in the extreme scenario of zero-shot translation, due to the language space mismatch problem between transferor (the parent model) and transferee (the child model) on the source side. To address this challenge, we propose an effective transfer learning approach based on cross-lingual pre-training. Our key idea is to make all source languages share the same feature space and thus enable a smooth transition for zero-shot translation. To this end, we introduce one monolingual pre-training method and two bilingual pre-training methods to obtain a universal encoder for different languages. Once the universal encoder is constructed, the parent model built on such encoder is trained with large-scale annotated data and then directly applied in zero-shot translation scenario. Experiments on two public datasets show that our approach significantly outperforms strong pivot-based baseline and various multilingual NMT approaches.\",\n",
      "\tfull_text: {'section_name': ['Introduction', 'Related Work', 'Approach', 'Approach ::: Masked and Translation Language Model Pretraining', 'Approach ::: Bridge Language Model Pretraining', 'Approach ::: Transfer Protocol', 'Experiments ::: Setup', 'Experiments ::: Setup ::: Datasets.', 'Experiments ::: Setup ::: Experimental Details.', 'Experiments ::: Main Results', 'Experiments ::: Main Results ::: Results on Europarl Dataset.', 'Experiments ::: Main Results ::: Results on MultiUN Dataset.', 'Experiments ::: Analysis ::: Sentence Representation.', 'Experiments ::: Analysis ::: Contextualized Word Representation.', 'Experiments ::: Analysis ::: The Effect of Freezing Parameters.', 'Conclusion', 'Acknowledgments'], 'paragraphs': [['Although Neural Machine Translation (NMT) has dominated recent research on translation tasks BIBREF0, BIBREF1, BIBREF2, NMT heavily relies on large-scale parallel data, resulting in poor performance on low-resource or zero-resource language pairs BIBREF3. Translation between these low-resource languages (e.g., Arabic$\\\\rightarrow $Spanish) is usually accomplished with pivoting through a rich-resource language (such as English), i.e., Arabic (source) sentence is translated to English (pivot) first which is later translated to Spanish (target) BIBREF4, BIBREF5. However, the pivot-based method requires doubled decoding time and suffers from the propagation of translation errors.', 'One common alternative to avoid pivoting in NMT is transfer learning BIBREF6, BIBREF7, BIBREF8, BIBREF9 which leverages a high-resource pivot$\\\\rightarrow $target model (parent) to initialize a low-resource source$\\\\rightarrow $target model (child) that is further optimized with a small amount of available parallel data. Although this approach has achieved success in some low-resource language pairs, it still performs very poorly in extremely low-resource or zero-resource translation scenario. Specifically, BIBREF8 reports that without any child model training data, the performance of the parent model on the child test set is miserable.', 'In this work, we argue that the language space mismatch problem, also named domain shift problem BIBREF10, brings about the zero-shot translation failure in transfer learning. It is because transfer learning has no explicit training process to guarantee that the source and pivot languages share the same feature distributions, causing that the child model inherited from the parent model fails in such a situation. For instance, as illustrated in the left of Figure FIGREF1, the points of the sentence pair with the same semantics are not overlapping in source space, resulting in that the shared decoder will generate different translations denoted by different points in target space. Actually, transfer learning for NMT can be viewed as a multi-domain problem where each source language forms a new domain. Minimizing the discrepancy between the feature distributions of different source languages, i.e., different domains, will ensure the smooth transition between the parent and child models, as shown in the right of Figure FIGREF1. One way to achieve this goal is the fine-tuning technique, which forces the model to forget the specific knowledge from parent data and learn new features from child data. However, the domain shift problem still exists, and the demand of parallel child data for fine-tuning heavily hinders transfer learning for NMT towards the zero-resource setting.', 'In this paper, we explore the transfer learning in a common zero-shot scenario where there are a lot of source$\\\\leftrightarrow $pivot and pivot$\\\\leftrightarrow $target parallel data but no source$\\\\leftrightarrow $target parallel data. In this scenario, we propose a simple but effective transfer approach, the key idea of which is to relieve the burden of the domain shift problem by means of cross-lingual pre-training. To this end, we firstly investigate the performance of two existing cross-lingual pre-training methods proposed by BIBREF11 in zero-shot translation scenario. Besides, a novel pre-training method called BRidge Language Modeling (BRLM) is designed to make full use of the source$\\\\leftrightarrow $pivot bilingual data to obtain a universal encoder for different languages. Once the universal encoder is constructed, we only need to train the pivot$\\\\rightarrow $target model and then test this model in source$\\\\rightarrow $target direction directly. The main contributions of this paper are as follows:', 'We propose a new transfer learning approach for NMT which uses the cross-lingual language model pre-training to enable a high performance on zero-shot translation.', 'We propose a novel pre-training method called BRLM, which can effectively alleviates the distance between different source language spaces.', 'Our proposed approach significantly improves zero-shot translation performance, consistently surpassing pivoting and multilingual approaches. Meanwhile, the performance on supervised translation direction remains the same level or even better when using our method.'], ['In recent years, zero-shot translation in NMT has attracted widespread attention in academic research. Existing methods are mainly divided into four categories: pivot-based method, transfer learning, multilingual NMT, and unsupervised NMT.', 'Pivot-based Method is a common strategy to obtain a source$\\\\rightarrow $target model by introducing a pivot language. This approach is further divided into pivoting and pivot-synthetic. While the former firstly translates a source language into the pivot language which is later translated to the target language BIBREF4, BIBREF5, BIBREF12, the latter trains a source$\\\\rightarrow $target model with pseudo data generated from source-pivot or pivot-target parallel data BIBREF13, BIBREF14. Although the pivot-based methods can achieve not bad performance, it always falls into a computation-expensive and parameter-vast dilemma of quadratic growth in the number of source languages, and suffers from the error propagation problem BIBREF15.', 'Transfer Learning is firstly introduced for NMT by BIBREF6, which leverages a high-resource parent model to initialize the low-resource child model. On this basis, BIBREF7 and BIBREF8 use shared vocabularies for source/target language to improve transfer learning, while BIBREF16 relieve the vocabulary mismatch by mainly using cross-lingual word embedding. Although these methods are successful in the low-resource scene, they have limited effects in zero-shot translation.', 'Multilingual NMT (MNMT) enables training a single model that supports translation from multiple source languages into multiple target languages, even those unseen language pairs BIBREF17, BIBREF18, BIBREF19, BIBREF20, BIBREF21. Aside from simpler deployment, MNMT benefits from transfer learning where low-resource language pairs are trained together with high-resource ones. However, BIBREF22 point out that MNMT for zero-shot translation easily fails, and is sensitive to the hyper-parameter setting. Also, MNMT usually performs worse than the pivot-based method in zero-shot translation setting BIBREF23.', 'Unsupervised NMT (UNMT) considers a harder setting, in which only large-scale monolingual corpora are available for training. Recently, many methods have been proposed to improve the performance of UNMT, including using denoising auto-encoder, statistic machine translation (SMT) and unsupervised pre-training BIBREF24, BIBREF25, BIBREF26, BIBREF11. Since UNMT performs well between similar languages (e.g., English-German translation), its performance between distant languages is still far from expectation.', 'Our proposed method belongs to the transfer learning, but it is different from traditional transfer methods which train a parent model as starting point. Before training a parent model, our approach fully leverages cross-lingual pre-training methods to make all source languages share the same feature space and thus enables a smooth transition for zero-shot translation.'], ['In this section, we will present a cross-lingual pre-training based transfer approach. This method is designed for a common zero-shot scenario where there are a lot of source$\\\\leftrightarrow $pivot and pivot$\\\\leftrightarrow $target bilingual data but no source$\\\\leftrightarrow $target parallel data, and the whole training process can be summarized as follows step by step:', 'Pre-train a universal encoder with source/pivot monolingual or source$\\\\leftrightarrow $pivot bilingual data.', 'Train a pivot$\\\\rightarrow $target parent model built on the pre-trained universal encoder with the available parallel data. During the training process, we freeze several layers of the pre-trained universal encoder to avoid the degeneracy issue BIBREF27.', 'Directly translate source sentences into target sentences with the parent model, which benefits from the availability of the universal encoder.', 'The key difficulty of this method is to ensure the intermediate representations of the universal encoder are language invariant. In the rest of this section, we first present two existing methods yet to be explored in zero-shot translation, and then propose a straightforward but effective cross-lingual pre-training method. In the end, we present the whole training and inference protocol for transfer.'], ['Two existing cross-lingual pre-training methods, Masked Language Modeling (MLM) and Translation Language Modeling (TLM), have shown their effectiveness on XNLI cross-lingual classification task BIBREF11, BIBREF28, but these methods have not been well studied on cross-lingual generation tasks in zero-shot condition. We attempt to take advantage of the cross-lingual ability of the two methods for zero-shot translation.', 'Specifically, MLM adopts the Cloze objective of BERT BIBREF29 and predicts the masked words that are randomly selected and replaced with [MASK] token on monolingual corpus. In practice, MLM takes different language monolingual corpora as input to find features shared across different languages. With this method, word pieces shared in all languages have been mapped into a shared space, which makes the sentence representations across different languages close BIBREF30.', 'Since MLM objective is unsupervised and only requires monolingual data, TLM is designed to leverage parallel data when it is available. Actually, TLM is a simple extension of MLM, with the difference that TLM concatenates sentence pair into a whole sentence, and then randomly masks words in both the source and target sentences. In this way, the model can either attend to surrounding words or to the translation sentence, implicitly encouraging the model to align the source and target language representations. Note that although each sentence pair is formed into one sentence, the positions of the target sentence are reset to count form zero.'], ['Aside from MLM and TLM, we propose BRidge Language Modeling (BRLM) to further obtain word-level representation alignment between different languages. This method is inspired by the assumption that if the feature spaces of different languages are aligned very well, the masked words in the corrupted sentence can also be guessed by the context of the correspondingly aligned words on the other side. To achieve this goal, BRLM is designed to strengthen the ability to infer words across languages based on alignment information, instead of inferring words within monolingual sentence as in MLM or within the pseudo sentence formed by concatenating sentence pair as in TLM.', 'As illustrated in Figure FIGREF9, BRLM stacks shared encoder over both side sentences separately. In particular, we design two network structures for BRLM, which are divided into Hard Alignment (BRLM-HA) and Soft Alignment (BRLM-SA) according to the way of generating the alignment information. These two structures actually extend MLM into a bilingual scenario, with the difference that BRLM leverages external aligner tool or additional attention layer to explicitly introduce alignment information during model training.', 'Hard Alignment (BRLM-HA). We first use external aligner tool on source$\\\\leftrightarrow $pivot parallel data to extract the alignment information of sentence pair. During model training, given source$\\\\leftrightarrow $pivot sentence pair, BRLM-HA randomly masks some words in source sentence and leverages alignment information to obtain the aligned words in pivot sentence for masked words. Based on the processed input, BRLM-HA adopts the Transformer BIBREF1 encoder to gain the hidden states for source and pivot sentences respectively. Then the training objective of BRLM-HA is to predict the masked words by not only the surrounding words in source sentence but also the encoder outputs of the aligned words. Note that this training process is also carried out in a symmetric situation, in which we mask some words in pivot sentence and obtain the aligned words in the source sentence.', 'Soft Alignment (BRLM-SA). Instead of using external aligner tool, BRLM-SA introduces an additional attention layer to learn the alignment information together with model training. In this way, BRLM-SA avoids the effect caused by external wrong alignment information and enables many-to-one soft alignment during model training. Similar with BRLM-HA, the training objective of BRLM-SA is to predict the masked words by not only the surrounding words in source sentence but also the outputs of attention layer. In our implementation, the attention layer is a multi-head attention layer adopted in Transformer, where the queries come from the masked source sentence, the keys and values come from the pivot sentence.', 'In principle, MLM and TLM can learn some implicit alignment information during model training. However, the alignment process in MLM is inefficient since the shared word pieces only account for a small proportion of the whole corpus, resulting in the difficulty of expanding the shared information to align the whole corpus. TLM also lacks effort in alignment between the source and target sentences since TLM concatenates the sentence pair into one sequence, making the explicit alignment between the source and target infeasible. BRLM fully utilizes the alignment information to obtain better word-level representation alignment between different languages, which better relieves the burden of the domain shift problem.'], ['We consider the typical zero-shot translation scenario in which a high resource pivot language has parallel data with both source and target languages, while source and target languages has no parallel data between themselves. Our proposed cross-lingual pretraining based transfer approach for source$\\\\rightarrow $target zero-shot translation is mainly divided into two phrases: the pretraining phase and the transfer phase.', 'In the pretraining phase, we first pretrain MLM on monolingual corpora of both source and pivot languages, and continue to pretrain TLM or the proposed BRLM on the available parallel data between source and pivot languages, in order to build a cross-lingual encoder shared by the source and pivot languages.', 'In the transfer phase, we train pivot$\\\\rightarrow $target NMT model initialized by the cross-lingually pre-trained encoder, and finally transfer the trained NMT model to source$\\\\rightarrow $target translation thanks to the shared encoder. Note that during training pivot$\\\\rightarrow $target NMT model, we freeze several layers of the cross-lingually pre-trained encoder to avoid the degeneracy issue.', 'For the more complicated scenario that either the source side or the target side has multiple languages, the encoder and the decoder are also shared across each side languages for efficient deployment of translation between multiple languages.'], ['We evaluate our cross-lingual pre-training based transfer approach against several strong baselines on two public datatsets, Europarl BIBREF31 and MultiUN BIBREF32, which contain multi-parallel evaluation data to assess the zero-shot performance. In all experiments, we use BLEU as the automatic metric for translation evaluation.'], ['The statistics of Europarl and MultiUN corpora are summarized in Table TABREF18. For Europarl corpus, we evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language, its left side is the source language, and its right side is the target language. We remove the multi-parallel sentences between different training corpora to ensure zero-shot settings. We use the devtest2006 as the validation set and the test2006 as the test set for Fr$\\\\rightarrow $Es and De$\\\\rightarrow $Fr. For distant language pair Ro$\\\\rightarrow $De, we extract 1,000 overlapping sentences from newstest2016 as the test set and the 2,000 overlapping sentences split from the training set as the validation set since there is no official validation and test sets. For vocabulary, we use 60K sub-word tokens based on Byte Pair Encoding (BPE) BIBREF33.', 'For MultiUN corpus, we use four languages: English (En) is set as the pivot language, which has parallel data with other three languages which do not have parallel data between each other. The three languages are Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation direction for evaluation. We use 80K BPE splits as the vocabulary. Note that all sentences are tokenized by the tokenize.perl script, and we lowercase all data to avoid a large vocabulary for the MultiUN corpus.'], [\"We use traditional transfer learning, pivot-based method and multilingual NMT as our baselines. For the fair comparison, the Transformer-big model with 1024 embedding/hidden units, 4096 feed-forward filter size, 6 layers and 8 heads per layer is adopted for all translation models in our experiments. We set the batch size to 2400 per batch and limit sentence length to 100 BPE tokens. We set the $\\\\text{attn}\\\\_\\\\text{drop}=0$ (a dropout rate on each attention head), which is favorable to the zero-shot translation and has no effect on supervised translation directions BIBREF22. For the model initialization, we use Facebook's cross-lingual pretrained models released by XLM to initialize the encoder part, and the rest parameters are initialized with xavier uniform. We employ the Adam optimizer with $\\\\text{lr}=0.0001$, $t_{\\\\text{warm}\\\\_\\\\text{up}}=4000$ and $\\\\text{dropout}=0.1$. At decoding time, we generate greedily with length penalty $\\\\alpha =1.0$.\", 'Regarding MLM, TLM and BRLM, as mentioned in the pre-training phase of transfer protocol, we first pre-train MLM on monolingual data of both source and pivot languages, then leverage the parameters of MLM to initialize TLM and the proposed BRLM, which are continued to be optimized with source-pivot bilingual data. In our experiments, we use MLM+TLM, MLM+BRLM to represent this training process. For the masking strategy during training, following BIBREF29, $15\\\\%$ of BPE tokens are selected to be masked. Among the selected tokens, $80\\\\%$ of them are replaced with [MASK] token, $10\\\\%$ are replaced with a random BPE token, and $10\\\\%$ unchanged. The prediction accuracy of masked words is used as a stopping criterion in the pre-training stage. Besides, we use fastalign tool BIBREF34 to extract word alignments for BRLM-HA.'], ['Table TABREF19 and TABREF26 report zero-shot results on Europarl and Multi-UN evaluation sets, respectively. We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23. Pivoting translates source to pivot then to target in two steps, causing inefficient translation process. Our approaches use one encoder-decoder model to translate between any zero-shot directions, which is more efficient than pivoting. Regarding the comparison between transfer approaches, our cross-lingual pretraining based transfer outperforms transfer method that does not use pretraining by a large margin.'], ['Regarding comparison between the baselines in table TABREF19, we find that pivoting is the strongest baseline that has significant advantage over other two baselines. Cross-lingual transfer for languages without shared vocabularies BIBREF16 manifests the worst performance because of not using source$\\\\leftrightarrow $pivot parallel data, which is utilized as beneficial supervised signal for the other two baselines.', 'Our best approach of MLM+BRLM-SA achieves the significant superior performance to all baselines in the zero-shot directions, improving by 0.9-4.8 BLEU points over the strong pivoting. Meanwhile, in the supervised direction of pivot$\\\\rightarrow $target, our approaches performs even better than the original supervised Transformer thanks to the shared encoder trained on both large-scale monolingual data and parallel data between multiple languages.', 'MLM alone that does not use source$\\\\leftrightarrow $pivot parallel data performs much better than the cross-lingual transfer, and achieves comparable results to pivoting. When MLM is combined with TLM or the proposed BRLM, the performance is further improved. MLM+BRLM-SA performs the best, and is better than MLM+BRLM-HA indicating that soft alignment is helpful than hard alignment for the cross-lingual pretraining.'], ['Like experimental results on Europarl, MLM+BRLM-SA performs the best among all proposed cross-lingual pretraining based transfer approaches as shown in Table TABREF26. When comparing systems consisting of one encoder-decoder model for all zero-shot translation, our approaches performs significantly better than MNMT BIBREF19.', 'Although it is challenging for one model to translate all zero-shot directions between multiple distant language pairs of MultiUN, MLM+BRLM-SA still achieves better performances on Es $\\\\rightarrow $ Ar and Es $\\\\rightarrow $ Ru than strong pivoting$_{\\\\rm m}$, which uses MNMT to translate source to pivot then to target in two separate steps with each step receiving supervised signal of parallel corpora. Our approaches surpass pivoting$_{\\\\rm m}$ in all zero-shot directions by adding back translation BIBREF33 to generate pseudo parallel sentences for all zero-shot directions based on our pretrained models such as MLM+BRLM-SA, and further training our universal encoder-decoder model with these pseudo data. BIBREF22 gu2019improved introduces back translation into MNMT, while we adopt it in our transfer approaches. Finally, our best MLM+BRLM-SA with back translation outperforms pivoting$_{\\\\rm m}$ by 2.4 BLEU points averagely, and outperforms MNMT BIBREF22 by 4.6 BLEU points averagely. Again, in supervised translation directions, MLM+BRLM-SA with back translation also achieves better performance than the original supervised Transformer.'], ['We first evaluate the representational invariance across languages for all cross-lingual pre-training methods. Following BIBREF23, we adopt max-pooling operation to collect the sentence representation of each encoder layer for all source-pivot sentence pairs in the Europarl validation sets. Then we calculate the cosine similarity for each sentence pair and average all cosine scores. As shown in Figure FIGREF27, we can observe that, MLM+BRLM-SA has the most stable and similar cross-lingual representations of sentence pairs on all layers, while it achieves the best performance in zero-shot translation. This demonstrates that better cross-lingual representations can benefit for the process of transfer learning. Besides, MLM+BRLM-HA is not as superior as MLM+BRLM-SA and even worse than MLM+TLM on Fr-En, since MLM+BRLM-HA may suffer from the wrong alignment knowledge from an external aligner tool. We also find an interesting phenomenon that as the number of layers increases, the cosine similarity decreases.'], ['We further sample an English-Russian sentence pair from the MultiUN validation sets and visualize the cosine similarity between hidden states of the top encoder layer to further investigate the difference of all cross-lingual pre-training methods. As shown in Figure FIGREF38, the hidden states generated by MLM+BRLM-SA have higher similarity for two aligned words. It indicates that MLM+BRLM-SA can gain better word-level representation alignment between source and pivot languages, which better relieves the burden of the domain shift problem.'], ['To freeze parameters is a common strategy to avoid catastrophic forgetting in transfer learning BIBREF27. Table TABREF43 shows the performance of transfer learning with freezing different layers on MultiUN test set, in which En$\\\\rightarrow $Ru denotes the parent model, Ar$\\\\rightarrow $Ru and Es$\\\\rightarrow $Ru are two child models, and all models are based on MLM+BRLM-SA. We can find that updating all parameters during training will cause a notable drop on the zero-shot direction due to the catastrophic forgetting. On the contrary, freezing all the parameters leads to the decline on supervised direction because the language features extracted during pre-training is not sufficient for MT task. Freezing the first four layers of the transformer shows the best performance and keeps the balance between pre-training and fine-tuning.'], ['In this paper, we propose a cross-lingual pretraining based transfer approach for the challenging zero-shot translation task, in which source and target languages have no parallel data, while they both have parallel data with a high resource pivot language. With the aim of building the language invariant representation between source and pivot languages for smooth transfer of the parent model of pivot$\\\\rightarrow $target direction to the child model of source$\\\\rightarrow $target direction, we introduce one monolingual pretraining method and two bilingual pretraining methods to construct an universal encoder for the source and pivot languages. Experiments on public datasets show that our approaches significantly outperforms several strong baseline systems, and manifest the language invariance characteristics in both sentence level and word level neural representations.'], ['We would like to thank the anonymous reviewers for the helpful comments. This work was supported by National Key R&D Program of China (Grant No. 2016YFE0132100), National Natural Science Foundation of China (Grant No. 61525205, 61673289). This work was also partially supported by Alibaba Group through Alibaba Innovative Research Program and the Priority Academic Program Development (PAPD) of Jiangsu Higher Education Institutions.']]},\n",
      "\tqas: {'question': ['which multilingual approaches do they compare with?', 'what are the pivot-based baselines?', 'which datasets did they experiment with?', 'what language pairs are explored?'], 'question_id': ['b6f15fb6279b82e34a5bf4828b7b5ddabfdf1d54', 'f5e6f43454332e0521a778db0b769481e23e7682', '9a05a5f4351db75da371f7ac12eb0b03607c4b87', '5eda469a8a77f028d0c5f1acd296111085614537'], 'nlp_background': ['', '', '', ''], 'topic_background': ['', '', '', ''], 'paper_read': ['', '', '', ''], 'search_query': ['', '', '', ''], 'question_writer': ['c1fbdd7a261021041f75fbe00a55b4c386ebbbb4', 'c1fbdd7a261021041f75fbe00a55b4c386ebbbb4', 'c1fbdd7a261021041f75fbe00a55b4c386ebbbb4', 'c1fbdd7a261021041f75fbe00a55b4c386ebbbb4'], 'answers': [{'answer': [{'unanswerable': False, 'extractive_spans': ['BIBREF19', 'BIBREF20'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Table TABREF19 and TABREF26 report zero-shot results on Europarl and Multi-UN evaluation sets, respectively. We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23. Pivoting translates source to pivot then to target in two steps, causing inefficient translation process. Our approaches use one encoder-decoder model to translate between any zero-shot directions, which is more efficient than pivoting. Regarding the comparison between transfer approaches, our cross-lingual pretraining based transfer outperforms transfer method that does not use pretraining by a large margin.'], 'highlighted_evidence': ['We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. ', 'The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23.']}, {'unanswerable': False, 'extractive_spans': ['multilingual NMT (MNMT) BIBREF19'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Table TABREF19 and TABREF26 report zero-shot results on Europarl and Multi-UN evaluation sets, respectively. We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23. Pivoting translates source to pivot then to target in two steps, causing inefficient translation process. Our approaches use one encoder-decoder model to translate between any zero-shot directions, which is more efficient than pivoting. Regarding the comparison between transfer approaches, our cross-lingual pretraining based transfer outperforms transfer method that does not use pretraining by a large margin.'], 'highlighted_evidence': ['We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16.']}], 'annotation_id': ['819898a2daf67225307aaf59d8048987c06b0c03', 'bbc549d0d6a598c0a93d0dde01d8cc6ffe316adc'], 'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58', '258ee4069f740c400c0049a2580945a1cc7f044c']}, {'answer': [{'unanswerable': False, 'extractive_spans': ['pivoting', 'pivoting$_{\\\\rm m}$'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Table TABREF19 and TABREF26 report zero-shot results on Europarl and Multi-UN evaluation sets, respectively. We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23. Pivoting translates source to pivot then to target in two steps, causing inefficient translation process. Our approaches use one encoder-decoder model to translate between any zero-shot directions, which is more efficient than pivoting. Regarding the comparison between transfer approaches, our cross-lingual pretraining based transfer outperforms transfer method that does not use pretraining by a large margin.', 'Although it is challenging for one model to translate all zero-shot directions between multiple distant language pairs of MultiUN, MLM+BRLM-SA still achieves better performances on Es $\\\\rightarrow $ Ar and Es $\\\\rightarrow $ Ru than strong pivoting$_{\\\\rm m}$, which uses MNMT to translate source to pivot then to target in two separate steps with each step receiving supervised signal of parallel corpora. Our approaches surpass pivoting$_{\\\\rm m}$ in all zero-shot directions by adding back translation BIBREF33 to generate pseudo parallel sentences for all zero-shot directions based on our pretrained models such as MLM+BRLM-SA, and further training our universal encoder-decoder model with these pseudo data. BIBREF22 gu2019improved introduces back translation into MNMT, while we adopt it in our transfer approaches. Finally, our best MLM+BRLM-SA with back translation outperforms pivoting$_{\\\\rm m}$ by 2.4 BLEU points averagely, and outperforms MNMT BIBREF22 by 4.6 BLEU points averagely. Again, in supervised translation directions, MLM+BRLM-SA with back translation also achieves better performance than the original supervised Transformer.'], 'highlighted_evidence': ['We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16.', 'Although it is challenging for one model to translate all zero-shot directions between multiple distant language pairs of MultiUN, MLM+BRLM-SA still achieves better performances on Es $\\\\rightarrow $ Ar and Es $\\\\rightarrow $ Ru than strong pivoting$_{\\\\rm m}$, which uses MNMT to translate source to pivot then to target in two separate steps with each step receiving supervised signal of parallel corpora. ']}, {'unanswerable': False, 'extractive_spans': ['firstly translates a source language into the pivot language which is later translated to the target language'], 'yes_no': None, 'free_form_answer': '', 'evidence': [\"We use traditional transfer learning, pivot-based method and multilingual NMT as our baselines. For the fair comparison, the Transformer-big model with 1024 embedding/hidden units, 4096 feed-forward filter size, 6 layers and 8 heads per layer is adopted for all translation models in our experiments. We set the batch size to 2400 per batch and limit sentence length to 100 BPE tokens. We set the $\\\\text{attn}\\\\_\\\\text{drop}=0$ (a dropout rate on each attention head), which is favorable to the zero-shot translation and has no effect on supervised translation directions BIBREF22. For the model initialization, we use Facebook's cross-lingual pretrained models released by XLM to initialize the encoder part, and the rest parameters are initialized with xavier uniform. We employ the Adam optimizer with $\\\\text{lr}=0.0001$, $t_{\\\\text{warm}\\\\_\\\\text{up}}=4000$ and $\\\\text{dropout}=0.1$. At decoding time, we generate greedily with length penalty $\\\\alpha =1.0$.\", 'Pivot-based Method is a common strategy to obtain a source$\\\\rightarrow $target model by introducing a pivot language. This approach is further divided into pivoting and pivot-synthetic. While the former firstly translates a source language into the pivot language which is later translated to the target language BIBREF4, BIBREF5, BIBREF12, the latter trains a source$\\\\rightarrow $target model with pseudo data generated from source-pivot or pivot-target parallel data BIBREF13, BIBREF14. Although the pivot-based methods can achieve not bad performance, it always falls into a computation-expensive and parameter-vast dilemma of quadratic growth in the number of source languages, and suffers from the error propagation problem BIBREF15.'], 'highlighted_evidence': ['We use traditional transfer learning, pivot-based method and multilingual NMT as our baselines.', 'Pivot-based Method is a common strategy to obtain a source$\\\\rightarrow $target model by introducing a pivot language. This approach is further divided into pivoting and pivot-synthetic. While the former firstly translates a source language into the pivot language which is later translated to the target language BIBREF4, BIBREF5, BIBREF12, the latter trains a source$\\\\rightarrow $target model with pseudo data generated from source-pivot or pivot-target parallel data BIBREF13, BIBREF14.']}], 'annotation_id': ['291a918eae943cf62b5d5ad4a9b6b24c4e3090f1', '8ca8aaa07c7ee1d00c02322d47244d4489f151d1'], 'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58', '258ee4069f740c400c0049a2580945a1cc7f044c']}, {'answer': [{'unanswerable': False, 'extractive_spans': ['Europarl', 'MultiUN'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We evaluate our cross-lingual pre-training based transfer approach against several strong baselines on two public datatsets, Europarl BIBREF31 and MultiUN BIBREF32, which contain multi-parallel evaluation data to assess the zero-shot performance. In all experiments, we use BLEU as the automatic metric for translation evaluation.'], 'highlighted_evidence': ['We evaluate our cross-lingual pre-training based transfer approach against several strong baselines on two public datatsets, Europarl BIBREF31 and MultiUN BIBREF32, which contain multi-parallel evaluation data to assess the zero-shot performance.']}, {'unanswerable': False, 'extractive_spans': ['Europarl BIBREF31', 'MultiUN BIBREF32'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We evaluate our cross-lingual pre-training based transfer approach against several strong baselines on two public datatsets, Europarl BIBREF31 and MultiUN BIBREF32, which contain multi-parallel evaluation data to assess the zero-shot performance. In all experiments, we use BLEU as the automatic metric for translation evaluation.'], 'highlighted_evidence': ['We evaluate our cross-lingual pre-training based transfer approach against several strong baselines on two public datatsets, Europarl BIBREF31 and MultiUN BIBREF32, which contain multi-parallel evaluation data to assess the zero-shot performance.']}], 'annotation_id': ['ce1cbd643169ab70cc3b807401798b400472868f', 'f5d30fb867823556668418ecbbafde77fa834f2f'], 'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58', '258ee4069f740c400c0049a2580945a1cc7f044c']}, {'answer': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'De-En, En-Fr, Fr-En, En-Es, Ro-En, En-De, Ar-En, En-Ru', 'evidence': ['For MultiUN corpus, we use four languages: English (En) is set as the pivot language, which has parallel data with other three languages which do not have parallel data between each other. The three languages are Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation direction for evaluation. We use 80K BPE splits as the vocabulary. Note that all sentences are tokenized by the tokenize.perl script, and we lowercase all data to avoid a large vocabulary for the MultiUN corpus.', 'The statistics of Europarl and MultiUN corpora are summarized in Table TABREF18. For Europarl corpus, we evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language, its left side is the source language, and its right side is the target language. We remove the multi-parallel sentences between different training corpora to ensure zero-shot settings. We use the devtest2006 as the validation set and the test2006 as the test set for Fr$\\\\rightarrow $Es and De$\\\\rightarrow $Fr. For distant language pair Ro$\\\\rightarrow $De, we extract 1,000 overlapping sentences from newstest2016 as the test set and the 2,000 overlapping sentences split from the training set as the validation set since there is no official validation and test sets. For vocabulary, we use 60K sub-word tokens based on Byte Pair Encoding (BPE) BIBREF33.', 'FLOAT SELECTED: Table 1: Data Statistics.'], 'highlighted_evidence': ['For MultiUN corpus, we use four languages: English (En) is set as the pivot language, which has parallel data with other three languages which do not have parallel data between each other. The three languages are Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation direction for evaluation. ', 'The statistics of Europarl and MultiUN corpora are summarized in Table TABREF18. For Europarl corpus, we evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language, its left side is the source language, and its right side is the target language. ', 'FLOAT SELECTED: Table 1: Data Statistics.']}, {'unanswerable': False, 'extractive_spans': ['French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De)', 'Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The statistics of Europarl and MultiUN corpora are summarized in Table TABREF18. For Europarl corpus, we evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language, its left side is the source language, and its right side is the target language. We remove the multi-parallel sentences between different training corpora to ensure zero-shot settings. We use the devtest2006 as the validation set and the test2006 as the test set for Fr$\\\\rightarrow $Es and De$\\\\rightarrow $Fr. For distant language pair Ro$\\\\rightarrow $De, we extract 1,000 overlapping sentences from newstest2016 as the test set and the 2,000 overlapping sentences split from the training set as the validation set since there is no official validation and test sets. For vocabulary, we use 60K sub-word tokens based on Byte Pair Encoding (BPE) BIBREF33.', 'For MultiUN corpus, we use four languages: English (En) is set as the pivot language, which has parallel data with other three languages which do not have parallel data between each other. The three languages are Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation direction for evaluation. We use 80K BPE splits as the vocabulary. Note that all sentences are tokenized by the tokenize.perl script, and we lowercase all data to avoid a large vocabulary for the MultiUN corpus.'], 'highlighted_evidence': ['For Europarl corpus, we evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language, its left side is the source language, and its right side is the target language. We remove the multi-parallel sentences between different training corpora to ensure zero-shot settings. We use the devtest2006 as the validation set and the test2006 as the test set for Fr$\\\\rightarrow $Es and De$\\\\rightarrow $Fr. For distant language pair Ro$\\\\rightarrow $De, we extract 1,000 overlapping sentences from newstest2016 as the test set and the 2,000 overlapping sentences split from the training set as the validation set since there is no official validation and test sets.', 'For MultiUN corpus, we use four languages: English (En) is set as the pivot language, which has parallel data with other three languages which do not have parallel data between each other. The three languages are Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation direction for evaluation.']}], 'annotation_id': ['36d46e79bc56de8706a4a7001d5e74fc25ecf15c', '81063a32b1f4c7450eb6c0fbbcb649870b47e344'], 'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58', '258ee4069f740c400c0049a2580945a1cc7f044c']}]},\n",
      "\tfigures_and_tables: {'caption': ['Figure 1: The circle and triangle dots represent source sentences in different language l1 and l2, and the square dots means target sentences in language l3. A sample of translation pairs is connected by the dashed line. We would like to force each of the translation pairs has the same latent representation as the right part of the figure so as to transfer l1 → l3 model directly to l2 → l3 model.', 'Figure 2: The overview of BRidge Language Modeling (BRLM). The BRLM extends MLM (Lample and Conneau 2019) to pairs of parallel sentences and leverages explicit alignment information obtained by external aligner tool or additional attention layer to encourage word representation alignment across different languages.', 'Table 1: Data Statistics.', 'Table 2: Results on Europarl test sets. Three pivot settings are conducted in our experiments. In each setting, the left column presents the zero-shot performances (source→target), and the right column denotes the performances in the supervised parent model direction (pivot→target).', 'Table 3: Results on MultiUN test sets. The six zero-shot translation directions are evaluated. The column “A-ZST\" reports averaged BLEU of zero-shot translation, while the column “A-ST\" reports averaged BLEU of supervised pivot→target direction.', 'Figure 3: Cosine similarity between sentence representation of each encoder layer across all source-pivot sentence pairs in the Europarl validation set.', 'Figure 4: Cosine similarity visualization at word level given an English-Russian sentence pair from the MultiUN validation sets. Brighter indicates higher similarity.', 'Table 4: BLEU score of freezing different layers. The number in Freezing Layers column denotes that the number of encoder layers will not be updated.'], 'file': ['1-Figure1-1.png', '3-Figure2-1.png', '4-Table1-1.png', '5-Table2-1.png', '6-Table3-1.png', '6-Figure3-1.png', '7-Figure4-1.png', '7-Table4-1.png']}\n",
      "}\n",
      "\n",
      "=== Example 2 ===\n",
      "{\n",
      "\tid: \"1810.08699\",\n",
      "\ttitle: \"pioNER: Datasets and Baselines for Armenian Named Entity Recognition\",\n",
      "\tabstract: \"In this work, we tackle the problem of Armenian named entity recognition, providing silver- and gold-standard datasets as well as establishing baseline results on popular models. We present a 163000-token named entity corpus automatically generated and annotated from Wikipedia, and another 53400-token corpus of news sentences with manual annotation of people, organization and location named entities. The corpora were used to train and evaluate several popular named entity recognition models. Alongside the datasets, we release 50-, 100-, 200-, 300-dimensional GloVe word embeddings trained on a collection of Armenian texts from Wikipedia, news, blogs, and encyclopedia.\",\n",
      "\tfull_text: {'section_name': ['Introduction', 'Automated training corpus generation', 'Dataset extraction', 'Using Wikidata to classify Wikipedia', 'Generated data', 'Test dataset', 'Word embeddings', 'Experiments', 'Models', 'Evaluation', 'Discussion', 'Conclusion'], 'paragraphs': [['Named entity recognition is an important task of natural language processing, featuring in many popular text processing toolkits. This area of natural language processing has been actively studied in the latest decades and the advent of deep learning reinvigorated the research on more effective and accurate models. However, most of existing approaches require large annotated corpora. To the best of our knowledge, no such work has been done for the Armenian language, and in this work we address several problems, including the creation of a corpus for training machine learning models, the development of gold-standard test corpus and evaluation of the effectiveness of established approaches for the Armenian language.', \"Considering the cost of creating manually annotated named entity corpus, we focused on alternative approaches. Lack of named entity corpora is a common problem for many languages, thus bringing the attention of many researchers around the globe. Projection based transfer schemes have been shown to be very effective (e.g. BIBREF0 , BIBREF1 , BIBREF2 ), using resource-rich language's corpora to generate annotated data for the low-resource language. In this approach, the annotations of high-resource language are projected over the corresponding tokens of the parallel low-resource language's texts. This strategy can be applied for language pairs that have parallel corpora. However, that approach would not work for Armenian as we did not have access to sufficiently large parallel corpus with a resource-rich language.\", 'Another popular approach is using Wikipedia. Klesti Hoxha and Artur Baxhaku employ gazetteers extracted from Wikipedia to generate an annotated corpus for Albanian BIBREF3 , and Weber and Pötzl propose a rule-based system for German that leverages the information from Wikipedia BIBREF4 . However, the latter relies on external tools such as part-of-speech taggers, making it nonviable for the Armenian language.', \"Nothman et al. generated a silver-standard corpus for 9 languages by extracting Wikipedia article texts with outgoing links and turning those links into named entity annotations based on the target article's type BIBREF5 . Sysoev and Andrianov used a similar approach for the Russian language BIBREF6 BIBREF7 . Based on its success for a wide range of languages, our choice fell on this model to tackle automated data generation and annotation for the Armenian language.\", 'Aside from the lack of training data, we also address the absence of a benchmark dataset of Armenian texts for named entity recognition. We propose a gold-standard corpus with manual annotation of CoNLL named entity categories: person, location, and organization BIBREF8 BIBREF9 , hoping it will be used to evaluate future named entity recognition models.', 'Furthermore, popular entity recognition models were applied to the mentioned data in order to obtain baseline results for future research in the area. Along with the datasets, we developed GloVe BIBREF10 word embeddings to train and evaluate the deep learning models in our experiments.', 'The contributions of this work are (i) the silver-standard training corpus, (ii) the gold-standard test corpus, (iii) GloVe word embeddings, (iv) baseline results for 3 different models on the proposed benchmark data set. All aforementioned resources are available on GitHub.'], [\"We used Sysoev and Andrianov's modification of the Nothman et al. approach to automatically generate data for training a named entity recognizer. This approach uses links between Wikipedia articles to generate sequences of named-entity annotated tokens.\"], ['The main steps of the dataset extraction system are described in Figure FIGREF3 .', \"First, each Wikipedia article is assigned a named entity class (e.g. the article Քիմ Քաշքաշյան (Kim Kashkashian) is classified as PER (person), Ազգերի լիգա(League of Nations) as ORG (organization), Սիրիա(Syria) as LOC etc). One of the core differences between our approach and Nothman's system is that we do not rely on manual classification of articles and do not use inter-language links to project article classifications across languages. Instead, our classification algorithm uses only an article's Wikidata entry's first instance of label's parent subclass of labels, which are, incidentally, language independent and thus can be used for any language.\", \"Then, outgoing links in articles are assigned the article's type they are leading to. Sentences are included in the training corpus only if they contain at least one named entity and all contained capitalized words have an outgoing link to an article of known type. Since in Wikipedia articles only the first mention of each entity is linked, this approach becomes very restrictive and in order to include more sentences, additional links are inferred. This is accomplished by compiling a list of common aliases for articles corresponding to named entities, and then finding text fragments matching those aliases to assign a named entity label. An article's aliases include its title, titles of disambiguation pages with the article, and texts of links leading to the article (e.g. Լենինգրադ (Leningrad), Պետրոգրադ (Petrograd), Պետերբուրգ (Peterburg) are aliases for Սանկտ Պետերբուրգ (Saint Petersburg)). The list of aliases is compiled for all PER, ORG, LOC articles.\", 'After that, link boundaries are adjusted by removing the labels for expressions in parentheses, the text after a comma, and in some cases breaking into separate named entities if the linked text contains a comma. For example, [LOC Աբովյան (քաղաք)] (Abovyan (town)) is reworked into [LOC Աբովյան] (քաղաք).'], [\"Instead of manually classifying Wikipedia articles as it was done in Nothman et al., we developed a rule-based classifier that used an article's Wikidata instance of and subclass of attributes to find the corresponding named entity type.\", \"The classification could be done using solely instance of labels, but these labels are unnecessarily specific for the task and building a mapping on it would require a more time-consuming and meticulous work. Therefore, we classified articles based on their first instance of attribute's subclass of values. Table TABREF4 displays the mapping between these values and named entity types. Using higher-level subclass of values was not an option as their values often were too general, making it impossible to derive the correct named entity category.\"], ['Using the algorithm described above, we generated 7455 annotated sentences with 163247 tokens based on 20 February 2018 dump of Armenian Wikipedia.', 'The generated data is still significantly smaller than the manually annotated corpora from CoNLL 2002 and 2003. For comparison, the train set of English CoNLL 2003 corpus contains 203621 tokens and the German one 206931, while the Spanish and Dutch corpora from CoNLL 2002 respectively 273037 and 218737 lines. The smaller size of our generated data can be attributed to the strict selection of candidate sentences as well as simply to the relatively small size of Armenian Wikipedia.', 'The accuracy of annotation in the generated corpus heavily relies on the quality of links in Wikipedia articles. During generation, we assumed that first mentions of all named entities have an outgoing link to their article, however this was not always the case in actual source data and as a result the train set contained sentences where not all named entities are labeled. Annotation inaccuracies also stemmed from wrongly assigned link boundaries (for example, in Wikipedia article Արթուր Ուելսլի Վելինգթոն (Arthur Wellesley) there is a link to the Napoleon article with the text \"է Նապոլեոնը\" (\"Napoleon is\"), when it should be \"Նապոլեոնը\" (\"Napoleon\")). Another kind of common annotation errors occurred when a named entity appeared inside a link not targeting a LOC, ORG, or PER article (e.g. \"ԱՄՆ նախագահական ընտրություններում\" (\"USA presidential elections\") is linked to the article ԱՄՆ նախագահական ընտրություններ 2016 (United States presidential election, 2016) and as a result [LOC ԱՄՆ] (USA) is lost).'], ['In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am. This dataset is comparable in size with the test sets of other languages (Table TABREF10 ). Included sentences are from political, sports, local and world news (Figures FIGREF8 , FIGREF9 ), covering the period between August 2012 and July 2018. The dataset provides annotations for 3 popular named entity classes: people (PER), organizations (ORG), and locations (LOC), and is released in CoNLL03 format with IOB tagging scheme. Tokens and sentences were segmented according to the UD standards for the Armenian language BIBREF11 .', \"During annotation, we generally relied on categories and guidelines assembled by BBN Technologies for TREC 2002 question answering track. Only named entities corresponding to BBN's person name category were tagged as PER. Those include proper names of people, including fictional people, first and last names, family names, unique nicknames. Similarly, organization name categories, including company names, government agencies, educational and academic institutions, sports clubs, musical ensembles and other groups, hospitals, museums, newspaper names, were marked as ORG. However, unlike BBN, we did not mark adjectival forms of organization names as named entities. BBN's gpe name, facility name, location name categories were combined and annotated as LOC.\", 'We ignored entities of other categories (e.g. works of art, law, or events), including those cases where an ORG, LOC or PER entity was inside an entity of extraneous type (e.g. ՀՀ (RA) in ՀՀ Քրեական Օրենսգիրք (RA Criminal Code) was not annotated as LOC).', 'Quotation marks around a named entity were not annotated unless those quotations were a part of that entity\\'s full official name (e.g. «Նաիրիտ գործարան» ՓԲԸ (\"Nairit Plant\" CJSC)).', 'Depending on context, metonyms such as Կրեմլ (Kremlin), Բաղրամյան 26 (Baghramyan 26) were annotated as ORG when referring to respective government agencies. Likewise, country or city names were also tagged as ORG when referring to sports teams representing them.'], ['Apart from the datasets, we also developed word embeddings for the Armenian language, which we used in our experiments to train and evaluate named entity recognition algorithms. Considering their ability to capture semantic regularities, we used GloVe to train word embeddings. We assembled a dataset of Armenian texts containing 79 million tokens from the articles of Armenian Wikipedia, The Armenian Soviet Encyclopedia, a subcorpus of Eastern Armenian National Corpus BIBREF12 , over a dozen Armenian news websites and blogs. Included texts covered topics such as economics, politics, weather forecast, IT, law, society and politics, coming from non-fiction as well as fiction genres.', 'Similar to the original embeddings published for the English language, we release 50-, 100-, 200- and 300-dimensional word vectors for Armenian with a vocabulary size of 400000. Before training, all the words in the dataset were lowercased. For the final models we used the following training hyperparameters: 15 window size and 20 training epochs.'], [\"In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\"], ['Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .', \"spaCy 2.0 uses a CNN-based transition system for named entity recognition. For each token, a Bloom embedding is calculated based on its lowercase form, prefix, suffix and shape, then using residual CNNs, a contextual representation of that token is extracted that potentially draws information from up to 4 tokens from each side BIBREF17 . Each update of the transition system's configuration is a classification task that uses the contextual representation of the top token on the stack, preceding and succeeding tokens, first two tokens of the buffer, and their leftmost, second leftmost, rightmost, second rightmost children. The valid transition with the highest score is applied to the system. This approach reportedly performs within 1% of the current state-of-the-art for English . In our experiments, we tried out 50-, 100-, 200- and 300-dimensional pre-trained GloVe embeddings. Due to time constraints, we did not tune the rest of hyperparameters and used their default values.\", \"The main model that we focused on was the recurrent model with a CRF top layer, and the above-mentioned methods served mostly as baselines. The distinctive feature of this approach is the way contextual word embeddings are formed. For each token separately, to capture its word shape features, character-based representation is extracted using a bidirectional LSTM BIBREF18 . This representation gets concatenated with a distributional word vector such as GloVe, forming an intermediate word embedding. Using another bidirectional LSTM cell on these intermediate word embeddings, the contextual representation of tokens is obtained (Figure FIGREF17 ). Finally, a CRF layer labels the sequence of these contextual representations. In our experiments, we used Guillaume Genthial's implementation of the algorithm. We set the size of character-based biLSTM to 100 and the size of second biLSTM network to 300.\"], ['Experiments were carried out using IOB tagging scheme, with a total of 7 class tags: O, B-PER, I-PER, B-LOC, I-LOC, B-ORG, I-ORG.', 'We randomly selected 80% of generated annotated sentences for training and used the other 20% as a development set. The models with the best F1 score on the development set were tested on the manually annotated gold dataset.'], ['Table TABREF19 shows the average scores of evaluated models. The highest F1 score was achieved by the recurrent model using a batch size of 8 and Adam optimizer with an initial learning rate of 0.001. Updating word embeddings during training also noticeably improved the performance. GloVe word vector models of four different sizes (50, 100, 200, and 300) were tested, with vectors of size 50 producing the best results (Table TABREF20 ).', 'For spaCy 2.0 named entity recognizer, the same word embedding models were tested. However, in this case the performance of 200-dimensional embeddings was highest (Table TABREF21 ). Unsurprisingly, both deep learning models outperformed the feature-based Stanford recognizer in recall, the latter however demonstrated noticeably higher precision.', \"It is clear that the development set of automatically generated examples was not an ideal indicator of models' performance on gold-standard test set. Higher development set scores often led to lower test scores as seen in the evaluation results for spaCy 2.0 and Char-biLSTM+biLSTM+CRF (Tables TABREF21 and TABREF20 ). Analysis of errors on the development set revealed that many were caused by the incompleteness of annotations, when named entity recognizers correctly predicted entities that were absent from annotations (e.g. [ԽՍՀՄ-ի LOC] (USSR's), [Դինամոն ORG] (the_Dinamo), [Պիրենեյան թերակղզու LOC] (Iberian Peninsula's) etc). Similarly, the recognizers often correctly ignored non-entities that are incorrectly labeled in data (e.g. [օսմանների PER], [կոնսերվատորիան ORG] etc).\", 'Generally, tested models demonstrated relatively high precision of recognizing tokens that started named entities, but failed to do so with descriptor words for organizations and, to a certain degree, locations. The confusion matrix for one of the trained recurrent models illustrates that difference (Table TABREF22 ). This can be partly attributed to the quality of generated data: descriptor words are sometimes superfluously labeled (e.g. [Հավայան կղզիների տեղաբնիկները LOC] (the indigenous people of Hawaii)), which is likely caused by the inconsistent style of linking in Armenian Wikipedia (in the article ԱՄՆ մշակույթ (Culture of the United States), its linked text fragment \"Հավայան կղզիների տեղաբնիկները\" (\"the indigenous people of Hawaii\") leads to the article Հավայան կղզիներ(Hawaii)).'], ['We release two named-entity annotated datasets for the Armenian language: a silver-standard corpus for training NER models, and a gold-standard corpus for testing. It is worth to underline the importance of the latter corpus, as we aim it to serve as a benchmark for future named entity recognition systems designed for the Armenian language. Along with the corpora, we publish GloVe word vector models trained on a collection of Armenian texts.', 'Additionally, to establish the applicability of Wikipedia-based approaches for the Armenian language, we provide evaluation results for 3 different named entity recognition systems trained and tested on our datasets. The results reinforce the ability of deep learning approaches in achieving relatively high recall values for this specific task, as well as the power of using character-extracted embeddings alongside conventional word embeddings.', 'There are several avenues of future work. Since Nothman et al. 2013, more efficient methods of exploiting Wikipedia have been proposed, namely WiNER BIBREF19 , which could help increase both the quantity and quality of the training corpus. Another potential area of work is the further enrichment of the benchmark test set with additional annotation of other classes such as MISC or more fine-grained types (e.g. CITY, COUNTRY, REGION etc instead of LOC).']]},\n",
      "\tqas: {'question': ['what ner models were evaluated?', 'what is the source of the news sentences?', 'did they use a crowdsourcing platform for manual annotations?'], 'question_id': ['18c5d366b1da8447b5404eab71f4cc658ba12e6f', 'b5e4866f0685299f1d7af267bbcc4afe2aab806f', '1f085b9bb7bfd0d6c8cba1a9d73f08fcf2da7590'], 'nlp_background': ['', '', ''], 'topic_background': ['', '', ''], 'paper_read': ['', '', ''], 'search_query': ['', '', ''], 'question_writer': ['c1fbdd7a261021041f75fbe00a55b4c386ebbbb4', 'c1fbdd7a261021041f75fbe00a55b4c386ebbbb4', 'c1fbdd7a261021041f75fbe00a55b4c386ebbbb4'], 'answers': [{'answer': [{'unanswerable': False, 'extractive_spans': ['Stanford NER', 'spaCy 2.0 ', 'recurrent model with a CRF top layer'], 'yes_no': None, 'free_form_answer': '', 'evidence': [\"In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\", 'Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .', \"spaCy 2.0 uses a CNN-based transition system for named entity recognition. For each token, a Bloom embedding is calculated based on its lowercase form, prefix, suffix and shape, then using residual CNNs, a contextual representation of that token is extracted that potentially draws information from up to 4 tokens from each side BIBREF17 . Each update of the transition system's configuration is a classification task that uses the contextual representation of the top token on the stack, preceding and succeeding tokens, first two tokens of the buffer, and their leftmost, second leftmost, rightmost, second rightmost children. The valid transition with the highest score is applied to the system. This approach reportedly performs within 1% of the current state-of-the-art for English . In our experiments, we tried out 50-, 100-, 200- and 300-dimensional pre-trained GloVe embeddings. Due to time constraints, we did not tune the rest of hyperparameters and used their default values.\", \"The main model that we focused on was the recurrent model with a CRF top layer, and the above-mentioned methods served mostly as baselines. The distinctive feature of this approach is the way contextual word embeddings are formed. For each token separately, to capture its word shape features, character-based representation is extracted using a bidirectional LSTM BIBREF18 . This representation gets concatenated with a distributional word vector such as GloVe, forming an intermediate word embedding. Using another bidirectional LSTM cell on these intermediate word embeddings, the contextual representation of tokens is obtained (Figure FIGREF17 ). Finally, a CRF layer labels the sequence of these contextual representations. In our experiments, we used Guillaume Genthial's implementation of the algorithm. We set the size of character-based biLSTM to 100 and the size of second biLSTM network to 300.\"], 'highlighted_evidence': [\"In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\", 'Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .', 'spaCy 2.0 uses a CNN-based transition system for named entity recognition.', 'The main model that we focused on was the recurrent model with a CRF top layer, and the above-mentioned methods served mostly as baselines. ']}, {'unanswerable': False, 'extractive_spans': ['Stanford NER', 'spaCy 2.0', 'recurrent model with a CRF top layer'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .', \"spaCy 2.0 uses a CNN-based transition system for named entity recognition. For each token, a Bloom embedding is calculated based on its lowercase form, prefix, suffix and shape, then using residual CNNs, a contextual representation of that token is extracted that potentially draws information from up to 4 tokens from each side BIBREF17 . Each update of the transition system's configuration is a classification task that uses the contextual representation of the top token on the stack, preceding and succeeding tokens, first two tokens of the buffer, and their leftmost, second leftmost, rightmost, second rightmost children. The valid transition with the highest score is applied to the system. This approach reportedly performs within 1% of the current state-of-the-art for English . In our experiments, we tried out 50-, 100-, 200- and 300-dimensional pre-trained GloVe embeddings. Due to time constraints, we did not tune the rest of hyperparameters and used their default values.\", \"The main model that we focused on was the recurrent model with a CRF top layer, and the above-mentioned methods served mostly as baselines. The distinctive feature of this approach is the way contextual word embeddings are formed. For each token separately, to capture its word shape features, character-based representation is extracted using a bidirectional LSTM BIBREF18 . This representation gets concatenated with a distributional word vector such as GloVe, forming an intermediate word embedding. Using another bidirectional LSTM cell on these intermediate word embeddings, the contextual representation of tokens is obtained (Figure FIGREF17 ). Finally, a CRF layer labels the sequence of these contextual representations. In our experiments, we used Guillaume Genthial's implementation of the algorithm. We set the size of character-based biLSTM to 100 and the size of second biLSTM network to 300.\"], 'highlighted_evidence': ['Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .', 'spaCy 2.0 uses a CNN-based transition system for named entity recognition.', 'The main model that we focused on was the recurrent model with a CRF top layer, and the above-mentioned methods served mostly as baselines.']}], 'annotation_id': ['663e9d9f52cfad714f96f2c57d21313ef20f8add', '9508e7842dd5b534193606a055e7198868a37c64'], 'worker_id': ['5053f146237e8fc8859ed3984b5d3f02f39266b7', '258ee4069f740c400c0049a2580945a1cc7f044c']}, {'answer': [{'unanswerable': False, 'extractive_spans': ['ilur.am'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am. This dataset is comparable in size with the test sets of other languages (Table TABREF10 ). Included sentences are from political, sports, local and world news (Figures FIGREF8 , FIGREF9 ), covering the period between August 2012 and July 2018. The dataset provides annotations for 3 popular named entity classes: people (PER), organizations (ORG), and locations (LOC), and is released in CoNLL03 format with IOB tagging scheme. Tokens and sentences were segmented according to the UD standards for the Armenian language BIBREF11 .'], 'highlighted_evidence': ['In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am.']}, {'unanswerable': False, 'extractive_spans': ['links between Wikipedia articles to generate sequences of named-entity annotated tokens'], 'yes_no': None, 'free_form_answer': '', 'evidence': [\"We used Sysoev and Andrianov's modification of the Nothman et al. approach to automatically generate data for training a named entity recognizer. This approach uses links between Wikipedia articles to generate sequences of named-entity annotated tokens.\"], 'highlighted_evidence': ['This approach uses links between Wikipedia articles to generate sequences of named-entity annotated tokens.']}], 'annotation_id': ['743b68259cb1f5b6a5564d04833deb2306b2f5f8', 'd89dfc03629fbabdc7ef95980dba2cff62c10c31'], 'worker_id': ['5053f146237e8fc8859ed3984b5d3f02f39266b7', '258ee4069f740c400c0049a2580945a1cc7f044c']}, {'answer': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': ['In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am. This dataset is comparable in size with the test sets of other languages (Table TABREF10 ). Included sentences are from political, sports, local and world news (Figures FIGREF8 , FIGREF9 ), covering the period between August 2012 and July 2018. The dataset provides annotations for 3 popular named entity classes: people (PER), organizations (ORG), and locations (LOC), and is released in CoNLL03 format with IOB tagging scheme. Tokens and sentences were segmented according to the UD standards for the Armenian language BIBREF11 .', \"During annotation, we generally relied on categories and guidelines assembled by BBN Technologies for TREC 2002 question answering track. Only named entities corresponding to BBN's person name category were tagged as PER. Those include proper names of people, including fictional people, first and last names, family names, unique nicknames. Similarly, organization name categories, including company names, government agencies, educational and academic institutions, sports clubs, musical ensembles and other groups, hospitals, museums, newspaper names, were marked as ORG. However, unlike BBN, we did not mark adjectival forms of organization names as named entities. BBN's gpe name, facility name, location name categories were combined and annotated as LOC.\"], 'highlighted_evidence': ['In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am.', 'During annotation, we generally relied on categories and guidelines assembled by BBN Technologies for TREC 2002 question answering track.']}, {'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': [\"Instead of manually classifying Wikipedia articles as it was done in Nothman et al., we developed a rule-based classifier that used an article's Wikidata instance of and subclass of attributes to find the corresponding named entity type.\", 'In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am. This dataset is comparable in size with the test sets of other languages (Table TABREF10 ). Included sentences are from political, sports, local and world news (Figures FIGREF8 , FIGREF9 ), covering the period between August 2012 and July 2018. The dataset provides annotations for 3 popular named entity classes: people (PER), organizations (ORG), and locations (LOC), and is released in CoNLL03 format with IOB tagging scheme. Tokens and sentences were segmented according to the UD standards for the Armenian language BIBREF11 .'], 'highlighted_evidence': [\"Instead of manually classifying Wikipedia articles as it was done in Nothman et al., we developed a rule-based classifier that used an article's Wikidata instance of and subclass of attributes to find the corresponding named entity type.\", 'In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am.']}], 'annotation_id': ['291b81d304ad0fe71cbc1dfb4eba274ecdc7a6ad', 'e93b7506b18fc588212b273240aa0098f8ae352b'], 'worker_id': ['5053f146237e8fc8859ed3984b5d3f02f39266b7', '258ee4069f740c400c0049a2580945a1cc7f044c']}]},\n",
      "\tfigures_and_tables: {'caption': ['Fig. 1: Steps of automatic dataset extraction from Wikipedia', 'TABLE I: The mapping of Wikidata subclass of values to named entity types', 'Fig. 2: Topics distribution in the gold-standard corpus', 'Fig. 3: Distribution of examples by location in the goldstandard corpus', 'TABLE II: Comparison of Armenian, English, German, Spanish and Russian test sets: sentence, token, and named entity counts', 'Fig. 4: The neural architecture for extracting contextual representations of tokens', 'TABLE III: Evaluation results for named entity recognition algorithms', 'TABLE IV: Evaluation results for Char-biLSTM+biLSTM+CRF', 'TABLE V: Evaluation results for spaCy 2.0 NER', 'TABLE VI: Confusion matrix on the development set'], 'file': ['2-Figure1-1.png', '3-TableI-1.png', '3-Figure2-1.png', '3-Figure3-1.png', '3-TableII-1.png', '4-Figure4-1.png', '5-TableIII-1.png', '5-TableIV-1.png', '5-TableV-1.png', '5-TableVI-1.png']}\n",
      "}\n",
      "\n",
      "=== Example 3 ===\n",
      "{\n",
      "\tid: \"1609.00425\",\n",
      "\ttitle: \"Identifying Dogmatism in Social Media: Signals and Models\",\n",
      "\tabstract: \"We explore linguistic and behavioral features of dogmatism in social media and construct statistical models that can identify dogmatic comments. Our model is based on a corpus of Reddit posts, collected across a diverse set of conversational topics and annotated via paid crowdsourcing. We operationalize key aspects of dogmatism described by existing psychology theories (such as over-confidence), finding they have predictive power. We also find evidence for new signals of dogmatism, such as the tendency of dogmatic posts to refrain from signaling cognitive processes. When we use our predictive model to analyze millions of other Reddit posts, we find evidence that suggests dogmatism is a deeper personality trait, present for dogmatic users across many different domains, and that users who engage on dogmatic comments tend to show increases in dogmatic posts themselves.\",\n",
      "\tfull_text: {'section_name': ['Introduction', 'Dogmatism data', 'Approaches to Identifying Dogmatism', 'Predicting dogmatism', 'Dogmatism in the Reddit Community ', 'What subreddits have the highest and lowest levels of dogmatism? (R1)', 'How do dogmatic beliefs cluster? (R2)', 'What user behaviors are predictive of dogmatism? (R3)', 'How does dogmatism impact a conversation? (R4)', 'Related Work', 'Conclusion'], 'paragraphs': [[\"“I'm supposed to trust the opinion of a MS minion? The people that produced Windows ME, Vista and 8? They don't even understand people, yet they think they can predict the behavior of new, self-guiding AI?” –anonymous\", \"“I think an AI would make it easier for Patients to confide their information because by nature, a robot cannot judge them. Win-win? :D”' –anonymous\", \"Dogmatism describes the tendency to lay down opinions as incontrovertibly true, without respect for conflicting evidence or the opinions of others BIBREF0 . Which user is more dogmatic in the examples above? This question is simple for humans. Phrases like “they think” and “they don't even understand,” suggest an intractability of opinion, while “I think” and “win-win?” suggest the opposite. Can we train computers to draw similar distinctions? Work in psychology has called out many aspects of dogmatism that can be modeled computationally via natural language, such as over-confidence and strong emotions BIBREF1 .\", \"We present a statistical model of dogmatism that addresses two complementary goals. First, we validate psychological theories by examining the predictive power of feature sets that guide the model's predictions. For example, do linguistic signals of certainty help to predict a post is dogmatic, as theory would suggest? Second, we apply our model to answer four questions:\", 'R1: What kinds of topics (e.g., guns, LGBT) attract the highest levels of dogmatism?', 'R2: How do dogmatic beliefs cluster?', 'R3: How does dogmatism influence a conversation on social media? R4: How do other user behaviors (e.g., frequency and breadth of posts) relate to dogmatism?', 'We train a predictive model to classify dogmatic posts from Reddit, one of the most popular discussion communities on the web. Posts on Reddit capture discussion and debate across a diverse set of domains and topics – users talk about everything from climate change and abortion, to world news and relationship advice, to the future of artificial intelligence. As a prerequisite to training our model, we have created a corpus of 5,000 Reddit posts annotated with levels of dogmatism, which we are releasing to share with other researchers.', 'Using the model, we operationalize key domain-independent aspects of psychological theories of dogmatism drawn from the literature. We find these features have predictive power that largely supports the underlying theory. For example, posts that use less confident language tend to be less dogmatic. We also discover evidence for new attributes of dogmatism. For example, dogmatic posts tend not to verbalize cognition, through terms such as “I think,” “possibly,” or “might be.”', \"Our model is trained on only 5,000 annotated posts, but once trained, we use it to analyze millions of other Reddit posts to answer our research questions. We find a diverse set of topics are colored by dogmatic language (e.g., people are dogmatic about religion, but also about LGBT issues). Further, we find some evidence for dogmatism as a deeper personality trait – people who are strongly dogmatic about one topic are more likely to express dogmatic views about others as well. Finally, in conversation, we discover that one user's dogmatism tends to bring out dogmatism in their conversational partner, forming a vicious cycle.\"], ['Posts on Reddit capture debate and discussion across a diverse set of topics, making them a natural starting point for untangling domain-independent linguistic features of dogmatism.', 'Data collection. Subreddits are sub-communities on Reddit oriented around specific interests or topics, such as technology or politics. Sampling from Reddit as a whole would bias the model towards the most commonly discussed content. But by sampling posts from individual subreddits, we can control the kinds of posts we use to train our model. To collect a diverse training dataset, we have randomly sampled 1000 posts each from the subreddits politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage. All posts in our sample appeared between January 2007 and March 2015, and to control for length effects, contain between 300 and 400 characters. This results in a total training dataset of 5000 posts.', 'Dogmatism annotations. Building a useful computational model requires labeled training data. We labeled the Reddit dataset using crowdworkers on Amazon Mechanical Turk (AMT), creating the first public corpus annotated with levels of dogmatism. We asked crowdworkers to rate levels of dogmatism on a 5-point Likert scale, as supported by similar annotation tasks in prior work BIBREF2 . Concretely, we gave crowdworkers the following task: ', \"Given a comment, imagine you hold a well-informed, different opinion from the commenter in question. We'd like you to tell us how likely that commenter would be to engage you in a constructive conversation about your disagreement, where you each are able to explore the other's beliefs. The options are:\", \"(5): It's unlikely you'll be able to engage in any substantive conversation. When you respectfully express your disagreement, they are likely to ignore you or insult you or otherwise lower the level of discourse.\", '(4): They are deeply rooted in their opinion, but you are able to exchange your views without the conversation degenerating too much.', \"(3): It's not likely you'll be able to change their mind, but you're easily able to talk and understand each other's point of view.\", '(2): They may have a clear opinion about the subject, but would likely be open to discussing alternative viewpoints.', \"(1): They are not set in their opinion, and it's possible you might change their mind. If the comment does not convey an opinion of any kind, you may also select this option.\", 'To ensure quality work, we restricted the task to Masters workers and provided examples corresponding to each point on the scale. Including examples in a task has been shown to significantly increase the agreement and quality of crowdwork BIBREF3 . For instance, here is an example of a highly dogmatic (5) comment: ', \"I won't be happy until I see the executive suite of BofA, Wells, and all the others, frog-marched into waiting squad cars. It's ALREADY BEEN ESTABLISHED that...\", 'And a minimally dogmatic (1) comment: ', \"I agree. I would like to compile a playlist for us trance yogi's, even if you just would like to experiment with it. Is there any preference on which platform to use?\", 'Each comment has been annotated by three independent workers on AMT, which is enough to produce reliable results in most labeling tasks BIBREF4 . To compute an aggregate measure of dogmatism for each comment, we summed the scores of all three workers. We show the resulting distribution of annotations in Figure 1 .', \"Inter-annotator agreement. To evaluate the reliability of annotations we compute Krippendorff's $\\\\alpha $ , a measure of agreement designed for variable levels of measurement such as a Likert scale BIBREF5 . An $\\\\alpha $ of 0 indicates agreement indistinguishable from chance, while an $\\\\alpha $ of 1 indicates perfect agreement. Across all annotations we find $\\\\alpha =0.44$ . While workers agree much more than chance, clearly dogmatism is also subjective. In fact, when we examine only the middle two quartiles of the dogmatism annotations, we find agreement is no better than chance. Alternatively, when we measure agreement only among the top and bottom quartiles of annotations, we find agreement of $\\\\alpha =0.69$ . This suggests comments with scores that are only slightly dogmatic are unreliable and often subject to human disagreement. For this reason, we use only the top and bottom quartiles of comments when training our model.\"], [\"We now consider strategies for identifying dogmatism based on prior work in psychology. We start with the Linguistic Inquiry and Word Count (LIWC), a lexicon popular in the social sciences BIBREF6 . LIWC provides human validated lists of words that correspond to high-level psychological categories such as certainty or perception. In other studies, LIWC has uncovered linguistic signals relating to politeness BIBREF2 , deception BIBREF7 , or authority in texts BIBREF8 . Here, we examine how dogmatism relates to 17 of LIWC's categories (Table 1 ).\", 'To compute the relationships between LIWC categories and dogmatism, we first count the relevant category terms that appear in each annotated Reddit comment, normalized by its word count. We then calculate odds ratios on the aggregate counts of each LIWC category over the top and bottom quartiles of dogmatic comments. As we have discussed, using the top and bottom quartiles of comments provides a more reliable signal of dogmatism. We check for significant differences in categories between dogmatic and non-dogmatic comments using the Mann-Whitney U test and apply Holmes method for correction. All odds we report in this section are significant after correction.', 'Dogmatic statements tend to express a high degree of certainty BIBREF1 . Here we consider LIWC categories that express certainty both positively (certainty) and negatively (tentativeness). For example, the word “always” is certain, while “possibly” is tentative. Conforming to existing theory, certainty is more associated with dogmatic comments (1.52 odds), while tentativeness is more associated with the absence of dogmatism (0.88 odds).', \"Terms used to verbalize cognition can act as a hedge that often characterizes non-dogmatic language. LIWC's insight category captures this effect through words such as “think,” “know,” or “believe.” These words add nuance to a statement BIBREF9 , signaling it is the product of someone's mind (“I think you should give this paper a good review”) and not meant to be interpreted as an objective truth. Along these lines, we find the use of terms in the insight category is associated with non-dogmatic comments (0.83 odds).\", \"Sensory language, with its focus on description and detail, often signals a lack of any kind of opinion, dogmatic or otherwise. LIWC's perception category captures this idea through words associated with hearing, feeling, or seeing. For example, these words might occur when recounting a personal experience (“I saw his incoming fist”), which even if emotionally charged or negative, is less likely to be dogmatic. We find perception is associated with non-dogmatic comments at 0.77 odds.\", \"Drawing comparisons or qualifying something as relative to something else conveys a nuance that is absent from traditionally dogmatic language. The LIWC categories comparison and relativity capture these effects through comparison words such as “than” or “as” and qualifying words such as “during” or “when.” For example, the statement “I hate politicians” is more dogmatic than “I hate politicians when they can't get anything done.' Relativity is associated with non-dogmatic comments at 0.80 odds, but comparison does not reach significance.\", \"Pronouns can be surprisingly revealing indicators of language: for example, signaling one's gender or hierarchical status in a conversation BIBREF10 . We find first person singular pronouns are a useful negative signal for dogmatism (0.46 odds), while second person singular pronouns (2.18 odds) and third person plural (1.63 odds) are a useful positive signal. Looking across the corpus, we see I often used with a hedge (“I think” or “I know”), while you and they tend to characterize the beliefs of others, often in a strongly opinionated way (“you are a moron” or “they are keeping us down”). Other pronoun types do not show significant relationships.\", \"Like pronouns, verb tense can reveal subtle signals in language use, such as the tendency of medical inpatients to focus on the past BIBREF11 . On social media, comments written in the present tense are more likely to be oriented towards a user's current interaction (“this is all so stupid”), creating opportunities to signal dogmatism. Alternatively, comments in the past tense are more likely to refer to outside experiences (“it was an awful party”), speaking less to a user's stance towards an ongoing discussion. We find present tense is a positive signal for dogmatism (1.11 odds) and past tense is a negative signal (0.69 odds).\", \"Dogmatic language can be either positively or negatively charged in sentiment: for example, consider the positive statement “Trump is the SAVIOR of this country!!!” or the negative statement “Are you REALLY that stupid?? Education is the only way out of this horrible mess. It's hard to imagine how anyone could be so deluded.” In diverse communities, where people hold many different kinds of opinions, dogmatic opinions will often tend to come into conflict with one another BIBREF12 , producing a greater likelihood of negative sentiment. Perhaps for this reason, negative emotion (2.09 odds) and swearing (3.80 odds) are useful positive signals of dogmatism, while positive emotion shows no significant relationship.\", 'Finally, we find that interrogative language (1.12 odds) and negation (1.35 odds) are two additional positive signals of dogmatism. While interrogative words like “how” or “what” have many benign uses, they disproportionately appear in our data in the form of rhetorical or emotionally charged questions, such as “how can anyone be that dumb?”', 'Many of these linguistic signals are correlated with each other, suggesting that dogmatism is the cumulative effect of many component relationships. For example, consider the relatively non-dogmatic statement: “I think the reviewers are wrong in this instance.” Removing signals of insight, we have: “the reviewers are wrong in this instance,” which is slightly more dogmatic. Then removing relativity, we have: “the reviewers are wrong.” And finally, adding certainty, we have a dogmatic statement: “the reviewers are always wrong.”'], ['We now show how we can use the linguistic feature sets we have described to build a classifier that predicts dogmatism in comments. A predictive model further validates our feature sets, and also allows us to analyze dogmatism in millions of other Reddit comments in a scalable way, with multiple uses in ongoing, downstream analyses.', 'Prediction task. Our goal is (1) to understand how well we can use the strategies in Section 3 to predict dogmatism, and (2) to test the domain-independence of these strategies. First, we test the performance of our model under cross-validation within the Reddit comment dataset. We then evaluate the Reddit-based model on a held out corpus of New York Times comments annotated using the technique in Section 2. We did not refer to this second dataset during feature construction.', 'For classification, we consider two classes of comments: dogmatic and non-dogmatic. As in the prior analysis, we draw these comments from the top and bottom quartiles of the dogmatism distribution. This means the classes are balanced, with 2,500 total comments in the Reddit training data and 500 total comments in the New York Times testing data.', 'We compare the predictions of logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features. BOW and SENT provide baselines for the task. We compute BOW features using term frequency-inverse document frequency (TF-IDF) and category-based features by normalizing counts for each category by the number of words in each document. The BOW classifiers are trained with regularization (L2 penalties of 1.5).', 'Classification results. We present classification accuracy in Table 2 . BOW shows an AUC of 0.853 within Reddit and 0.776 on the held out New York Times comments. The linguistic features boost classification results within Reddit (0.881) and on the held out New York Times comments (0.791). While linguistic signals by themselves provide strong predictive power (0.801 AUC within domain), sentiment signals are much less predictive.', 'These results suggest that linguistic features inspired by prior efforts in psychology are useful for predicting dogmatism in practice and generalize across new domains.'], ['We now apply our dogmatism classifier to a larger dataset of posts, examining how dogmatic language shapes the Reddit community. Concretely, we apply the BOW+LING model trained on the full Reddit dataset to millions of new unannotated posts, labeling these posts with a probability of dogmatism according to the classifier (0=non-dogmatic, 1=dogmatic). We then use these dogmatism annotations to address four research questions.'], [\"A natural starting point for analyzing dogmatism on Reddit is to examine how it characterizes the site's sub-communities. For example, we might expect to see that subreddits oriented around topics such as abortion or climate change are more dogmatic, and subreddits about cooking are less so.\", 'To answer this question, we randomly sample 1.6 million posts from the entire Reddit community between 2007 and 2015. We then annotate each of these posts with dogmatism using our classifier, and compute the average dogmatism level for each subreddit in the sample with at least 100 posts.', 'We present the results of this analysis in Table 3 . The subreddits with the highest levels of dogmatism tend to be oriented around politics and religion (DebateAChristian or ukpolitics), while those with the lowest levels tend to focus on hobbies (photography or homebrewing). The subreddit with the highest average dogmatism level, cringepics, is a place to make fun of socially awkward messages, often from would-be romantic partners. Dogmatism here tends to take the form of “how could someone be that stupid” and is directed at the subject of the post, as opposed to other members of the community.', \"Similarly, SubredditDrama is a community where people come to talk about fights on the internet or social media. These fights are often then extended in discussion, for example: “If the best you can come up with is that something you did was legal, it's probably time to own up to being an ass.” The presence of this subreddit in our analysis provides a further sanity check that our model is capturing a robust signal of dogmatism.\"], ['Dogmatism is widely considered to be a domain-specific attitude (for example, oriented towards religion or politics) as opposed to a deeper personality trait BIBREF1 . Here we use Reddit as a lens to examine this idea more closely. Are users who are dogmatic about one topic likely to be dogmatic about others? Do clusters of dogmatism exist around particular topics? To find out, we examine the relationships between subreddits over which individual users are dogmatic. For example, if many users often post dogmatic comments on both the politics and Christianity subreddits, but less often on worldnews, that would suggest politics and Christianity are linked per a boost in likelihood of individuals being dogmatic in both.', \"We sample 1000 Reddit users who posted at least once a year between 2007 and 2015 to construct a corpus of 10 million posts that constitute their entire post history. We then annotate these posts using the classifier and compute the average dogmatism score per subreddit per user. For example, one user might have an average dogmatism level of 0.55 for the politics subreddit and 0.45 for the economics subreddit. Most users do not post in all subreddits, so we track only subreddits for which a user had posted at least 10 times. Any subreddits with an average dogmatism score higher than 0.50 we consider to be a user's dogmatic subreddits. We then count all pairs of these dogmatic subreddits. For example, 45 users have politics and technology among their dogmatic subreddits, so we consider politics and technology as linked 45 times. We compute the mutual information BIBREF13 between these links, which gives us a measure of the subreddits that are most related through dogmatism.\", 'We present the results of this analysis in Table 4 , choosing clusters that represent a diverse set of topics. For example, Libertarianism is linked through dogmatism to other political communities like Anarcho_Capitalism, ronpaul, or ukpolitics, as well as other topical subreddits like guns or economy. Similarly, people who are dogmatic in the business subreddit also tend to be dogmatic in subreddits for Bitcoin, socialism, and technology. Notably, when we apply the same mutual information analysis to links defined by subreddits posted in by the same user, we see dramatically different results. For example, the subreddits most linked to science through user posts are UpliftingNews, photoshopbattles, and firstworldanarchist, and millionairemakers.', 'Finally, we see less obvious connections between subreddits that suggest some people may be dogmatic by nature. For example, among the users who are dogmatic on politics, they are also disproportionately dogmatic on unrelated subreddits such as science ( $p<0.001$ ), technology ( $p<0.001$ ), IAmA ( $p<0.001$ ), and AskReddit ( $p<0.05$ ), with p-values computed under a binomial test.'], ['We have shown dogmatism is captured by many linguistic features, but can we discover other high-level user behaviors that are similarly predictive?', \"To find out, we compute metrics of user behavior using the data sample of 1000 users and 10 million posts described in Section 5.2. Specifically, we calculate (1) activity: a user's total number of posts, (2) breadth: the number of subreddits a user has posted in, (3) focus: the proportion of a user's posts that appear in the subreddit where they are most active, and (4) engagement: the average number of posts a user contributes to each discussion they engage in. We then fit these behavioral features to a linear regression model where we predict each user's average dogmatism level. Positive coefficients in this model are positively predictive of dogmatism, while negative coefficients are negatively predictive. We find this model is significantly predicitive of dogmatism ( $R^2=0.1$ , $p<0.001$ ), with all features reaching statistical significance ( $p<0.001$ ). Activity and focus are positively associated with dogmatism, while breadth and engagement are negatively associated (Table 5 ). Together, these results suggest dogmatic users tend to post frequently and in specific communities, but are not as inclined to continue to engage with a discussion, once it has begun.\"], ['How does interacting with a dogmatic comment impact a conversation? Are users able to shrug it off? Or do otherwise non-dogmatic users become more dogmatic themselves?', \"To answer this question, we sample 600,000 conversations triples from Reddit. These conversations consist of two people (A and B) talking, with the structure: A1 $\\\\rightarrow $ B $\\\\rightarrow $ A2. This allows us to measure the impact of B's dogmatism on A's response, while also controlling for the dogmatism level initially set by A. Concretely, we model the impact of dogmatism on these conversations through a linear regression. This model takes two features, the dogmatism levels of A1 and B, and predicts the dogmatism response of A2. If B's dogmatism has no effect on A's response, the coefficient that corresponds to B will not be significant in the model. Alternatively, if B's dogmatism does have some effect, it will be captured by the model's coefficient.\", \"We find the coefficient of the B feature in the model is positively associated with dogmatism ( $p<0.001$ ). In other words, engagement with a dogmatic comment tends to make a user more dogmatic themselves. This effect holds when we run the same model on data subsets consisting only of dogmatic or non-dogmatic users, and also when we conservatively remove all words used by B from A's response (i.e., controlling for quoting effects).\"], ['In contrast to the computational models we have presented, dogmatism is usually measured in psychology through survey scales, in which study participants answer questions designed to reveal underlying personality attributes BIBREF1 . Over time, these surveys have been updated BIBREF14 and improved to meet standards of psychometric validity BIBREF15 .', 'These surveys are often used to study the relationship between dogmatism and other psychological phenomena. For example, dogmatic people tend to show an increased tendency for confrontation BIBREF16 or moral conviction and religiosity BIBREF17 , and less likelihood of cognitive flexibility BIBREF18 , even among stereotypically non-dogmatic groups like atheists BIBREF19 . From a behavioral standpoint, dogmatic people solve problems differently, spending less time framing a problem and expressing more certainty in their solution BIBREF20 . Here we similarly examine how user behaviors on Reddit relate to a language model of dogmatism.', 'Ertel sought to capture dogmatism linguistically, though a small lexicon of words that correspond with high-level concepts like certainty and compromise dota. McKenny then used this dictionary to relate dogmatism to argument quality in student essays dogmatism-essays. Our work expands on this approach, applying supervised models based on a broader set of linguistic categories to identify dogmatism in text.', 'Other researchers have studied topics similar to dogmatism, such as signals of cognitive style in right-wing political thought BIBREF21 , the language used by trolls on social media BIBREF22 , or what makes for impartial language on twitter BIBREF23 . A similar flavor of work has examined linguistic models that capture politeness BIBREF2 , deception BIBREF24 , and authority BIBREF8 . We took inspiration from these models when constructing the feature sets in our work.', 'Finally, while we examine what makes an opinion dogmatic, other work has pushed further into the structure of arguments, for example classifying their justifications BIBREF25 , or what makes an argument likely to win BIBREF26 . Our model may allow future researchers to probe these questions more deeply.'], ['We have constructed the first corpus of social media posts annotated with dogmatism scores, allowing us to explore linguistic features of dogmatism and build a predictive model that analyzes new content. We apply this model to Reddit, where we discover behavioral predictors of dogmatism and topical patterns in the comments of dogmatic users.', 'Could we use this computational model to help users shed their dogmatic beliefs? Looking forward, our work makes possible new avenues for encouraging pro-social behavior in online communities. ']]},\n",
      "\tqas: {'question': ['what are the topics pulled from Reddit?', 'What predictive model do they build?'], 'question_id': ['b6ae8e10c6a0d34c834f18f66ab730b670fb528c', 'a87a009c242d57c51fc94fe312af5e02070f898b'], 'nlp_background': ['two', 'two'], 'topic_background': ['unfamiliar', 'unfamiliar'], 'paper_read': ['somewhat', 'somewhat'], 'search_query': ['social', 'social'], 'question_writer': ['50d8b4a941c26b89482c94ab324b5a274f9ced66', '50d8b4a941c26b89482c94ab324b5a274f9ced66'], 'answers': [{'answer': [{'unanswerable': False, 'extractive_spans': ['politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage. '], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Data collection. Subreddits are sub-communities on Reddit oriented around specific interests or topics, such as technology or politics. Sampling from Reddit as a whole would bias the model towards the most commonly discussed content. But by sampling posts from individual subreddits, we can control the kinds of posts we use to train our model. To collect a diverse training dataset, we have randomly sampled 1000 posts each from the subreddits politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage. All posts in our sample appeared between January 2007 and March 2015, and to control for length effects, contain between 300 and 400 characters. This results in a total training dataset of 5000 posts.'], 'highlighted_evidence': ['To collect a diverse training dataset, we have randomly sampled 1000 posts each from the subreddits politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage.']}, {'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'training data has posts from politics, business, science and other popular topics; the trained model is applied to millions of unannotated posts on all of Reddit', 'evidence': ['Data collection. Subreddits are sub-communities on Reddit oriented around specific interests or topics, such as technology or politics. Sampling from Reddit as a whole would bias the model towards the most commonly discussed content. But by sampling posts from individual subreddits, we can control the kinds of posts we use to train our model. To collect a diverse training dataset, we have randomly sampled 1000 posts each from the subreddits politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage. All posts in our sample appeared between January 2007 and March 2015, and to control for length effects, contain between 300 and 400 characters. This results in a total training dataset of 5000 posts.', 'We now apply our dogmatism classifier to a larger dataset of posts, examining how dogmatic language shapes the Reddit community. Concretely, we apply the BOW+LING model trained on the full Reddit dataset to millions of new unannotated posts, labeling these posts with a probability of dogmatism according to the classifier (0=non-dogmatic, 1=dogmatic). We then use these dogmatism annotations to address four research questions.'], 'highlighted_evidence': ['To collect a diverse training dataset, we have randomly sampled 1000 posts each from the subreddits politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage.', 'Concretely, we apply the BOW+LING model trained on the full Reddit dataset to millions of new unannotated posts, labeling these posts with a probability of dogmatism according to the classifier (0=non-dogmatic, 1=dogmatic).']}], 'annotation_id': ['294cb3efe84c256e1dbc602d1527da260d2306f6', '500690d7e0ef2e420f5614cef342a008281bc68f'], 'worker_id': ['1ba1b5b562aef9cd264cace5b7bdd46a7c065c0a', '02c984a519661853b8ece0a27ce665d854551f7c']}, {'answer': [{'unanswerable': False, 'extractive_spans': ['logistic regression models'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We compare the predictions of logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features. BOW and SENT provide baselines for the task. We compute BOW features using term frequency-inverse document frequency (TF-IDF) and category-based features by normalizing counts for each category by the number of words in each document. The BOW classifiers are trained with regularization (L2 penalties of 1.5).'], 'highlighted_evidence': ['We compare the predictions of logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features.']}, {'unanswerable': False, 'extractive_spans': ['logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features.'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We compare the predictions of logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features. BOW and SENT provide baselines for the task. We compute BOW features using term frequency-inverse document frequency (TF-IDF) and category-based features by normalizing counts for each category by the number of words in each document. The BOW classifiers are trained with regularization (L2 penalties of 1.5).'], 'highlighted_evidence': ['We compare the predictions of logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features.']}], 'annotation_id': ['a0fdd35b9537e29d4c6fe7dbdd6e810cdf97636a', 'b93c42d01594b0e8799f18b4c9476c78735f4d3a'], 'worker_id': ['45b212ff3348e2473d3e5504ca1200bcf85fcbf5', '1ba1b5b562aef9cd264cace5b7bdd46a7c065c0a']}]},\n",
      "\tfigures_and_tables: {'caption': ['Figure 1: We crowdsourced dogmatism labels for 5000 comments. The distribution is slightly skewed towards higher levels of dogmatism. For example, crowdworkers unanimously labeled 206 comments as highly dogmatic (5× 3 = 15), but only 47 as minimally dogmatic (1× 3 = 3).', 'Table 1: Linguistic features that capture high level psychological categories and their relationship with dogmatic comments. Strategy describes the psychological category. Odds describes the likelihood that a category will appear more often in a dogmatic comment (e.g., dogmatic comments are 2.18 times more likely to mention you-oriented phrases). Example illustrates a comment that matches the category. * indicates significance (p < 0.05) after correction with Holmes method.', 'Table 2: The AUC scores for dogmatism classifiers within and across domains. BOW (bag-of-words) and SENT (sentiment signals) are baselines, and LING uses the linguistic features from Table 1. We compute in-domain accuracy using 15-fold cross-validation on the Reddit dataset, and cross-domain accuracy by training on Reddit and evaluating on comments on articles from the New York Times. Chance AUC is 0.5.', 'Table 3: Subreddits with the highest and lowest dogmatism scores. Politics and religion are common themes among the most dogmatic subreddits, while hobbies (e.g., photography, homebrewing, buildapc) show the least dogmatism.', 'Table 4: Clusters of subreddits that share dogmatic users. For example, users who are dogmatic on the conspiracy subreddit (a place to discuss conspiracy theories) are also likely to be dogmatic on guns or occupywallstreet.', 'Table 5: User behavioral features that are positively and negatively associated with dogmatism. ↑ means the feature is positively predictive with dogmatism, and ↓ means the feature is negatively predictive. For example, the more subreddits a user posts in, the less likely they are to be dogmatic. All features are statistically significant (p < 0.001).'], 'file': ['2-Figure1-1.png', '4-Table1-1.png', '5-Table2-1.png', '6-Table3-1.png', '7-Table4-1.png', '7-Table5-1.png']}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataset = \"allenai/qasper\"\n",
    "data = load_dataset(dataset, split='validation', streaming=True)\n",
    "sample = list(data.take(3))\n",
    "\n",
    "print(F\"--------======== DATASET: {dataset} ========--------\")\n",
    "for i, example in enumerate(sample):\n",
    "    print(f\"\\n=== Example {i+1} ===\")\n",
    "    print_example(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing allenai/qasper, split: train...: 100%|██████████| 888/888 [00:00<00:00, 2511.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished! 2050 records saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing allenai/qasper, split: validation...: 100%|██████████| 281/281 [00:00<00:00, 2142.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished! 725 records saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing allenai/qasper, split: test...: 100%|██████████| 416/416 [00:00<00:00, 2055.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished! 1104 records saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def load_qasper_dataset(dataset_split, output_file):\n",
    "    dataset_name = \"allenai/qasper\"\n",
    "    data = load_dataset(dataset_name)\n",
    "    dataset = data[dataset_split]\n",
    "\n",
    "    processed_data = []\n",
    "    for item in tqdm(dataset, desc=f\"Processing {dataset_name}, split: {dataset_split}...\"):\n",
    "        title = item[\"title\"]\n",
    "        abstract = item[\"abstract\"]\n",
    "\n",
    "        for q_idx, question in enumerate(item[\"qas\"][\"question\"][:3]):\n",
    "            formatted_question = (\n",
    "                f\"You are given a paper:\\n\"\n",
    "                f\"Title: {title}\\n\"\n",
    "                f\"Abstract: {abstract}\\n\\n\"\n",
    "                f\"{question}\"\n",
    "            )\n",
    "            if len(formatted_question) > MAX_QUESTION_LENGTH:\n",
    "                continue\n",
    "\n",
    "            conversation = [\n",
    "                {\"content\": formatted_question, \"role\": \"user\"},\n",
    "            ]\n",
    "            \n",
    "            processed_row = {\n",
    "                \"question_id\": str(hash(formatted_question)),\n",
    "                \"tstamp\": str(datetime.now().timestamp()),\n",
    "                \"conversation_a\": conversation,\n",
    "                \"source\": dataset_name,\n",
    "                \"tag\": \"context_based\",\n",
    "                \"meta_info\": {\"id_in_dataset\": item[\"id\"]},\n",
    "            }\n",
    "            processed_data.append(processed_row)\n",
    "\n",
    "    df = pd.DataFrame(processed_data)\n",
    "    with open(output_file, 'wb') as f:\n",
    "        f.write(orjson.dumps(json.loads(df.to_json(orient='records'))))\n",
    "    print(f\"Finished! {len(df)} records saved\")\n",
    "\n",
    "# Example usage:\n",
    "load_qasper_dataset(\"train\", \"./qasper-train.json\")\n",
    "load_qasper_dataset(\"validation\", \"./qasper-valid.json\")\n",
    "load_qasper_dataset(\"test\", \"./qasper-test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIQA [Common sense] [Reasoning]\n",
    "\n",
    "https://huggingface.co/datasets/ybisk/piqa\n",
    "\n",
    "More or less common sense question answering. I will drop the options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f285940626d4620b2cfda7c15313c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b72b991724943408568620ff2bccc42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/815k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d655d775a9f402ea9ed521b48ef6d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/16113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac0f6aa571843e699151e1f25cfa3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3084 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc028337fe14bf5844e091ac7495d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1838 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['goal', 'sol1', 'sol2', 'label'],\n",
       "        num_rows: 16113\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['goal', 'sol1', 'sol2', 'label'],\n",
       "        num_rows: 3084\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['goal', 'sol1', 'sol2', 'label'],\n",
       "        num_rows: 1838\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"ybisk/piqa\", trust_remote_code=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ybisk/piqa, split: train...: 100%|██████████| 16113/16113 [00:00<00:00, 35307.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished! 15825 records saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ybisk/piqa, split: validation...: 100%|██████████| 1838/1838 [00:00<00:00, 36479.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished! 1803 records saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ybisk/piqa, split: test...: 100%|██████████| 3084/3084 [00:00<00:00, 36357.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished! 3030 records saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def load_piqa_dataset(dataset_split, output_file):\n",
    "    dataset_name = \"ybisk/piqa\"\n",
    "    data = load_dataset(dataset_name)\n",
    "    dataset = data[dataset_split]\n",
    "\n",
    "    processed_data = []\n",
    "    for item in tqdm(dataset, desc=f\"Processing {dataset_name}, split: {dataset_split}...\"):\n",
    "        answer = item[\"sol2\"] if item[\"label\"] else item[\"sol1\"]\n",
    "        conversation = [\n",
    "            {\"content\": item['goal'], \"role\": \"user\"},\n",
    "            {\"content\": answer, \"role\": \"assistant\"}\n",
    "        ]\n",
    "\n",
    "        if len(conversation[0][\"content\"]) > 200 or len(conversation[1][\"content\"]) > MAX_ANSWER_LENGTH:\n",
    "            continue\n",
    "        \n",
    "        processed_row = {\n",
    "            \"question_id\": str(hash(answer)),\n",
    "            \"tstamp\": str(datetime.now().timestamp()),\n",
    "            \"conversation_a\": conversation,\n",
    "            \"source\": dataset_name,\n",
    "            \"tag\": \"physical_reasoning\"\n",
    "        }\n",
    "        processed_data.append(processed_row)\n",
    "\n",
    "    df = pd.DataFrame(processed_data)\n",
    "    with open(output_file, 'wb') as f:\n",
    "        f.write(orjson.dumps(json.loads(df.to_json(orient='records'))))\n",
    "    print(f\"Finished! {len(df)} records saved\")\n",
    "\n",
    "load_piqa_dataset(\"train\", \"./piqa-train.json\")\n",
    "load_piqa_dataset(\"validation\", \"./piqa-valid.json\")\n",
    "load_piqa_dataset(\"test\", \"./piqa-test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI2 ARC [Reasoning] [School grade]\n",
    "https://huggingface.co/datasets/allenai/ai2_arc\n",
    "\n",
    "Simple school grade reasoning questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------======== DATASET: allenai/ai2_arc ========--------\n",
      "\n",
      "=== Example 1 ===\n",
      "{\n",
      "\tid: \"MCAS_2000_4_6\",\n",
      "\tquestion: \"Which technology was developed most recently?\",\n",
      "\tchoices: {'text': ['cellular telephone', 'television', 'refrigerator', 'airplane'], 'label': ['A', 'B', 'C', 'D']},\n",
      "\tanswerKey: \"A\"\n",
      "}\n",
      "\n",
      "=== Example 2 ===\n",
      "{\n",
      "\tid: \"Mercury_7057260\",\n",
      "\tquestion: \"A student hypothesizes that algae are producers. Which question will best help the student determine if this is correct?\",\n",
      "\tchoices: {'text': ['Do algae consume other organisms?', 'Which organisms consume algae?', 'Do algae use sunlight to make food?', 'Could an ecosystem survive without algae?'], 'label': ['A', 'B', 'C', 'D']},\n",
      "\tanswerKey: \"C\"\n",
      "}\n",
      "\n",
      "=== Example 3 ===\n",
      "{\n",
      "\tid: \"ACTAAP_2014_7_6\",\n",
      "\tquestion: \"Soccer players use their muscle systems to kick a ball into a goal. What organ system coordinates the muscles?\",\n",
      "\tchoices: {'text': ['The nervous system', 'The endocrine system', 'The respiratory system', 'The circulatory system'], 'label': ['A', 'B', 'C', 'D']},\n",
      "\tanswerKey: \"A\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = \"allenai/ai2_arc\"\n",
    "data = load_dataset(dataset, \"ARC-Easy\", split='validation', streaming=True)\n",
    "sample = list(data.take(3))\n",
    "\n",
    "print(F\"--------======== DATASET: {dataset} ========--------\")\n",
    "for i, example in enumerate(sample):\n",
    "    print(f\"\\n=== Example {i+1} ===\")\n",
    "    print_example(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing allenai/ai2_arc, split: train...: 2251it [00:00, 8568.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished! 2209 records saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing allenai/ai2_arc, split: validation...: 570it [00:00, 19151.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished! 557 records saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing allenai/ai2_arc, split: test...: 2376it [00:00, 26124.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished! 2322 records saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def load_ai2_arc_dataset(dataset_split, output_file):\n",
    "    dataset_name = \"allenai/ai2_arc\"\n",
    "    data = load_dataset(dataset_name, \"ARC-Easy\")\n",
    "    dataset = data[dataset_split]\n",
    "\n",
    "    processed_data = []\n",
    "    for id_, item in tqdm(enumerate(dataset), desc=f\"Processing {dataset_name}, split: {dataset_split}...\"):\n",
    "        formatted_question = item['question']\n",
    "\n",
    "        correct_label = item[\"answerKey\"]\n",
    "        correct_index = item[\"choices\"][\"label\"].index(correct_label)\n",
    "        correct_answer = item[\"choices\"][\"text\"][correct_index]\n",
    "\n",
    "        conversation = [\n",
    "            {\"content\": formatted_question, \"role\": \"user\"},\n",
    "            {\"content\": correct_answer, \"role\": \"assistant\"}\n",
    "        ]\n",
    "\n",
    "        if len(conversation[0][\"content\"]) > 300 or len(conversation[1][\"content\"]) > MAX_ANSWER_LENGTH:\n",
    "            continue\n",
    "\n",
    "        processed_row = {\n",
    "            \"question_id\": id_,\n",
    "            \"tstamp\": str(datetime.now().timestamp()),\n",
    "            \"conversation_a\": conversation,\n",
    "            \"tag\": \"multiple_choice\",\n",
    "            \"source\": dataset_name,\n",
    "            \"meta_info\": {\"id_in_dataset\": item[\"id\"]}\n",
    "        }\n",
    "        processed_data.append(processed_row)\n",
    "\n",
    "    df = pd.DataFrame(processed_data)\n",
    "    with open(output_file, 'wb') as f:\n",
    "        f.write(orjson.dumps(json.loads(df.to_json(orient='records'))))\n",
    "    print(f\"Finished! {len(df)} records saved\")\n",
    "\n",
    "load_ai2_arc_dataset(\"train\", \"./ai2_arc-train.json\")\n",
    "load_ai2_arc_dataset(\"validation\", \"./ai2_arc-valid.json\")\n",
    "load_ai2_arc_dataset(\"test\", \"./ai2_arc-test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infinity Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec16c315dcb841979e47e85f8ab2484d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/21.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetNotFoundError",
     "evalue": "Dataset 'BAAI/Infinity-Instruct' is a gated dataset on the Hub. You must be authenticated to access it.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the dataset (streaming mode to avoid downloading the full dataset)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m dataset_7M \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBAAI/Infinity-Instruct\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m7M\u001b[39m\u001b[38;5;124m'\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, streaming\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Take 10 samples and convert to a list for viewing\u001b[39;00m\n\u001b[1;32m      7\u001b[0m sample_10 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(dataset_7M\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m10\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:2062\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2057\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2058\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2059\u001b[0m )\n\u001b[1;32m   2061\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2062\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   2063\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m   2064\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   2065\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[1;32m   2066\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[1;32m   2067\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   2068\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m   2069\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m   2070\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[1;32m   2071\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   2072\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   2073\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m   2074\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[1;32m   2075\u001b[0m     _require_default_config_name\u001b[38;5;241m=\u001b[39mname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2076\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[1;32m   2077\u001b[0m )\n\u001b[1;32m   2079\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2080\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:1782\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1781\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1782\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1783\u001b[0m     path,\n\u001b[1;32m   1784\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1785\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m   1786\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[1;32m   1787\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[1;32m   1788\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[1;32m   1789\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1790\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[1;32m   1791\u001b[0m     _require_default_config_name\u001b[38;5;241m=\u001b[39m_require_default_config_name,\n\u001b[1;32m   1792\u001b[0m     _require_custom_configs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(config_kwargs),\n\u001b[1;32m   1793\u001b[0m )\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1795\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:1652\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt reach the Hugging Face Hub for dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (DataFilesNotFoundError, DatasetNotFoundError, EmptyDatasetError)):\n\u001b[0;32m-> 1652\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:1636\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1634\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m   1635\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Visit the dataset page at https://huggingface.co/datasets/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to ask for access.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1636\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1637\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1638\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\n\u001b[1;32m   1639\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRevision \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist for dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1640\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m: Dataset 'BAAI/Infinity-Instruct' is a gated dataset on the Hub. You must be authenticated to access it."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset (streaming mode to avoid downloading the full dataset)\n",
    "dataset_7M = load_dataset('BAAI/Infinity-Instruct', '7M', split='train', streaming=True)\n",
    "\n",
    "# Take 10 samples and convert to a list for viewing\n",
    "sample_10 = list(dataset_7M.take(10))\n",
    "\n",
    "# Print the samples\n",
    "for i, example in enumerate(sample_10):\n",
    "    print(f\"\\n=== Example {i+1} ===\")\n",
    "    print(example)\n",
    "    print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human-Like-DPO-Dataset\n",
    "[dataset](https://huggingface.co/datasets/HumanLLMs/Human-Like-DPO-Dataset)\n",
    "Basically casual conversations which are supposed to be evaluated based on the more realistic reply\n",
    "\n",
    "I decided... that it does not fit our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Example 1 ===\n",
      "😂 Ah, no I haven't! I'm dying to know, what's the meme about? Is it a funny cat or a ridiculous situation? Spill the beans! 🤣\n",
      "==============================\n",
      "\n",
      "=== Example 2 ===\n",
      "Oh, totally! 😄 I'm a sucker for a good ol' rock ballad. Give me some Bon Jovi any day of the week! \"Livin' on a Prayer\" is my go-to karaoke jam. There's just something about belting out \"Oh, we're halfway there!\" at the top of my lungs that gets me pumped up! 🎤 What about you, do you have a favorite karaoke song? 🎶\n",
      "==============================\n",
      "\n",
      "=== Example 3 ===\n",
      "😊 I'm actually a big fan of DIY projects! I'm not super skilled, but I love trying out new things and getting creative. I've tried my hand at painting, making jewelry, and even some woodworking (with lots of supervision, of course 😉). One time, I even attempted to make my own candles, and let's just say it was... an experience. The result was more \"rustic\" than \"refined,\" but it was fun!\n",
      "\n",
      "What about you? Are you crafty? Do you have a favorite DIY project or craft that you enjoy working on? 🎨💡\n",
      "==============================\n",
      "\n",
      "=== Example 4 ===\n",
      "Oh, man! I'm a total sucker for Italian food! There's just something about the combination of rich flavors, aromas, and carbs 😋 that gets me every time. Give me a steaming plate of spaghetti Bolognese, a slice (or three) of pizza, or a hearty serving of lasagna, and I'm in heaven! \n",
      "\n",
      "But, I'm also a huge fan of exploring different cuisines and trying new recipes. I recently got into cooking Indian food, and the complexity of spices and flavors has really got me hooked. Have you ever tried making naan from scratch? 🍞️ Game-changer!\n",
      "\n",
      "What about you? Do you have a go-to cuisine or dish that never fails to hit the spot? 🤔\n",
      "==============================\n",
      "\n",
      "=== Example 5 ===\n",
      "You know, I've always been fascinated by music, and I've been known to bust out some mean air guitar solos in my free time 🎸😂. But, if I'm being completely honest, I've always had a secret desire to learn how to play the harmonica. There's something about the bluesy, soulful sound that just speaks to me. I've even got a harmonica sitting on my desk, just waiting for me to finally take the leap and learn how to play it! 🎺 Do you have any hidden talents or skills that you'd like to share? 😊\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('HumanLLMs/Human-Like-DPO-Dataset', split='train', streaming=True)\n",
    "sample_10 = list(dataset.take(5))\n",
    "\n",
    "for i, example in enumerate(sample_10):\n",
    "    print(f\"\\n=== Example {i+1} ===\")\n",
    "    print(example['chosen'])\n",
    "    print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Example 1 ===\n",
      "{\n",
      "\tid: \"oasst1_5921\",\n",
      "\tmessages: [{'content': 'Create a snippet of Terraform HCL code that create an AWS autoscaling group, and an ALB in front to expose an application to internet.', 'role': 'user'}, {'content': 'Sure, here\\'s an example Terraform HCL code that creates an AWS Autoscaling Group and an Application Load Balancer to expose an application to the internet:\\n``` \\n# Configure the AWS provider\\nprovider \"aws\" {\\n  region = \"us-east-1\"\\n}\\n\\n# Create a security group to allow traffic to the ALB\\nresource \"aws_security_group\" \"alb_sg\" {\\n  name_prefix = \"alb_sg\"\\n  ingress {\\n    from_port = 80\\n    to_port = 80\\n    protocol = \"tcp\"\\n    cidr_blocks = [\"0.0.0.0/0\"]\\n  }\\n}\\n\\n# Create an ALB and target group\\nresource \"aws_lb\" \"alb\" {\\n  name               = \"example-alb\"\\n  internal           = false\\n  load_balancer_type = \"application\"\\n\\n  subnets = [\"subnet-12345678\", \"subnet-87654321\"]\\n\\n  security_groups = [aws_security_group.alb_sg.id]\\n\\n  tags = {\\n    Environment = \"production\"\\n  }\\n}\\n\\nresource \"aws_lb_target_group\" \"target_group\" {\\n  name        = \"example-target-group\"\\n  port        = 80\\n  protocol    = \"HTTP\"\\n  vpc_id      = \"vpc-12345678\"\\n\\n  health_check {\\n    path = \"/health\"\\n  }\\n\\n  tags = {\\n    Environment = \"production\"\\n  }\\n}\\n\\n# Create an autoscaling group\\nresource \"aws_launch_configuration\" \"launch_configuration\" {\\n  name_prefix   = \"example-lc-\"\\n  image_id      = \"ami-12345678\"\\n  instance_type = \"t2.micro\"\\n\\n  # Other settings here (e.g. security groups, userdata, etc.)\\n}\\n\\nresource \"aws_autoscaling_group\" \"autoscaling_group\" {\\n  name                      = \"example-asg\"\\n  launch_configuration      = aws_launch_configuration.launch_configuration.id\\n  target_group_arns         = [aws_lb_target_group.target_group.arn]\\n  health_check_type         = \"EC2\"\\n  min_size                  = 1\\n  max_size                  = 3\\n  desired_capacity          = 2\\n  vpc_zone_identifier       = [\"subnet-12345678\", \"subnet-87654321\"]\\n  termination_policies      = [\"Default\"]\\n  wait_for_capacity_timeout = \"10m\"\\n\\n  tags = {\\n    Environment = \"production\"\\n  }\\n}\\n``` \\nNote that you will need to customize the settings for your specific use case (e.g. specify your own AMI ID, VPC ID, subnet IDs, etc.). This is just an example to give you an idea of how to create an autoscaling group with an ALB in Terraform.', 'role': 'assistant'}],\n",
      "\tsource: \"ai2-adapt-dev/oasst1_converted\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 2 ===\n",
      "{\n",
      "\tid: \"oasst1_1096\",\n",
      "\tmessages: [{'content': '¿Por qué crees que cada año es más difícil tener una casa propia en comparación a décadas anteriores?', 'role': 'user'}, {'content': 'Existen varios factores que pueden contribuir a que cada año sea más difícil tener una casa propia en comparación con décadas anteriores. Algunos de los factores más importantes son:\\n- El aumento del precio de la vivienda: En muchas ciudades del mundo, el precio de la vivienda ha aumentado considerablemente en las últimas décadas, lo que hace que sea más difícil para muchas personas comprar una casa propia. Esto se debe a una combinación de factores, incluyendo la escasez de viviendas disponibles, el aumento de la demanda de viviendas, y el aumento del costo de la construcción.\\n- La falta de acceso a créditos: En algunos casos, las personas pueden tener dificultades para obtener un crédito hipotecario debido a la falta de historial crediticio o ingresos estables. Esto hace que sea más difícil para muchas personas comprar una casa propia.\\n- El aumento de la deuda estudiantil: Muchos jóvenes que buscan comprar una casa propia también enfrentan una gran deuda estudiantil, lo que puede dificultar el ahorro para un pago inicial o el cumplimiento de los requisitos de los prestamistas hipotecarios.\\n- Cambios demográficos: Las tendencias demográficas también pueden afectar el mercado de la vivienda. Por ejemplo, el aumento de las tasas de divorcio ha dado lugar a más hogares unipersonales, lo que puede dificultar a algunas personas pagar una hipoteca por su cuenta.', 'role': 'assistant'}],\n",
      "\tsource: \"ai2-adapt-dev/oasst1_converted\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 3 ===\n",
      "{\n",
      "\tid: \"oasst1_2492\",\n",
      "\tmessages: [{'content': 'Cuál es equivalente a una bombilla de 100w en led', 'role': 'user'}, {'content': 'La equivalencia entre una bombilla incandescente de 100 vatios y un LED depende de varios factores, como la eficiencia energética del LED y la cantidad de luz que emite. En general, los LED son mucho más eficientes energéticamente que las bombillas incandescentes, por lo que un LED de menor potencia puede ser suficiente para producir la misma cantidad de luz que una bombilla incandescente de 100 vatios.\\n\\nComo referencia, un LED de alrededor de 10-14 vatios puede producir la misma cantidad de luz que una bombilla incandescente de 100 vatios. Sin embargo, la mejor manera de determinar la equivalencia exacta es comparar la cantidad de lúmenes que emite el LED con la cantidad de lúmenes que emite la bombilla incandescente de 100 vatios.', 'role': 'assistant'}],\n",
      "\tsource: \"ai2-adapt-dev/oasst1_converted\"\n",
      "}\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('allenai/tulu-3-sft-mixture', split='train', streaming=True)\n",
    "sample_10 = list(dataset.take(3))\n",
    "\n",
    "for i, example in enumerate(sample_10):\n",
    "    print(f\"\\n=== Example {i+1} ===\")\n",
    "    print_example(example)\n",
    "    print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tulu-3-sft-mixture [Common sense] [Multi-language]\n",
    "[dataset](https://huggingface.co/datasets/allenai/tulu-3-sft-mixture)\n",
    "The Tulu 3 SFT mixture was used to train the Tulu 3 series of models and included itself 18 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing allenai/tulu-3-sft-mixture, split: train...: 9999it [00:08, 1221.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished! 4979 records saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def load_tulu3_sft_dataset(dataset_split, output_file):\n",
    "    dataset_name = \"allenai/tulu-3-sft-mixture\"\n",
    "    dataset = load_dataset(dataset_name, split=dataset_split, streaming=True)\n",
    "    processed_data = []\n",
    "    max_items = 10000\n",
    "    for item in tqdm(dataset, desc = f\"Processing {dataset_name}, split: {dataset_split}...\"):\n",
    "        max_items -= 1\n",
    "        if max_items == 0:\n",
    "            break\n",
    "        if len(item[\"messages\"][0]['content']) < MAX_QUESTION_LENGTH and len(item[\"messages\"][1]['content']) < MAX_ANSWER_LENGTH:\n",
    "            conversation = item[\"messages\"]\n",
    "\n",
    "            processed_row = {\n",
    "                \"question_id\": str(hash(item[\"messages\"][0]['content'])),\n",
    "                \"tstamp\": str(datetime.now().timestamp()),\n",
    "                \"conversation_a\": conversation,\n",
    "                \"source\": dataset_name,\n",
    "                \"meta_info\": {\n",
    "                    \"id_in_dataset\": item[\"id\"],\n",
    "                    \"original_dataset\": item[\"source\"],\n",
    "                    \"question_length\": len(item[\"messages\"][0]['content']),\n",
    "                },\n",
    "                \"tag\": \"knowledge_based\",\n",
    "            }\n",
    "            processed_data.append(processed_row)\n",
    "\n",
    "    df = pd.DataFrame(processed_data)\n",
    "    with open(output_file, 'wb') as f:\n",
    "        f.write(orjson.dumps(json.loads(df.to_json(orient='records'))))\n",
    "    print(f\"Finished! {len(df)} records saved\")\n",
    "\n",
    "load_tulu3_sft_dataset(\"train\", \"./tulu-3-sft-mixture-train.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------======== DATASET: qasper ========--------\n",
      "\n",
      "=== Example 1 ===\n",
      "{\n",
      "\tinput: \"Which Facebook pages did they look at?\",\n",
      "\tcontext: \"Introduction\n",
      "This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/\n",
      "In the spirit of the brevity of social m...roviding comments on draft versions of this paper.\",\n",
      "\tanswers: [\"FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney\", \"FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.\"],\n",
      "\tlength: 3411,\n",
      "\tdataset: \"qasper_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"b26174d8ec97e8d01421548d3a640e8382db6235eb506661\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 2 ===\n",
      "{\n",
      "\tinput: \"What type of latent context is used to predict instructor intervention?\",\n",
      "\tcontext: \"Introduction\n",
      "Massive Open Online Courses (MOOCs) have strived to bridge the social gap in higher education by bringing quality education from reputed universities to students at large. Such massive sc... work to better prediction on such longer threads.\",\n",
      "\tanswers: [\"the series of posts that trigger an intervention\"],\n",
      "\tlength: 3732,\n",
      "\tdataset: \"qasper_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"8e5e6c35689e17e9f50df76ada506a4c05be9ed081c883d4\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 3 ===\n",
      "{\n",
      "\tinput: \"What other evaluation metrics are looked at?\",\n",
      "\tcontext: \"Introduction\n",
      "Sarcasm is an intensive, indirect and complex construct that is often intended to express contempt or ridicule . Sarcasm, in speech, is multi-modal, involving tone, body-language and gest...students of IIT Bombay for their help and support.\",\n",
      "\tanswers: [\"F-score, Kappa\", \"Unanswerable\"],\n",
      "\tlength: 3544,\n",
      "\tdataset: \"qasper_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"ebfe29b14637ff245ca9117167c10d252b21c64aa484e871\"\n",
      "}\n",
      "==============================\n",
      "--------======== DATASET: multifieldqa_en ========--------\n",
      "\n",
      "=== Example 1 ===\n",
      "{\n",
      "\tinput: \"What may happen if the VR headset lenses are exposed to sunlight or strong light?\",\n",
      "\tcontext: \"'用户指南 * User Guide 02 CN 11 EN * 包装内含 使用前注意事项 快速引导 产品部件详情说明 操作说明 02 02 03 06 08 01 \n",
      "•本产品支持在系统设置中进行瞳距调节 , 调节时请务必注意，最小瞳距可能会碰触鼻梁。当您佩戴头盔后，您 “显示”中进行手动调节，请注意设置使用不合适的瞳距，可能会引起视觉重影或者眼睛疲劳。 可在“设置” ► •本产品“护眼模式”经德...s • More exciting features waiting for you 19 EN\n",
      "'\",\n",
      "\tanswers: [\"Exposure to sunlight or strong light may cause permanent yellow spot damage on the screen.\"],\n",
      "\tlength: 2188,\n",
      "\tdataset: \"multifieldqa_en_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"45e06726b160cfb851e2179017f90e37f4f4dec62f346e33\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 2 ===\n",
      "{\n",
      "\tinput: \"What was Hugh H. Goodwin's rank in the United States Navy?\",\n",
      "\tcontext: \"Hugh Hilton Goodwin (December 21, 1900 – February 25, 1980) was a decorated officer in the United States Navy with the rank of Vice Admiral. A veteran of both World Wars, he commanded escort carrier  ...arine commanders\n",
      "Recipients of the Legion of Merit\",\n",
      "\tanswers: [\"Vice Admiral.\"],\n",
      "\tlength: 2292,\n",
      "\tdataset: \"multifieldqa_en_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"e02f6a69d7b2a96a3aa6cd84a9189c2d552f6fb089f216e1\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 3 ===\n",
      "{\n",
      "\tinput: \"Who was Ralph Rokebye's brother?\",\n",
      "\tcontext: \"Rokebye, Ralph of Yorks, arm. Gloucester Hall, matric. 9 Nov., 1582, aged 15; student of Lincoln's Inn 1585. See Foster's Inns of Court Register.\n",
      "Rokebye, Ralph of Herts (? Yorks), gent. Broadgates Ha...c. See Foster's Index Eccl.\n",
      "Rose, Thomas s. Edw., \",\n",
      "\tanswers: [\"Sir Richard.\"],\n",
      "\tlength: 2952,\n",
      "\tdataset: \"multifieldqa_en_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"dee63735510f6958f4c5bf318421c9b9fac0bc8b3e341a4a\"\n",
      "}\n",
      "==============================\n",
      "--------======== DATASET: hotpotqa ========--------\n",
      "\n",
      "=== Example 1 ===\n",
      "{\n",
      "\tinput: \"Which city is under Jining, Kaiyuan, Liaoning or Yanzhou District?\",\n",
      "\tcontext: \"Passage 1:\n",
      "Kaiyuan, Liaoning\n",
      "Kaiyuan (simplified Chinese: 开原; traditional Chinese: 開原; pinyin: Kāiyuán; lit. 'Open Plains') is a county-level city in the northeast of Liaoning, People's Republic of Ch...icial website of Liaoning Experimental High School\",\n",
      "\tanswers: [\"Yanzhou\"],\n",
      "\tlength: 3745,\n",
      "\tdataset: \"hotpotqa_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"1dae9ec530a93f43f85bd215a5c0e6737dbcf226cd3541d8\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 2 ===\n",
      "{\n",
      "\tinput: \"Mawson  is an electoral district that includes the wine regional around which town 35 km south of Adelaide?\",\n",
      "\tcontext: \"Passage 1:\n",
      "Młyniska, Myślibórz County\n",
      "Młyniska [mwɨˈniska] (formerly German Mühlenfünftel) is a village in the administrative district of Gmina Dębno, within Myślibórz County, West Pomeranian Voivodes... of Pomerania.\n",
      "The village has a population of 93.\",\n",
      "\tanswers: [\"McLaren Vale\"],\n",
      "\tlength: 3669,\n",
      "\tdataset: \"hotpotqa_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"b2af3aefc3bc4f5ba5340eca87389e8dcaa6fa1834110aaf\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 3 ===\n",
      "{\n",
      "\tinput: \"Reima Karppinen won a silver medal in the double sculls at the Rowing Championships held in what German city?\",\n",
      "\tcontext: \"Passage 1:\n",
      "Andrew Campbell (rower)\n",
      "Andrew Campbell (born February 2, 1992) is an American rower. He graduated from Harvard University in 2014. He is a two time bronze medalist in the lightweight men's...r Hayden, Joe Findlay and Italian Daniele Danesin.\",\n",
      "\tanswers: [\"Oberschleißheim\"],\n",
      "\tlength: 2697,\n",
      "\tdataset: \"hotpotqa_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"8a8f9b9a86f187871fec7c7adf266d2f5e8a657933b690b1\"\n",
      "}\n",
      "==============================\n",
      "--------======== DATASET: 2wikimqa ========--------\n",
      "\n",
      "=== Example 1 ===\n",
      "{\n",
      "\tinput: \"Who is Hermann Ii, Count Palatine Of Lotharingia's paternal grandfather?\",\n",
      "\tcontext: \"Passage 1:\n",
      "Adolf I of Lotharingia\n",
      "Adolf I of Lotharingia, count of Keldachgau, Vogt of Deutz from 1008 until 1018, was the son of Hermann I \"Pusillus\" (the Little Pfalzgraf), count palatine of Lothari...1444 - ?) Nun in the Liebenau monastery near Worms\",\n",
      "\tanswers: [\"Hezzelin I\"],\n",
      "\tlength: 2521,\n",
      "\tdataset: \"2wikimqa_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"8c17c1259ed0c31bff3ee48ef8a6c4c54469c7769ffbc1c1\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 2 ===\n",
      "{\n",
      "\tinput: \"Where was the director of film Requiem For Dominic born?\",\n",
      "\tcontext: \"Passage 1:\n",
      "Brian Kennedy (gallery director)\n",
      "Brian Patrick Kennedy (born 5 November 1961) is an Irish-born art museum director who has worked in Ireland and Australia, and now lives and works in the Un...8) (documentary)\n",
      "The Doctor Blake Mysteries (2013)\",\n",
      "\tanswers: [\"Temesvár\"],\n",
      "\tlength: 2873,\n",
      "\tdataset: \"2wikimqa_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"52150ca076ef6a156239c8fd2cb45983ba9b8cd446312f22\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 3 ===\n",
      "{\n",
      "\tinput: \"What is the place of birth of Ibrahim Ibn Muhammad's mother?\",\n",
      "\tcontext: \"Passage 1:\n",
      "Marzuban ibn Muhammad ibn Shaddad\n",
      "Marzuban ibn Muhammad ibn Shaddad was a Kurdish ruler, the brother of Lashkari ibn Muhammad. He succeeded his brother to the throne of the Shaddadids in 97...ror Go-Uda.\n",
      "She was the mother of Prince Morinaga.\",\n",
      "\tanswers: [\"Egypt\"],\n",
      "\tlength: 3618,\n",
      "\tdataset: \"2wikimqa_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"61fa50e62866fbeeffc9822aa76f8f363578e72c1a7436af\"\n",
      "}\n",
      "==============================\n",
      "--------======== DATASET: gov_report ========--------\n",
      "\n",
      "=== Example 1 ===\n",
      "{\n",
      "\tinput: \"\",\n",
      "\tcontext: \"The design and development of information systems can be complex undertakings, consisting of a multitude of pieces of equipment and software products, and service providers. Each of the components of ...if you wish to reproduce this material separately.\",\n",
      "\tanswers: [\"IT systems are essential to the operations of the federal government. The supply chain—the set of organizations, people, activities, and resources that create and move a product from suppliers to end ...d partially implemented the eighth recommendation.\"],\n",
      "\tlength: 3092,\n",
      "\tdataset: \"gov_report_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"f91242d1926360ba71923587c1ba63e18d6a3ba749e6fbf6\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 2 ===\n",
      "{\n",
      "\tinput: \"\",\n",
      "\tcontext: \"From 2007 to 2015, HHS, DOL, and DOJ awarded funding to at least 2,586 grantees through at least 53 grant programs that were subject to statutory restrictions on religious-based hiring. Specifically: ...rina Taylor made key contributions to this report.\",\n",
      "\tanswers: [\"The federal government provides billions of dollars in grant funding to organizations offering social services, including FBOs. In carrying out their mission, some FBOs prefer to hire individuals who ...her approves them or provides a reason for denial.\"],\n",
      "\tlength: 2968,\n",
      "\tdataset: \"gov_report_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"9787f39937497170cf50f8309aec87d0972fda7e3642a99c\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 3 ===\n",
      "{\n",
      "\tinput: \"\",\n",
      "\tcontext: \"This report provides links to cybersecurity-related bills with some type of committee, floor, or chamber action (i.e., action subsequent to introduction). It also provides links to cybersecurity-relat... committee's website for that particular hearing .\",\n",
      "\tanswers: [\"Most major cybersecurity legislative provisions were enacted prior to 2002, despite many recommendations having been made over the past decade. More recently, in the 115th and 116th Congresses, cybers...ersecurity issues and the Senate held 40 hearings.\"],\n",
      "\tlength: 881,\n",
      "\tdataset: \"gov_report_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"6043ce523e2bf770ac7378c4b3db3611d3fec19e891d7470\"\n",
      "}\n",
      "==============================\n",
      "--------======== DATASET: multi_news ========--------\n",
      "\n",
      "=== Example 1 ===\n",
      "{\n",
      "\tinput: \"\",\n",
      "\tcontext: \"Passage 1:\n",
      "Image copyright Antarctic Gurkha Image caption Scott Sears arrived at the South Pole on Christmas Day NEWLINE_CHAR NEWLINE_CHAR Tennis star Andy Murray's brother-in-law believes he has beco...r nearly 6 weeks so it’s all a bit overwhelming”.\n",
      "\",\n",
      "\tanswers: [\"While many people were unwrapping gifts on Christmas Day, the brother-in-law of tennis star Andy Murray was wrapping up a 700-mile, 38-day journey through brutally cold temps and blistering winds—and,...prosthetic leg when he arrived at the South Pole.)\"],\n",
      "\tlength: 951,\n",
      "\tdataset: \"multi_news_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"3cfe6d5139ea2e17d17b9c8d040c015c3db4936578091cf6\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 2 ===\n",
      "{\n",
      "\tinput: \"\",\n",
      "\tcontext: \"Passage 1:\n",
      "LAPORTE, Ind. (WOOD) — A man convicted of four 1993 murders who escaped Sunday from a state prison outside of Ionia was captured in Indiana Monday evening. NEWLINE_CHAR NEWLINE_CHAR Michael...sing a firearm during the commission of a felony.\n",
      "\",\n",
      "\tanswers: [\"A convicted killer who escaped from a Michigan prison yesterday and carjacked a woman has been arrested after a car chase in rural LaPorte Country, Indiana, WNDU reports. Michael David Elliot, 40, all...y by slipping through a gap in the fencing system.\"],\n",
      "\tlength: 1950,\n",
      "\tdataset: \"multi_news_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"3cdb9ee2c7cd7b125c274234e29c319b831ab836bd40616e\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 3 ===\n",
      "{\n",
      "\tinput: \"\",\n",
      "\tcontext: \"Passage 1:\n",
      "Please enable Javascript to watch this video NEWLINE_CHAR NEWLINE_CHAR DENVER -- Officers pursued a stolen tractor towing an attachment through central Denver Friday night. The chase is bel... a police cruiser rammed it did the tractor stop.\n",
      "\",\n",
      "\tanswers: [\"Imagine looking outside and seeing over a dozen cop cars chasing ... a tractor. That was the scene in Denver last night when a stolen tractor towing an attachment smashed into cars and buildings as it...best. (Read about the easiest suspect chase ever.)\"],\n",
      "\tlength: 637,\n",
      "\tdataset: \"multi_news_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"c954dc7458c76e0d160906b15f13f8c19cba5c0a7b15ce12\"\n",
      "}\n",
      "==============================\n",
      "--------======== DATASET: trec ========--------\n",
      "\n",
      "=== Example 1 ===\n",
      "{\n",
      "\tinput: \"Question: What is nanotechnology ?\n",
      "Type:\",\n",
      "\tcontext: \"Question: What was the only country you were allowed to drive into Israel from in 1979 ?\n",
      "Type: Country\n",
      "Question: What was the name of the orca that died of a fungal infection ?\n",
      "Type: Animal\n",
      "Question: ...t struck the city of Dogtown in 1899 ?\n",
      "Type: Event\",\n",
      "\tanswers: [\"Definition of something\"],\n",
      "\tlength: 1093,\n",
      "\tdataset: \"trec_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: [\"Food\", \"Date\", \"Order, rank\", \"Speed\", \"Disease and medicine\", \"Word with a special property\", \"Abbreviation\", \"Language\", \"Letter like a-z\", \"Other entity\", \"Animal\", \"Expression abbreviated\", \"Price\", \"Techniques and method\", \"Musical instrument\", \"Mountain\", \"Currency name\", \"Event\", \"Product\", \"State\", \"Individual\", \"Organ of body\", \"Reason\", \"Manner of an action\", \"City\", \"Religion\", \"Invention, book and other creative piece\", \"Distance, linear measure\", \"Temperature\", \"Postcode or other code\", \"Size, area and volume\", \"Sport\", \"Country\", \"Other location\", \"Lasting time of somethin\", \"Equivalent term\", \"Description of something\", \"Weight\", \"Vehicle\", \"Color\", \"Other number\", \"Definition of something\", \"Element and substance\", \"Description of a person\", \"Symbols and sign\", \"Number of something\", \"Plant\", \"Percent, fraction\", \"Group or organization of person\", \"Title of a person\"],\n",
      "\t_id: \"37ada5a32aa71f2b906e18d53a6b0e82ab87ba4590b65583\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 2 ===\n",
      "{\n",
      "\tinput: \"Question: What day and month did John Lennon die ?\n",
      "Type:\",\n",
      "\tcontext: \"Question: What U.S. state is Dixville Notch in ?\n",
      "Type: State\n",
      "Question: Who is Archie Bunker 's son-in-law ?\n",
      "Type: Individual\n",
      "Question: How many languages are there in the world ?\n",
      "Type: Number of somet...ange call his profession ?\n",
      "Type: Title of a person\",\n",
      "\tanswers: [\"Date\"],\n",
      "\tlength: 3449,\n",
      "\tdataset: \"trec_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: [\"Food\", \"Date\", \"Order, rank\", \"Speed\", \"Disease and medicine\", \"Word with a special property\", \"Abbreviation\", \"Language\", \"Letter like a-z\", \"Other entity\", \"Animal\", \"Expression abbreviated\", \"Price\", \"Techniques and method\", \"Musical instrument\", \"Mountain\", \"Currency name\", \"Event\", \"Product\", \"State\", \"Individual\", \"Organ of body\", \"Reason\", \"Manner of an action\", \"City\", \"Religion\", \"Invention, book and other creative piece\", \"Distance, linear measure\", \"Temperature\", \"Postcode or other code\", \"Size, area and volume\", \"Sport\", \"Country\", \"Other location\", \"Lasting time of somethin\", \"Equivalent term\", \"Description of something\", \"Weight\", \"Vehicle\", \"Color\", \"Other number\", \"Definition of something\", \"Element and substance\", \"Description of a person\", \"Symbols and sign\", \"Number of something\", \"Plant\", \"Percent, fraction\", \"Group or organization of person\", \"Title of a person\"],\n",
      "\t_id: \"df03b10db69a1beab3fbdf1343e701db841fac48df1a98a2\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 3 ===\n",
      "{\n",
      "\tinput: \"Question: How many Admirals are there in the U.S. Navy ?\n",
      "Type:\",\n",
      "\tcontext: \"Question: Why is Chicago named Chicago ?\n",
      "Type: Reason\n",
      "Question: What did Lenny Bruce say that got him arrested ?\n",
      "Type: Description of something\n",
      "Question: What film did Louise Fletcher win the best act...rtainer '' ?\n",
      "Type: Group or organization of person\",\n",
      "\tanswers: [\"Number of something\"],\n",
      "\tlength: 3301,\n",
      "\tdataset: \"trec_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: [\"Food\", \"Date\", \"Order, rank\", \"Speed\", \"Disease and medicine\", \"Word with a special property\", \"Abbreviation\", \"Language\", \"Letter like a-z\", \"Other entity\", \"Animal\", \"Expression abbreviated\", \"Price\", \"Techniques and method\", \"Musical instrument\", \"Mountain\", \"Currency name\", \"Event\", \"Product\", \"State\", \"Individual\", \"Organ of body\", \"Reason\", \"Manner of an action\", \"City\", \"Religion\", \"Invention, book and other creative piece\", \"Distance, linear measure\", \"Temperature\", \"Postcode or other code\", \"Size, area and volume\", \"Sport\", \"Country\", \"Other location\", \"Lasting time of somethin\", \"Equivalent term\", \"Description of something\", \"Weight\", \"Vehicle\", \"Color\", \"Other number\", \"Definition of something\", \"Element and substance\", \"Description of a person\", \"Symbols and sign\", \"Number of something\", \"Plant\", \"Percent, fraction\", \"Group or organization of person\", \"Title of a person\"],\n",
      "\t_id: \"d737e335f7fd3d45fd9a07f23845711d21ce9dc038223ddf\"\n",
      "}\n",
      "==============================\n",
      "--------======== DATASET: triviaqa ========--------\n",
      "\n",
      "=== Example 1 ===\n",
      "{\n",
      "\tinput: \"Passage:\n",
      "Costermonger\n",
      "Costermonger, coster, or costard is a street seller of fruit and vegetables, in London and other British towns. Costermongers were ubiquitous in mid-Victorian England, and some a...\n",
      "Question:\n",
      "What does a costermonger sell?\n",
      "Answer:\n",
      "\",\n",
      "\tcontext: \"Passage:\n",
      "Billy Cotton\n",
      "William Edward \"Billy\" Cotton (6 May 1899 – 25 March 1969) was an English band leader and entertainer, one of the few whose orchestras survived the British dance band era. Cotton...unded on three sides by water?\n",
      "Answer:\n",
      "Penninsula\n",
      "\",\n",
      "\tanswers: [\"Fruiting\", \"Friut\", \"Fruiting Shrubs\", \"Prutas\", \"Bacca\", \"Nutritious fruits\", \"Simple fruit\", \"Culture of fruits\", \"Fruits\", \"Fleshy fruit\", \"Seed pod\", \"Pod types\", \"Fruit\", \"Fruity\"],\n",
      "\tlength: 3953,\n",
      "\tdataset: \"triviaqa_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"a6b0e6b9bf53f05d1569572cebd9359a0d4136a026613c7d\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 2 ===\n",
      "{\n",
      "\tinput: \"Passage:\n",
      "GORDON JACKSON, 66; WAS BUTLER IN SERIES 'UPSTAIRS ...\n",
      "GORDON JACKSON, 66; WAS BUTLER IN SERIES 'UPSTAIRS, DOWNSTAIRS' - The Boston Globe (Boston, MA) | HighBeam Research\n",
      "APA\n",
      "Associated Press...al series of ITV's Upstairs, Downstairs ?\n",
      "Answer:\n",
      "\",\n",
      "\tcontext: \"Passage:\n",
      "Bellini (cocktail)\n",
      "A Bellini cocktail is a mixture of Prosecco sparkling wine and peach purée or nectar. This cocktail originated in Venice, Italy.\n",
      "\n",
      "History\n",
      "\n",
      "The Bellini was invented sometime...or constituent of hair and nails?\n",
      "Answer:\n",
      "Keratin\n",
      "\",\n",
      "\tanswers: [\"Gordon Jackson (disambiguation)\", \"GORDON JACKSON\", \"Gordon Jackson\", \"Jackson, Gordon\"],\n",
      "\tlength: 2682,\n",
      "\tdataset: \"triviaqa_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"2a54de979ab1eec474e0411cd269f9600f54b1557608a94f\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 3 ===\n",
      "{\n",
      "\tinput: \"Passage:\n",
      "Banjo - definition of banjo by The Free Dictionary\n",
      "Banjo - definition of banjo by The Free Dictionary\n",
      "Banjo - definition of banjo by The Free Dictionary\n",
      "http://www.thefreedictionary.com/banjo...f parchment stretched over a metal frame?\n",
      "Answer:\n",
      "\",\n",
      "\tcontext: \"Passage:\n",
      "Raymond Francis\n",
      "Raymond Francis (6 October 1911 – 24 October 1987) was an English actor best known for his role as Detective Chief Superintendent Tom Lockhart in the Associated-Rediffusion de...Island in the Galapagos in 2012?\n",
      "Answer:\n",
      "Tortoise\n",
      "\",\n",
      "\tanswers: [\"Open back banjo\", \"Fingerpicking banjo\", \"Tenor banjo\", \"Electric banjo\", \"Banjer\", \"Five-string banjo\", \"Stealth Banjo\", \"Banjos\", \"Tenor Banjo\", \"Classic banjo\", \"Banjo string\", \"Banjo\", \"5-string banjo\"],\n",
      "\tlength: 2992,\n",
      "\tdataset: \"triviaqa_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"4eea3cbcc28d2e2b3588987ce428800a798ba798e2eb6f5d\"\n",
      "}\n",
      "==============================\n",
      "--------======== DATASET: samsum ========--------\n",
      "\n",
      "=== Example 1 ===\n",
      "{\n",
      "\tinput: \"Dialogue: Oscar: A coffee at Tristano's?\n",
      "Payne: Why not. in 15 mins?\n",
      "Oscar: let's make it half an hour ok?\n",
      "Payne: great, i'll be there\n",
      "Oscar: see you there\n",
      "Summary: \",\n",
      "\tcontext: \"Dialogue: Andrew: Hello Janny, is it still convenient for us to come and check your gas meter at 2.45 today?\n",
      "Janny: Hi Andrew, that's fine. \n",
      "Andrew: Thank you, we will see you then\n",
      "Summary: Andrew w... He will propose eventually because he loves her.\n",
      "\",\n",
      "\tanswers: [\"Payne and Oscar will have a coffee at Tristano's in half an hour.\"],\n",
      "\tlength: 1909,\n",
      "\tdataset: \"samsum_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"158b7db2e111643f66315da64d8459f61ad6dd62f5f24c2c\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 2 ===\n",
      "{\n",
      "\tinput: \"Dialogue: Steffen: Any room in any of the cars going to the infinity pool? Im more handicapped than usual since I twisted my ancle yesterday :(\n",
      "Irene: we can give you a lift. Don’t think the car can ...istas de olas?\n",
      "Ben: Yes! Vistas de olas\n",
      "Summary: \",\n",
      "\tcontext: \"Dialogue: Barb: Dina, tell me you are there.\n",
      "Dina: I am, what's up?\n",
      "Barb: I was just browsing the web and guess what?\n",
      "Dina: What?\n",
      "Barb: I'm asking you. Guess what I found.\n",
      "Dina: No idea. You've g...mary: Alex is seating in row 7, seats 12 and 13. \n",
      "\",\n",
      "\tanswers: [\"Steffen twisted his ankle yesterday and needs a lift to the infinity pool. Irene's car probably won't make it up the hill, so they'd have to park at the bottom and hike up. Mr.Budd should make it up t...ld make it up the hill since it's a 4-wheel drive.\"],\n",
      "\tlength: 3931,\n",
      "\tdataset: \"samsum_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"c80d4bf6971ba2e7a0b3a5affdaa50a01a03faa368077524\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 3 ===\n",
      "{\n",
      "\tinput: \"Dialogue: Rob: Hey there, what's up?\n",
      "Bob: Not much, watching the game. You?\n",
      "Rob: Same. Having a few people over.\n",
      "Rob: But the game is boring as fuck lol. That's why I'm writing\n",
      "Bob: Yeah, true tha...ou on weekend!\n",
      "Rob: yeah, see you then!\n",
      "Summary: \",\n",
      "\tcontext: \"Dialogue: Tessa Rick: Good morning, Mr Chards. I hope you have found the documents arranged the way you wanted them. The whiteboard in the conference room has been set as well, just the way you wanted...inn will take over some of the stuff for a while.\n",
      "\",\n",
      "\tanswers: [\"Rob and Bob are watching the game. Bob will run some errands on the weekend. Jim's birthday is next Wednesday. He might organize a meetup this weekend. Bob will see Rob on the weekend.\"],\n",
      "\tlength: 1714,\n",
      "\tdataset: \"samsum_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"66fe3774d3686f64c487b2b42c83421d10367deeb6bfed00\"\n",
      "}\n",
      "==============================\n",
      "--------======== DATASET: passage_count ========--------\n",
      "\n",
      "=== Example 1 ===\n",
      "{\n",
      "\tinput: \"\",\n",
      "\tcontext: \"Paragraph 1: Early Canadian court cases accepted expert opinions about PAS, used the term \"syndrome\" and concurred with Gardner's theory that only one parent was fully responsible for it. Gardner test... brothers, fathers and sons of marginalized women.\",\n",
      "\tanswers: [\"6\"],\n",
      "\tlength: 3615,\n",
      "\tdataset: \"passage_count_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"0358aa51ab38acc1b97405f4307203e982cbb19500435368\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 2 ===\n",
      "{\n",
      "\tinput: \"\",\n",
      "\tcontext: \"Paragraph 1: Two years later, on 29 September 1977, another attempted launch of an Intelsat (Intelsat IVA F-5) communications satellite took place on AC-43. Shortly after liftoff, abnormal temperature..., written for the Australian guitarist Zane Banks.\",\n",
      "\tanswers: [\"10\"],\n",
      "\tlength: 3667,\n",
      "\tdataset: \"passage_count_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"ec5670be0aea90e8e1a05d687158371f45f64a07f6177b69\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 3 ===\n",
      "{\n",
      "\tinput: \"\",\n",
      "\tcontext: \"Paragraph 1: Because of its high alcohol content, Bacardi 151 was particularly flammable among liquors. For this reason it was often used as the ignition source for \"flaming shots\" and even for at-hom...y good guess that Bacardi got sick of being sued.\"\",\n",
      "\tanswers: [\"4\"],\n",
      "\tlength: 1973,\n",
      "\tdataset: \"passage_count_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"1a276604ab10692c64480e83d99a53c121f3dca26d05e5ff\"\n",
      "}\n",
      "==============================\n",
      "--------======== DATASET: passage_retrieval_en ========--------\n",
      "\n",
      "=== Example 1 ===\n",
      "{\n",
      "\tinput: \"During a race, the starter mistakenly waved the green flag too early, causing a chain reaction accident involving 11 cars. One car had ignition problems and slowed down, causing other cars to shift to... the fire affected the visibility of some drivers.\",\n",
      "\tcontext: \"Paragraph 1: The story opens with an unknown narrator addressing a letter to someone referred to as \"V\". The narrator writes his letter from New York having obtained \"V\" 's address from a mutual acqua...ally to springs on the western side of the valley.\",\n",
      "\tanswers: [\"Paragraph 6\"],\n",
      "\tlength: 3446,\n",
      "\tdataset: \"passage_retrieval_en_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"f363b5cfe63d7095514d7273aaa2e31dbfa73f10a71fd7d8\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 2 ===\n",
      "{\n",
      "\tinput: \"In April 2009, during the filming of Season 6, Duane Chapman, also known as Dog the Bounty Hunter, was allegedly shot at by a fugitive named Hoang Nguyen in Colorado Springs. Chapman's crew, along wit...2011, with a trial date set for November 28, 2011.\",\n",
      "\tcontext: \"Paragraph 1: On April 21, 2009, during filming of Season 6, Chapman was allegedly shot at with a handgun while his crew, along with bail bondsman Bobby Brown, were attempting to arrest a fugitive name...spell at Swindon Town before his permanent switch.\",\n",
      "\tanswers: [\"Paragraph 1\"],\n",
      "\tlength: 3490,\n",
      "\tdataset: \"passage_retrieval_en_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"721b6ef6b4641a7517d4228be3a141ddb2e698348da36083\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 3 ===\n",
      "{\n",
      "\tinput: \"A bass reflex system is a type of loudspeaker design that has an opening called a port or vent, which allows air to move in and out of the enclosure. This opening resonates with the air inside the enc...s is still necessary to ensure good sound quality.\",\n",
      "\tcontext: \"Paragraph 1: On a train bound for Bombay from Surat, a Police Inspector questions a young man named Farhaan, who has just obtained his law degree, for his reason to go to Bombay, and whether he has an... for this switching to be continued indefinitely.)\",\n",
      "\tanswers: [\"Paragraph 3\"],\n",
      "\tlength: 3315,\n",
      "\tdataset: \"passage_retrieval_en_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"871d9417ba06b98b501a20f626a0744f6ba0713376630258\"\n",
      "}\n",
      "==============================\n",
      "--------======== DATASET: lcc ========--------\n",
      "\n",
      "=== Example 1 ===\n",
      "{\n",
      "\tinput: \"\",\n",
      "\tcontext: \"/**\n",
      " * \n",
      " */\n",
      "package com.gint.app.bisis4.client.editor.inventar;\n",
      "import java.awt.Component;\n",
      "import java.awt.Container;\n",
      "import java.awt.Dimension;\n",
      "import java.awt.FocusTraversalPolicy;\n",
      "import java.awt.G...\tprimerak.setOdeljenje(odeljenjePanel.getCode());\n",
      "\",\n",
      "\tanswers: [\"\t\t\tString invKnj = invKnjPanel.getCode();\"],\n",
      "\tlength: 451,\n",
      "\tdataset: \"lcc_e\",\n",
      "\tlanguage: \"java\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"5a3807e791e54f080c852b4a4ffaa104dc61429725efd608\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 2 ===\n",
      "{\n",
      "\tinput: \"\",\n",
      "\tcontext: \"#region License\n",
      "// Copyright (c) 2013, ClearCanvas Inc.\n",
      "// All rights reserved.\n",
      "// http://www.clearcanvas.ca\n",
      "//\n",
      "// This file is part of the ClearCanvas RIS/PACS open source project.\n",
      "//\n",
      "// The ClearCan...bCriteria.ContainsKey(\"GroupID\"))\n",
      "              {\n",
      "\",\n",
      "\tanswers: [\"                 SubCriteria[\"GroupID\"] = new SearchCondition<String>(\"GroupID\");\"],\n",
      "\tlength: 475,\n",
      "\tdataset: \"lcc_e\",\n",
      "\tlanguage: \"csharp\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"d1cdb285b1f1b73f8d637fde4adf331aa839bb2e31eb8599\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 3 ===\n",
      "{\n",
      "\tinput: \"\",\n",
      "\tcontext: \"/*\n",
      " * Handlers.cs - Implementation of the \"I18N.Common.Handlers\" class.\n",
      " *\n",
      " * Copyright (c) 2002  Southern Storm Software, Pty Ltd\n",
      " *\n",
      " * Permission is hereby granted, free of charge, to any person obt...\"gb2312\");\n",
      "\t\taliases.Add (\"GB2312_80\", \"gb2312\");\n",
      "\",\n",
      "\tanswers: [\"\t\taliases.Add (\"GBK\", \"gb2312\");\"],\n",
      "\tlength: 545,\n",
      "\tdataset: \"lcc_e\",\n",
      "\tlanguage: \"csharp\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"2332b1a0569df7051e4f8442139c2dbbdf7b27bf1ef7cd16\"\n",
      "}\n",
      "==============================\n",
      "--------======== DATASET: repobench-p ========--------\n",
      "\n",
      "=== Example 1 ===\n",
      "{\n",
      "\tinput: \"import waterwave.net.aio.define.AioServerDataDealer;\n",
      "import waterwave.proxy.router.ProxyRouter;\n",
      "import java.io.IOException;\n",
      "import java.net.SocketAddress;\n",
      "import java.nio.ByteBuffer;\n",
      "import java.... {\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t}\n",
      "\t\t\t\tb0.put(b1);\n",
      "\",\t\tcq.poll();\n",
      "\tcontext: \"src/main/java/waterwave/proxy/router/ProxyRouter.java\n",
      "public class ProxyRouter {\n",
      "\n",
      "\tpublic static void main(String[] args) {\n",
      "\t\t// TODO Auto-generated method stub\n",
      "\n",
      "\t}\n",
      "\n",
      "\tpublic static SocketAddres...\n",
      "\t\t// TODO Auto-generated method stub\n",
      "\n",
      "\t}\n",
      "\n",
      "}\n",
      "\",\n",
      "\"],swers: [\"\t\t\t\tBufferTools.returnBuffer(b1);\n",
      "\tlength: 2646,\n",
      "\tdataset: \"repobench-p_e\",\n",
      "\tlanguage: \"java\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"2f63d9a3d25c12812b10eb407a92d3744aaa94835463eb5b\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 2 ===\n",
      "{\n",
      "\tinput: \"from math import pi\n",
      "from PyQt5 import QtGui\n",
      "from PyQt5.QtCore import QLocale\n",
      "from PyQt5.QtWidgets import QComboBox, QLineEdit, QHBoxLayout, QTableWidget, QPushButton, QTableWidgetItem, QInputDialog\n",
      "fr...rameters\n",
      "\n",
      "\t\tself._caption_label = QLabel(caption)\n",
      "\",\n",
      "\tcontext: \"GUI/Widgets/SketchViewWidget.py\n",
      "class SketchViewWidget(QWidget):\n",
      "\tdef __init__(self, parent, sketch, document):\n",
      "\t\tQWidget.__init__(self, parent)\n",
      "\t\tself._doc = document\n",
      "\t\tself._sketch = sketch\n",
      "\t\tself.s...meters_object.make_type(standard_name, type_name)\n",
      "\",\n",
      "\tanswers: [\"\t\tself._caption_label.setMinimumWidth(100*gui_scale())\"],\n",
      "\tlength: 2012,\n",
      "\tdataset: \"repobench-p_e\",\n",
      "\tlanguage: \"python\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"4d2668d05f3f636a1bb6b0389a03b382f3f3e06b19dec51d\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 3 ===\n",
      "{\n",
      "\tinput: \"import re\n",
      "import struct\n",
      "from itertools import zip_longest\n",
      "from mutagen._tags import Tags\n",
      "from mutagen._util import DictProxy, convert_error, read_full\n",
      "from ._util import BitPaddedInt, unsynch, ID3Junk...:\n",
      "        config = ID3SaveConfig()\n",
      "\n",
      "    flags = 0\n",
      "\",\n",
      "\tcontext: \"mutagen/id3/_frames.py\n",
      "def _bytes2key(b):\n",
      "    def __init__(self, *args, **kwargs):\n",
      "    def __setattr__(self, name, value):\n",
      "    def _setattr(self, name, value):\n",
      "    def _to_other(self, other):\n",
      "    def ... return bytes(bytearray(b'\\xff').join(fragments))\n",
      "\",\n",
      "\tanswers: [\"    if isinstance(frame, TextFrame):\"],\n",
      "\tlength: 1608,\n",
      "\tdataset: \"repobench-p_e\",\n",
      "\tlanguage: \"python\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"22cc5edeea2a251a75ec8950fd9bd0000a1dc7ee084af4ce\"\n",
      "}\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"qasper\", \"multifieldqa_en\", \"hotpotqa\", \"2wikimqa\", \"gov_report\", \"multi_news\", \"trec\", \\\n",
    "            \"triviaqa\", \"samsum\", \"passage_count\", \"passage_retrieval_en\", \"lcc\", \"repobench-p\"]\n",
    "\n",
    "for dataset in datasets:\n",
    "    data = load_dataset('THUDM/LongBench', f\"{dataset}_e\", split='test', streaming=True)\n",
    "    sample_10 = list(data.take(3))\n",
    "\n",
    "    print(F\"--------======== DATASET: {dataset} ========--------\")\n",
    "    for i, example in enumerate(sample_10):\n",
    "        print(f\"\\n=== Example {i+1} ===\")\n",
    "        print_example(example)\n",
    "        print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------======== DATASET: qasper ========--------\n",
      "\n",
      "=== Example 1 ===\n",
      "{\n",
      "\tinput: \"Which Facebook pages did they look at?\",\n",
      "\tcontext: \"Introduction\n",
      "This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/\n",
      "In the spirit of the brevity of social m...roviding comments on draft versions of this paper.\",\n",
      "\tanswers: [\"FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney\", \"FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.\"],\n",
      "\tlength: 3411,\n",
      "\tdataset: \"qasper_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"b26174d8ec97e8d01421548d3a640e8382db6235eb506661\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 2 ===\n",
      "{\n",
      "\tinput: \"What type of latent context is used to predict instructor intervention?\",\n",
      "\tcontext: \"Introduction\n",
      "Massive Open Online Courses (MOOCs) have strived to bridge the social gap in higher education by bringing quality education from reputed universities to students at large. Such massive sc... work to better prediction on such longer threads.\",\n",
      "\tanswers: [\"the series of posts that trigger an intervention\"],\n",
      "\tlength: 3732,\n",
      "\tdataset: \"qasper_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"8e5e6c35689e17e9f50df76ada506a4c05be9ed081c883d4\"\n",
      "}\n",
      "==============================\n",
      "\n",
      "=== Example 3 ===\n",
      "{\n",
      "\tinput: \"What other evaluation metrics are looked at?\",\n",
      "\tcontext: \"Introduction\n",
      "Sarcasm is an intensive, indirect and complex construct that is often intended to express contempt or ridicule . Sarcasm, in speech, is multi-modal, involving tone, body-language and gest...students of IIT Bombay for their help and support.\",\n",
      "\tanswers: [\"F-score, Kappa\", \"Unanswerable\"],\n",
      "\tlength: 3544,\n",
      "\tdataset: \"qasper_e\",\n",
      "\tlanguage: \"en\",\n",
      "\tall_classes: None,\n",
      "\t_id: \"ebfe29b14637ff245ca9117167c10d252b21c64aa484e871\"\n",
      "}\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"qasper\"]\n",
    "\n",
    "example = None\n",
    "\n",
    "for dataset in datasets:\n",
    "    data = load_dataset('THUDM/LongBench', f\"{dataset}_e\", split='test', streaming=True)\n",
    "    sample_10 = list(data.take(3))\n",
    "\n",
    "    print(F\"--------======== DATASET: {dataset} ========--------\")\n",
    "    for i, example in enumerate(sample_10):\n",
    "        print(f\"\\n=== Example {i+1} ===\")\n",
    "        print_example(example)\n",
    "        print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing qasper/test: 224it [00:01, 118.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished! 39 records saved to ./long-qasper-test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing multifieldqa_en/test: 150it [00:01, 109.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished! 42 records saved to ./long-multifieldqa_en-test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hotpotqa/test: 300it [00:02, 109.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished! 33 records saved to ./long-hotpotqa-test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 2wikimqa/test: 300it [00:01, 150.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished! 28 records saved to ./long-2wikimqa-test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1000\n",
    "MAX_QUESTION_LENGTH = 4000\n",
    "MAX_ANSWER_LENGTH = 400\n",
    "\n",
    "def process_batch(batch, dataset_name, all_lengths):\n",
    "    processed_data = []\n",
    "    for item in batch:\n",
    "        input = item[\"input\"]\n",
    "        context = item[\"context\"]\n",
    "        dataset_source = item[\"dataset\"]\n",
    "\n",
    "        formatted_question = f\"Context: {context}\\nQuestion: {input}\"\n",
    "        all_lengths.append(len(formatted_question))\n",
    "        if len(formatted_question) > MAX_QUESTION_LENGTH:\n",
    "            if len(context) < MAX_QUESTION_LENGTH*4:\n",
    "                context = context[:(MAX_QUESTION_LENGTH - (len(formatted_question) - len(context)))]\n",
    "                formatted_question = f\"Context: {context}\\nQuestion: {input}\"\n",
    "            else:\n",
    "                continue\n",
    "        if not item[\"answers\"] or len(item[\"answers\"][0]) > MAX_ANSWER_LENGTH:\n",
    "            continue\n",
    "\n",
    "        conversation = [\n",
    "            {\"content\": formatted_question, \"role\": \"user\"},\n",
    "            {\"content\": item[\"answers\"][0], \"role\": \"assistant\"}\n",
    "        ]\n",
    "\n",
    "        processed_row = {\n",
    "            \"question_id\": str(hash(formatted_question)),\n",
    "            \"tstamp\": str(datetime.now().timestamp()),\n",
    "            \"conversation_a\": conversation,\n",
    "            \"source\": f\"LongBench/{dataset_name}_e\",\n",
    "            \"tag\": \"context_based\",\n",
    "            \"meta_info\": {\n",
    "                \"id_in_dataset\": item[\"_id\"],\n",
    "                \"original_dataset\": dataset_source,\n",
    "                \"question_length\": len(formatted_question),\n",
    "            },\n",
    "        }\n",
    "        processed_data.append(processed_row)\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "def load_longbench_dataset(dataset_name, dataset_split, output_file):\n",
    "    all_lengths = []\n",
    "    try:\n",
    "        dataset = load_dataset(\n",
    "            'THUDM/LongBench', \n",
    "            f\"{dataset_name}_e\", \n",
    "            split=dataset_split, \n",
    "            streaming=True\n",
    "        )\n",
    "\n",
    "        with open(output_file, 'wb') as f:\n",
    "            f.write(b'[\\n')\n",
    "\n",
    "        total_processed = 0\n",
    "        batch = []\n",
    "\n",
    "        for item in tqdm(dataset, desc=f\"Processing {dataset_name}/{dataset_split}\"):\n",
    "            batch.append(item)\n",
    "            \n",
    "            if len(batch) >= BATCH_SIZE:\n",
    "                processed_batch = process_batch(batch, dataset_name, all_lengths)\n",
    "                if processed_batch:\n",
    "                    save_batch(processed_batch, output_file, total_processed > 0)\n",
    "                    total_processed += len(processed_batch)\n",
    "                batch = []\n",
    "\n",
    "        if batch:\n",
    "            processed_batch = process_batch(batch, dataset_name, all_lengths)\n",
    "            if processed_batch:\n",
    "                save_batch(processed_batch, output_file, total_processed > 0)\n",
    "                total_processed += len(processed_batch)\n",
    "        #plt.hist(all_lengths)\n",
    "        #plt.show()\n",
    "\n",
    "        with open(output_file, 'ab') as f:\n",
    "            f.write(b'\\n]')\n",
    "\n",
    "        print(f\"Finished! {total_processed} records saved to {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {dataset_name}: {str(e)}\")\n",
    "\n",
    "def save_batch(batch, output_file, needs_comma):\n",
    "    \"\"\"Save a batch of records to the output file\"\"\"\n",
    "    json_batch = orjson.dumps(batch, option=orjson.OPT_INDENT_2)\n",
    "\n",
    "    with open(output_file, 'ab') as f:\n",
    "        if needs_comma:\n",
    "            f.write(b',\\n')\n",
    "        f.write(json_batch[1:-1])\n",
    "\n",
    "datasets = [\"qasper\", \"multifieldqa_en\", \"hotpotqa\", \"2wikimqa\"]\n",
    "for dataset_name in datasets:\n",
    "    load_longbench_dataset(dataset_name, \"test\", f\"./long-{dataset_name}-test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples conclusion:\n",
    "\n",
    "This project combines multiple high-quality QA datasets to create a diverse training mixture covering different question types and difficulty levels.\n",
    "\n",
    "### Dataset Summary\n",
    "\n",
    "| Dataset | Type | Characteristics | Samples | Source |\n",
    "|---------|------|-----------------|---------|--------|\n",
    "| [QA Expert Multi-Hop QA](https://huggingface.co/datasets/khaimaitien/qa-expert-multi-hop-qa-V1.0) | Knowledge-based | Difficult multi-hop questions | 1,200 (23.1%) | Original train: 11,322 |\n",
    "| [Qasper](https://huggingface.co/datasets/allenai/qasper) | Context-based | Research paper QA | 1,197 (23.0%) | Original train: 2,033 |\n",
    "| [PIQA](https://huggingface.co/datasets/ybisk/piqa) | Commonsense | Physical reasoning | 1,200 (23.1%) | Original train: 15,825 |\n",
    "| [AI2 ARC](https://huggingface.co/datasets/allenai/ai2_arc) | Reasoning | Science questions | 700 (13.5%) | Original train: 2,209 |\n",
    "| [Tulu 3 SFT Mixture](https://huggingface.co/datasets/allenai/tulu-3-sft-mixture) | Multi-domain | 18 combined datasets, multi-language | 800 (15.4%) | Sampled: 4,979 |\n",
    "| [LongBench](https://huggingface.co/datasets/THUDM/LongBench) | Long-context | Extended context QA | 142 (2.7%) | Sampled 142 |\n",
    "\n",
    "**Total samples:** ~36368 (train), ~4,633 (val), ~6,595 (test)\n",
    "\n",
    "### Sampling Strategy\n",
    "\n",
    "For balanced training, we used the following proportions:\n",
    "\n",
    "1. **Primary Distribution:**\n",
    "   - 23.0% Context-based (Qasper)\n",
    "   - 23.1% Knowledge-based (Multi-Hop QA)\n",
    "   - 23.1% Commonsense (PIQA)\n",
    "   - 13.5% Reasoning (AI2 ARC)\n",
    "   - 15.4% General SFT (Tulu-3)\n",
    "   - 2.7% Long-context eval (LongBench)\n",
    "\n",
    "2. **Quality Filtering:**\n",
    "   - Applied length constraints:\n",
    "     - Max question length: 8,000 chars\n",
    "     - Max answer length: 1,000 chars\n",
    "   - Removed malformed/empty answers\n",
    "\n",
    "3. **Special Considerations:**\n",
    "   - **LongBench:** Use all available test samples (142) for evaluation\n",
    "   - **Multi-Hop QA:** Valuable for hard questions - keep full training set\n",
    "   - **Qasper:** Important for academic writing understanding\n",
    "\n",
    "### Final dataset\n",
    "\n",
    "For a 5k sample training set that will be reduced down to 300-500:\n",
    "```python\n",
    "{\n",
    "    \"qasper\": 1197,           # 23.0% (context)\n",
    "    \"multi_hop_qa\": 1200,      # 23.1% (knowledge)\n",
    "    \"piqa\": 1200,              # 23.1% (commonsense)\n",
    "    \"ai2_arc\": 700,            # 13.5% (reasoning)\n",
    "    \"tulu_3_sft\": 800,         # 15.4% (general SFT)\n",
    "    \"longbench\": 142           # 2.7% (long-context eval)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing qasper...\n",
      "Added 1158 samples from qasper\n",
      "Processing multi-hop-qa...\n",
      "Added 1200 samples from multi-hop-qa\n",
      "Processing piqa...\n",
      "Added 1200 samples from piqa\n",
      "Processing allenai/ai2_arc...\n",
      "Added 700 samples from allenai/ai2_arc\n",
      "Processing tulu-3-sft-mixture...\n",
      "Added 800 samples from tulu-3-sft-mixture\n",
      "Processing LongBench...\n",
      "Added 142 samples from LongBench\n",
      "\n",
      "Finished! Created dataset with 5200 records\n",
      "Dataset composition:\n",
      "- qasper: 1197 samples (23.0%)\n",
      "- multi-hop-qa: 1200 samples (23.1%)\n",
      "- piqa: 1200 samples (23.1%)\n",
      "- allenai/ai2_arc: 700 samples (13.5%)\n",
      "- tulu-3-sft-mixture: 800 samples (15.4%)\n",
      "- LongBench: 142 samples (2.7%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import orjson\n",
    "import json\n",
    "import random\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "def load_and_sample_datasets(\n",
    "    file_paths: Union[str, List[str]], \n",
    "    n_samples: int, \n",
    "    random_state: int = 42,\n",
    "    longbench_mode: bool = False\n",
    ") -> List[Dict]:\n",
    "    random.seed(random_state)\n",
    "    \n",
    "    if isinstance(file_paths, str):\n",
    "        file_paths = [file_paths]\n",
    "    \n",
    "    all_records = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = orjson.loads(f.read())\n",
    "            all_records.extend(data)\n",
    "    \n",
    "    # Special case for LongBench - take all test samples\n",
    "    if longbench_mode:\n",
    "        return all_records\n",
    "\n",
    "    if len(all_records) <= n_samples:\n",
    "        return all_records\n",
    "    return random.sample(all_records, n_samples)\n",
    "\n",
    "def create_final_dataset(output_file: str = \"final_dataset.json\"):\n",
    "    sampling_config = {\n",
    "        \"qasper\": {\n",
    "            \"files\": [\"./qasper-train.json\", \"./qasper-valid.json\"],\n",
    "            \"n_samples\": 800 + (500 - 142),  # 1158 total\n",
    "            \"longbench\": False\n",
    "        },\n",
    "        \"multi-hop-qa\": {\n",
    "            \"files\": [\"./qa-expert-multi-hop-qa-train.json\", \"./qa-expert-multi-hop-qa-valid.json\"],\n",
    "            \"n_samples\": 1200,\n",
    "            \"longbench\": False\n",
    "        },\n",
    "        \"piqa\": {\n",
    "            \"files\": [\"./piqa-train.json\", \"./piqa-valid.json\"],\n",
    "            \"n_samples\": 1200,\n",
    "            \"longbench\": False\n",
    "        },\n",
    "        \"allenai/ai2_arc\": {\n",
    "            \"files\": [\"./ai2_arc-train.json\", \"./ai2_arc-valid.json\"],\n",
    "            \"n_samples\": 700,\n",
    "            \"longbench\": False\n",
    "        },\n",
    "        \"tulu-3-sft-mixture\": {\n",
    "            \"files\": [\"./tulu-3-sft-mixture-train.json\"],\n",
    "            \"n_samples\": 800,\n",
    "            \"longbench\": False\n",
    "        },\n",
    "        \"LongBench\": {\n",
    "            \"files\": [\n",
    "                \"./long-qasper-test.json\",\n",
    "                \"./long-multifieldqa_en-test.json\",\n",
    "                \"./long-hotpotqa-test.json\",\n",
    "                \"./long-2wikimqa-test.json\"\n",
    "            ],\n",
    "            \"n_samples\": 142,\n",
    "            \"longbench\": True\n",
    "        },\n",
    "        \n",
    "    }\n",
    "\n",
    "    final_data = []\n",
    "    for dataset_name, config in sampling_config.items():\n",
    "        print(f\"Processing {dataset_name}...\")\n",
    "        samples = load_and_sample_datasets(\n",
    "            file_paths=config[\"files\"],\n",
    "            n_samples=config[\"n_samples\"],\n",
    "            longbench_mode=config[\"longbench\"]\n",
    "        )\n",
    "        final_data.extend(samples)\n",
    "        print(f\"Added {len(samples)} samples from {dataset_name}\")\n",
    "\n",
    "    random.shuffle(final_data)\n",
    "\n",
    "    df = pd.DataFrame(final_data)\n",
    "    with open(output_file, 'wb') as f:\n",
    "        f.write(orjson.dumps(json.loads(df.to_json(orient='records'))))\n",
    "    \n",
    "    print(f\"\\nFinished! Created dataset with {len(df)} records\")\n",
    "    print(\"Dataset composition:\")\n",
    "    for dataset_name in sampling_config:\n",
    "        count = sum(1 for item in final_data if dataset_name in item[\"source\"])\n",
    "        print(f\"- {dataset_name}: {count} samples ({count/len(final_data)*100:.1f}%)\")\n",
    "\n",
    "create_final_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
