{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e294fac-976a-4ea1-8444-7ad94184ed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a8378d3f-6c44-4022-b126-c38e2bb20673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_questions(dir_path):\n",
    "    judgement_files = []\n",
    "    \n",
    "    all_questions = []\n",
    "    for file_path in os.listdir(dir_path):\n",
    "        if not file_path.endswith(\".jsonl\"):\n",
    "            continue\n",
    "        full_file_path = os.path.join(dir_path, file_path)\n",
    "        judgement_files.append(full_file_path)\n",
    "        data = pd.read_json(full_file_path, lines=True)\n",
    "        data_pd = data[[\"question_id\", \"model_1\", \"model_2\", \"judge\"]].copy()\n",
    "        data_pd.rename(columns={\"model_1\": \"model_2\", \"model_2\": \"model_1\"}, inplace=True)\n",
    "        data_pd[\"conv\"] = data[\"games\"].map(lambda x: x[1])\n",
    "        data_pd[\"id\"] = data.apply(\n",
    "            lambda row: f'{row[\"question_id\"]}_{row[\"model_2\"]}_{row[\"model_1\"]}', \n",
    "            axis=1\n",
    "        )\n",
    "        print(f\"Loading filepath: {file_path}, total questions: {data_pd.shape[0]}\")\n",
    "        all_questions.append(data_pd)\n",
    "\n",
    "    all_questions = pd.concat(all_questions, axis=0, ignore_index=True)\n",
    "    print(f\"Joined all questions: {all_questions.shape[0]}\")\n",
    "    return all_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4396f8bb-ee84-430d-90d6-416f3426ec2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading filepath: falcon-7b-base_falcon-7b-int8.jsonl, total questions: 395\n",
      "Loading filepath: falcon-7b-awq_falcon-7b-int4.jsonl, total questions: 395\n",
      "Loading filepath: falcon-7b-int8_falcon-7b-base.jsonl, total questions: 395\n",
      "Loading filepath: falcon-7b-base_falcon-7b-awq.jsonl, total questions: 395\n",
      "Loading filepath: falcon-7b-awq_falcon-7b-int8.jsonl, total questions: 395\n",
      "Loading filepath: falcon-7b-int8_falcon-7b-awq.jsonl, total questions: 395\n",
      "Loading filepath: falcon-7b-base_falcon-7b-int4.jsonl, total questions: 395\n",
      "Loading filepath: falcon-7b-int4_falcon-7b-awq.jsonl, total questions: 395\n",
      "Loading filepath: falcon-7b-awq_falcon-7b-base.jsonl, total questions: 395\n",
      "Loading filepath: falcon-7b-int8_falcon-7b-int4.jsonl, total questions: 395\n",
      "Loading filepath: falcon-7b-int4_falcon-7b-base.jsonl, total questions: 395\n",
      "Loading filepath: falcon-7b-int4_falcon-7b-int8.jsonl, total questions: 395\n",
      "Joined all questions: 4740\n"
     ]
    }
   ],
   "source": [
    "dir_path = \"data/brain-hard-v0.1/model_judgment/gpt-4o-falcon/\"\n",
    "all_questions = load_all_questions(dir_path)\n",
    "all_questions.to_json(\"data/brain-hard-v0.1/model_judgment/gpt-4o-falcon-family.json\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "42b0553c-6208-4e48-b122-bbd66bde4cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading filepath: qwen-2.5-7b-int4_qwen-2.5-7b-base.jsonl, total questions: 395\n",
      "Loading filepath: qwen-2.5-7b-base_qwen-2.5-7b-int4.jsonl, total questions: 395\n",
      "Loading filepath: qwen-2.5-7b-base_qwen-2.5-7b-int8.jsonl, total questions: 395\n",
      "Loading filepath: qwen-2.5-7b-int8_qwen-2.5-7b-int4.jsonl, total questions: 395\n",
      "Loading filepath: qwen-2.5-7b-int8_qwen-2.5-7b-awq.jsonl, total questions: 395\n",
      "Loading filepath: qwen-2.5-7b-int4_qwen-2.5-7b-awq.jsonl, total questions: 395\n",
      "Loading filepath: qwen-2.5-7b-awq_qwen-2.5-7b-base.jsonl, total questions: 395\n",
      "Loading filepath: qwen-2.5-7b-awq_qwen-2.5-7b-int8.jsonl, total questions: 395\n",
      "Loading filepath: qwen-2.5-7b-int4_qwen-2.5-7b-int8.jsonl, total questions: 395\n",
      "Loading filepath: qwen-2.5-7b-int8_qwen-2.5-7b-base.jsonl, total questions: 395\n",
      "Loading filepath: qwen-2.5-7b-base_qwen-2.5-7b-awq.jsonl, total questions: 395\n",
      "Loading filepath: qwen-2.5-7b-awq_qwen-2.5-7b-int4.jsonl, total questions: 395\n",
      "Joined all questions: 4740\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>old_question_id</th>\n",
       "      <th>model_2</th>\n",
       "      <th>model_1</th>\n",
       "      <th>judge</th>\n",
       "      <th>turns</th>\n",
       "      <th>question_id</th>\n",
       "      <th>category</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>qwen-2.5-7b-int4</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>[{'role': 'system', 'content': 'Please act as ...</td>\n",
       "      <td>0_qwen-2.5-7b-base_qwen-2.5-7b-int4</td>\n",
       "      <td>brain-hard-v0.1</td>\n",
       "      <td>Judge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>qwen-2.5-7b-int4</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>[{'role': 'system', 'content': 'Please act as ...</td>\n",
       "      <td>1_qwen-2.5-7b-base_qwen-2.5-7b-int4</td>\n",
       "      <td>brain-hard-v0.1</td>\n",
       "      <td>Judge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>qwen-2.5-7b-int4</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>[{'role': 'system', 'content': 'Please act as ...</td>\n",
       "      <td>2_qwen-2.5-7b-base_qwen-2.5-7b-int4</td>\n",
       "      <td>brain-hard-v0.1</td>\n",
       "      <td>Judge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>qwen-2.5-7b-int4</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>[{'role': 'system', 'content': 'Please act as ...</td>\n",
       "      <td>5_qwen-2.5-7b-base_qwen-2.5-7b-int4</td>\n",
       "      <td>brain-hard-v0.1</td>\n",
       "      <td>Judge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>qwen-2.5-7b-int4</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>[{'role': 'system', 'content': 'Please act as ...</td>\n",
       "      <td>7_qwen-2.5-7b-base_qwen-2.5-7b-int4</td>\n",
       "      <td>brain-hard-v0.1</td>\n",
       "      <td>Judge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   old_question_id           model_2           model_1             judge  \\\n",
       "0                0  qwen-2.5-7b-int4  qwen-2.5-7b-base  qwen-2.5-7b-base   \n",
       "1                1  qwen-2.5-7b-int4  qwen-2.5-7b-base  qwen-2.5-7b-base   \n",
       "2                2  qwen-2.5-7b-int4  qwen-2.5-7b-base  qwen-2.5-7b-base   \n",
       "3                5  qwen-2.5-7b-int4  qwen-2.5-7b-base  qwen-2.5-7b-base   \n",
       "4                7  qwen-2.5-7b-int4  qwen-2.5-7b-base  qwen-2.5-7b-base   \n",
       "\n",
       "                                               turns  \\\n",
       "0  [{'role': 'system', 'content': 'Please act as ...   \n",
       "1  [{'role': 'system', 'content': 'Please act as ...   \n",
       "2  [{'role': 'system', 'content': 'Please act as ...   \n",
       "3  [{'role': 'system', 'content': 'Please act as ...   \n",
       "4  [{'role': 'system', 'content': 'Please act as ...   \n",
       "\n",
       "                           question_id         category cluster  \n",
       "0  0_qwen-2.5-7b-base_qwen-2.5-7b-int4  brain-hard-v0.1   Judge  \n",
       "1  1_qwen-2.5-7b-base_qwen-2.5-7b-int4  brain-hard-v0.1   Judge  \n",
       "2  2_qwen-2.5-7b-base_qwen-2.5-7b-int4  brain-hard-v0.1   Judge  \n",
       "3  5_qwen-2.5-7b-base_qwen-2.5-7b-int4  brain-hard-v0.1   Judge  \n",
       "4  7_qwen-2.5-7b-base_qwen-2.5-7b-int4  brain-hard-v0.1   Judge  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_path = \"data/brain-hard-v0.1/model_judgment/qwen-2.5-7b-base/\"\n",
    "all_questions = load_all_questions(dir_path)\n",
    "all_questions = all_questions.rename(\n",
    "    columns = {\"question_id\": \"old_question_id\", \"id\": \"question_id\", \"conv\": \"turns\"}\n",
    ")\n",
    "all_questions[\"category\"] = \"brain-hard-v0.1\"\n",
    "all_questions[\"cluster\"] = \"Judge\"\n",
    "all_questions.to_json(\"data/brain-hard-v0.1/model_judgment/qwen-2.5-7b-family-questions.json\", orient='records', lines=True)\n",
    "all_questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4e109f1-51e6-4ad3-80da-82b28e321748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>specificity</th>\n",
       "      <th>domain_knowledge</th>\n",
       "      <th>complexity</th>\n",
       "      <th>problem_solving</th>\n",
       "      <th>creativity</th>\n",
       "      <th>technical_accuracy</th>\n",
       "      <th>real_world</th>\n",
       "      <th>context_complete</th>\n",
       "      <th>answer</th>\n",
       "      <th>tag</th>\n",
       "      <th>source</th>\n",
       "      <th>meta_info</th>\n",
       "      <th>prompt</th>\n",
       "      <th>default_answer</th>\n",
       "      <th>model_answer</th>\n",
       "      <th>row-id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Earthworm tunnels help plants by aerating the ...</td>\n",
       "      <td>multiple_choice</td>\n",
       "      <td>allenai/ai2_arc</td>\n",
       "      <td>{'id_in_dataset': 'OHAT_2009_5_9'}</td>\n",
       "      <td>Earthworms live underground in the soil. As th...</td>\n",
       "      <td>Earthworm tunnels loosen the soil so plant roo...</td>\n",
       "      <td>Earthworm tunnels help plants by aerating the ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>To temporarily seal a broken window, you can u...</td>\n",
       "      <td>physical_reasoning</td>\n",
       "      <td>ybisk/piqa</td>\n",
       "      <td>None</td>\n",
       "      <td>How can I temporarily seal a broken window?</td>\n",
       "      <td>Tape a sheet of heavy plastic over the hole.</td>\n",
       "      <td>To temporarily seal a broken window, you can u...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>To clean earbuds, gently remove any detachable...</td>\n",
       "      <td>physical_reasoning</td>\n",
       "      <td>ybisk/piqa</td>\n",
       "      <td>None</td>\n",
       "      <td>How to clean earbuds</td>\n",
       "      <td>Take a small, damp cloth or paper towel and ge...</td>\n",
       "      <td>To clean earbuds, gently remove any detachable...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>To remove the skin from tomatoes, blanch them ...</td>\n",
       "      <td>physical_reasoning</td>\n",
       "      <td>ybisk/piqa</td>\n",
       "      <td>None</td>\n",
       "      <td>To remove skin from tomatoes for cooking the p...</td>\n",
       "      <td>Blanch the tomato in boiling water followed by...</td>\n",
       "      <td>To remove the skin from tomatoes, blanch them ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>To make breakfast cheesecakes, you can follow ...</td>\n",
       "      <td>physical_reasoning</td>\n",
       "      <td>ybisk/piqa</td>\n",
       "      <td>None</td>\n",
       "      <td>how to make breakfast cheesecakes</td>\n",
       "      <td>Whisk 1/2 cup each softened cream cheese and l...</td>\n",
       "      <td>To make breakfast cheesecakes, you can follow ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   specificity  domain_knowledge  complexity  problem_solving  creativity  \\\n",
       "2            2                 3           2                2           2   \n",
       "3            2                 2           2                3           3   \n",
       "7            2                 2           2                2           2   \n",
       "8            1                 2           1                2           1   \n",
       "9            2                 2           2                2           3   \n",
       "\n",
       "   technical_accuracy  real_world  context_complete  \\\n",
       "2                   3           4                 3   \n",
       "3                   2           5                 1   \n",
       "7                   2           5                 1   \n",
       "8                   2           5                 1   \n",
       "9                   2           4                 1   \n",
       "\n",
       "                                              answer                 tag  \\\n",
       "2  Earthworm tunnels help plants by aerating the ...     multiple_choice   \n",
       "3  To temporarily seal a broken window, you can u...  physical_reasoning   \n",
       "7  To clean earbuds, gently remove any detachable...  physical_reasoning   \n",
       "8  To remove the skin from tomatoes, blanch them ...  physical_reasoning   \n",
       "9  To make breakfast cheesecakes, you can follow ...  physical_reasoning   \n",
       "\n",
       "            source                           meta_info  \\\n",
       "2  allenai/ai2_arc  {'id_in_dataset': 'OHAT_2009_5_9'}   \n",
       "3       ybisk/piqa                                None   \n",
       "7       ybisk/piqa                                None   \n",
       "8       ybisk/piqa                                None   \n",
       "9       ybisk/piqa                                None   \n",
       "\n",
       "                                              prompt  \\\n",
       "2  Earthworms live underground in the soil. As th...   \n",
       "3        How can I temporarily seal a broken window?   \n",
       "7                               How to clean earbuds   \n",
       "8  To remove skin from tomatoes for cooking the p...   \n",
       "9                  how to make breakfast cheesecakes   \n",
       "\n",
       "                                      default_answer  \\\n",
       "2  Earthworm tunnels loosen the soil so plant roo...   \n",
       "3       Tape a sheet of heavy plastic over the hole.   \n",
       "7  Take a small, damp cloth or paper towel and ge...   \n",
       "8  Blanch the tomato in boiling water followed by...   \n",
       "9  Whisk 1/2 cup each softened cream cheese and l...   \n",
       "\n",
       "                                        model_answer  row-id  \n",
       "2  Earthworm tunnels help plants by aerating the ...       2  \n",
       "3  To temporarily seal a broken window, you can u...       3  \n",
       "7  To clean earbuds, gently remove any detachable...       7  \n",
       "8  To remove the skin from tomatoes, blanch them ...       8  \n",
       "9  To make breakfast cheesecakes, you can follow ...       9  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_questions = pd.read_json(\"datasets/final_dataser_filtered-v1.json\")\n",
    "processed_questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7810b383-aad1-4053-a94c-a76f0a7dd32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "falcon-7b-base\t\t   gpt-4o-qwen\t\t\t qwen-7b-markup-gpt\n",
      "falcon-7b-markup-gpt\t   gpt-4o-qwen-7b-family-2.json  qwen-7b-markup-gpt-2\n",
      "gpt-4o-falcon\t\t   gpt-4o-qwen-7b-family.json\n",
      "gpt-4o-falcon-family.json  qwen-2.5-7b-base\n"
     ]
    }
   ],
   "source": [
    "!ls data/brain-hard-v0.1/model_judgment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2db5902a-28af-45bf-a038-320d5cf6bcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading filepath: falcon-7b-base_falcon-7b-int8.jsonl, total questions: 395\n",
      "Loading filepath: falcon-7b-awq_falcon-7b-int4.jsonl, total questions: 395\n",
      "Loading filepath: falcon-7b-int8_falcon-7b-base.jsonl, total questions: 395\n",
      "Loading filepath: falcon-7b-base_falcon-7b-awq.jsonl, total questions: 395\n",
      "Loading filepath: falcon-7b-awq_falcon-7b-int8.jsonl, total questions: 395\n",
      "Loading filepath: falcon-7b-int8_falcon-7b-awq.jsonl, total questions: 395\n",
      "Loading filepath: falcon-7b-base_falcon-7b-int4.jsonl, total questions: 395\n",
      "Loading filepath: falcon-7b-int4_falcon-7b-awq.jsonl, total questions: 395\n",
      "Loading filepath: falcon-7b-awq_falcon-7b-base.jsonl, total questions: 395\n",
      "Loading filepath: falcon-7b-int8_falcon-7b-int4.jsonl, total questions: 395\n",
      "Loading filepath: falcon-7b-int4_falcon-7b-base.jsonl, total questions: 395\n",
      "Loading filepath: falcon-7b-int4_falcon-7b-int8.jsonl, total questions: 395\n",
      "Joined all questions: 4740\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>old_question_id</th>\n",
       "      <th>model_2</th>\n",
       "      <th>model_1</th>\n",
       "      <th>judge</th>\n",
       "      <th>turns</th>\n",
       "      <th>question_id</th>\n",
       "      <th>category</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "      <td>falcon-7b-int8</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "      <td>[{'role': 'system', 'content': 'Please act as ...</td>\n",
       "      <td>0_falcon-7b-int8_falcon-7b-base</td>\n",
       "      <td>brain-hard-v0.1</td>\n",
       "      <td>Judge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "      <td>falcon-7b-int8</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "      <td>[{'role': 'system', 'content': 'Please act as ...</td>\n",
       "      <td>1_falcon-7b-int8_falcon-7b-base</td>\n",
       "      <td>brain-hard-v0.1</td>\n",
       "      <td>Judge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "      <td>falcon-7b-int8</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "      <td>[{'role': 'system', 'content': 'Please act as ...</td>\n",
       "      <td>2_falcon-7b-int8_falcon-7b-base</td>\n",
       "      <td>brain-hard-v0.1</td>\n",
       "      <td>Judge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "      <td>falcon-7b-int8</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "      <td>[{'role': 'system', 'content': 'Please act as ...</td>\n",
       "      <td>5_falcon-7b-int8_falcon-7b-base</td>\n",
       "      <td>brain-hard-v0.1</td>\n",
       "      <td>Judge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "      <td>falcon-7b-int8</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "      <td>[{'role': 'system', 'content': 'Please act as ...</td>\n",
       "      <td>7_falcon-7b-int8_falcon-7b-base</td>\n",
       "      <td>brain-hard-v0.1</td>\n",
       "      <td>Judge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   old_question_id         model_2         model_1           judge  \\\n",
       "0                0  falcon-7b-base  falcon-7b-int8  falcon-7b-base   \n",
       "1                1  falcon-7b-base  falcon-7b-int8  falcon-7b-base   \n",
       "2                2  falcon-7b-base  falcon-7b-int8  falcon-7b-base   \n",
       "3                5  falcon-7b-base  falcon-7b-int8  falcon-7b-base   \n",
       "4                7  falcon-7b-base  falcon-7b-int8  falcon-7b-base   \n",
       "\n",
       "                                               turns  \\\n",
       "0  [{'role': 'system', 'content': 'Please act as ...   \n",
       "1  [{'role': 'system', 'content': 'Please act as ...   \n",
       "2  [{'role': 'system', 'content': 'Please act as ...   \n",
       "3  [{'role': 'system', 'content': 'Please act as ...   \n",
       "4  [{'role': 'system', 'content': 'Please act as ...   \n",
       "\n",
       "                       question_id         category cluster  \n",
       "0  0_falcon-7b-int8_falcon-7b-base  brain-hard-v0.1   Judge  \n",
       "1  1_falcon-7b-int8_falcon-7b-base  brain-hard-v0.1   Judge  \n",
       "2  2_falcon-7b-int8_falcon-7b-base  brain-hard-v0.1   Judge  \n",
       "3  5_falcon-7b-int8_falcon-7b-base  brain-hard-v0.1   Judge  \n",
       "4  7_falcon-7b-int8_falcon-7b-base  brain-hard-v0.1   Judge  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_path = \"data/brain-hard-v0.1/model_judgment/falcon-7b-base/\"\n",
    "all_questions = load_all_questions(dir_path)\n",
    "all_questions = all_questions.rename(\n",
    "    columns = {\"question_id\": \"old_question_id\", \"id\": \"question_id\", \"conv\": \"turns\"}\n",
    ")\n",
    "all_questions[\"category\"] = \"brain-hard-v0.1\"\n",
    "all_questions[\"cluster\"] = \"Judge\"\n",
    "all_questions.to_json(\"data/brain-hard-v0.1/model_judgment/falcon-7b-family-questions.json\", orient='records', lines=True)\n",
    "all_questions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08970c76-3a89-4f3f-abb8-86cf6e8dcbe4",
   "metadata": {},
   "source": [
    "### Markup stats functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "13589a8c-a3c5-4ef3-8bb6-c4465b77351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tabulate import tabulate\n",
    "\n",
    "# 1. Create mirrored comparisons for consistency check\n",
    "def create_mirror_comparisons(df):\n",
    "    \"\"\"\n",
    "    mirror = df.copy()\n",
    "    # Swap model names and reverse verdicts\n",
    "    \n",
    "    mirror['model_1'], mirror['model_2'] = mirror['model_2'], mirror['model_1']\n",
    "    mirror['verdict'] = mirror['verdict'].replace({\n",
    "        '[[A>B]]': '[[B>A]]',\n",
    "        '[[B>A]]': '[[A>B]]',\n",
    "        '[[A>>B]]': '[[B>>A]]',\n",
    "        '[[B>>A]]': '[[A>>B]]',\n",
    "        '[[A=B]]': '[[A=B]]'\n",
    "    })\n",
    "    \"\"\"\n",
    "    # Create a string key instead of list for hashing\n",
    "    df['comparison_key'] = df.apply(\n",
    "        lambda x: f\"{x['question_id']}_{min(x['model_1'], x['model_2'])}_{max(x['model_1'], x['model_2'])}\", \n",
    "        axis=1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculate_statistics(df):\n",
    "    stats = {\n",
    "        'total_comparisons': len(df),\n",
    "        'consistent_pairs': 0,\n",
    "        'inconsistent_pairs': 0,\n",
    "        'verdict_counts': df['verdict'].value_counts().to_dict(),\n",
    "        'model_win_rates': defaultdict(lambda: {\n",
    "            'wins': 0,\n",
    "            'losses': 0,\n",
    "            'ties': 0,\n",
    "            'strong_wins': 0,    # [[A>>B]] or [[B>>A]]\n",
    "            'weak_wins': 0,      # [[A>B]] or [[B>A]]\n",
    "            'draws': 0           # [[A=B]]\n",
    "        }),\n",
    "        'model_matchups': defaultdict(lambda: defaultdict(lambda: {\n",
    "            'wins': 0,\n",
    "            'losses': 0,\n",
    "            'ties': 0,\n",
    "            'total': 0\n",
    "        }))\n",
    "    }\n",
    "\n",
    "    # Count model performance\n",
    "    for _, row in df.iterrows():\n",
    "        model_a = row['model_1']\n",
    "        model_b = row['model_2']\n",
    "        verdict = row['verdict']\n",
    "        \n",
    "        # Update matchup stats\n",
    "        stats['model_matchups'][model_a][model_b]['total'] += 1\n",
    "        stats['model_matchups'][model_b][model_a]['total'] += 1\n",
    "        \n",
    "        if '>>' in verdict:  # Strong win\n",
    "            if 'A>>' in verdict:\n",
    "                winner, loser = model_a, model_b\n",
    "                stats['model_win_rates'][winner]['strong_wins'] += 1\n",
    "            else:\n",
    "                winner, loser = model_b, model_a\n",
    "                stats['model_win_rates'][winner]['strong_wins'] += 1\n",
    "            stats['model_win_rates'][winner]['wins'] += 1\n",
    "            stats['model_win_rates'][loser]['losses'] += 1\n",
    "            \n",
    "            # Update matchup specific stats\n",
    "            stats['model_matchups'][winner][loser]['wins'] += 1\n",
    "            stats['model_matchups'][loser][winner]['losses'] += 1\n",
    "            \n",
    "        elif '>' in verdict:  # Weak win\n",
    "            if 'A>' in verdict:\n",
    "                winner, loser = model_a, model_b\n",
    "            else:\n",
    "                winner, loser = model_b, model_a\n",
    "            stats['model_win_rates'][winner]['weak_wins'] += 1\n",
    "            stats['model_win_rates'][winner]['wins'] += 1\n",
    "            stats['model_win_rates'][loser]['losses'] += 1\n",
    "            \n",
    "            # Update matchup specific stats\n",
    "            stats['model_matchups'][winner][loser]['wins'] += 1\n",
    "            stats['model_matchups'][loser][winner]['losses'] += 1\n",
    "            \n",
    "        else:  # Draw\n",
    "            stats['model_win_rates'][model_a]['ties'] += 1\n",
    "            stats['model_win_rates'][model_b]['ties'] += 1\n",
    "            stats['model_win_rates'][model_a]['draws'] += 1\n",
    "            stats['model_win_rates'][model_b]['draws'] += 1\n",
    "            \n",
    "            # Update matchup specific stats\n",
    "            stats['model_matchups'][model_a][model_b]['ties'] += 1\n",
    "            stats['model_matchups'][model_b][model_a]['ties'] += 1\n",
    "\n",
    "    return stats\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculate_consistent_statistics(df):\n",
    "    stats = {\n",
    "        'total_comparisons': len(df),\n",
    "        'verdict_counts': df['verdict'].value_counts().to_dict(),\n",
    "        'model_win_rates': defaultdict(lambda: {\n",
    "            'wins': 0,\n",
    "            'losses': 0,\n",
    "            'ties': 0,\n",
    "            'strong_wins': 0,    # [[A>>B]] or [[B>>A]] (worth 2 points)\n",
    "            'weak_wins': 0,      # [[A>B]] or [[B>A]] (worth 1 point)\n",
    "            'draws': 0,          # [[A=B]]\n",
    "            'score': 0,          # Total score (strong_wins*2 + weak_wins*1)\n",
    "            'inconsistent_pairs': 0  # Count of judgments that were inconsistent\n",
    "        }),\n",
    "        'model_matchups': defaultdict(lambda: defaultdict(lambda: {\n",
    "            'wins': 0,\n",
    "            'losses': 0,\n",
    "            'ties': 0,\n",
    "            'total': 0,\n",
    "            'inconsistent': 0\n",
    "        })),\n",
    "        'inconsistencies': defaultdict(list)  # Track specific inconsistent pairs\n",
    "    }\n",
    "\n",
    "    # First pass: Identify all comparisons\n",
    "    comparison_dict = defaultdict(list)\n",
    "    for _, row in df.iterrows():\n",
    "        key = frozenset([row['question_id'], row['model_1'], row['model_2']])\n",
    "        comparison_dict[key].append(row['verdict'])\n",
    "\n",
    "    # Second pass: Calculate stats with consistency checks\n",
    "    for _, row in df.iterrows():\n",
    "        model_a = row['model_1']\n",
    "        model_b = row['model_2']\n",
    "        question_id = row['question_id']\n",
    "        verdict = row['verdict']\n",
    "        key = frozenset([question_id, model_a, model_b])\n",
    "        \n",
    "        # Check for inconsistent judgments\n",
    "        verdicts = comparison_dict[key]\n",
    "        is_inconsistent = ((verdicts.count('[[A>B]]') > 0 and verdicts.count('[[B>A]]') > 0) or \\\n",
    "                         (verdicts.count('[[A>>B]]') > 0 and verdicts.count('[[B>>A]]') > 0))\n",
    "        \n",
    "        # Update matchup stats\n",
    "        stats['model_matchups'][model_a][model_b]['total'] += 1\n",
    "        stats['model_matchups'][model_b][model_a]['total'] += 1\n",
    "        \n",
    "        if is_inconsistent:\n",
    "            # Treat inconsistent judgments as ties\n",
    "            stats['model_win_rates'][model_a]['ties'] += 1\n",
    "            stats['model_win_rates'][model_b]['ties'] += 1\n",
    "            stats['model_win_rates'][model_a]['draws'] += 1\n",
    "            stats['model_win_rates'][model_b]['draws'] += 1\n",
    "            stats['model_win_rates'][model_a]['inconsistent_pairs'] += 1\n",
    "            stats['model_win_rates'][model_b]['inconsistent_pairs'] += 1\n",
    "            \n",
    "            stats['model_matchups'][model_a][model_b]['ties'] += 1\n",
    "            stats['model_matchups'][model_b][model_a]['ties'] += 1\n",
    "            stats['model_matchups'][model_a][model_b]['inconsistent'] += 1\n",
    "            stats['model_matchups'][model_b][model_a]['inconsistent'] += 1\n",
    "            \n",
    "            stats['inconsistencies'][(model_a, model_b)].append(question_id)\n",
    "            continue\n",
    "            \n",
    "        if '>>' in verdict:  # Strong win (worth 2 points)\n",
    "            if 'A>>' in verdict:\n",
    "                winner, loser = model_a, model_b\n",
    "            else:\n",
    "                winner, loser = model_b, model_a\n",
    "                \n",
    "            stats['model_win_rates'][winner]['strong_wins'] += 1\n",
    "            stats['model_win_rates'][winner]['wins'] += 1\n",
    "            stats['model_win_rates'][winner]['score'] += 2\n",
    "            stats['model_win_rates'][loser]['losses'] += 1\n",
    "            \n",
    "            stats['model_matchups'][winner][loser]['wins'] += 1\n",
    "            stats['model_matchups'][loser][winner]['losses'] += 1\n",
    "            \n",
    "        elif '>' in verdict:  # Weak win (worth 1 point)\n",
    "            if 'A>' in verdict:\n",
    "                winner, loser = model_a, model_b\n",
    "            else:\n",
    "                winner, loser = model_b, model_a\n",
    "                \n",
    "            stats['model_win_rates'][winner]['weak_wins'] += 1\n",
    "            stats['model_win_rates'][winner]['wins'] += 1\n",
    "            stats['model_win_rates'][winner]['score'] += 1\n",
    "            stats['model_win_rates'][loser]['losses'] += 1\n",
    "            \n",
    "            stats['model_matchups'][winner][loser]['wins'] += 1\n",
    "            stats['model_matchups'][loser][winner]['losses'] += 1\n",
    "            \n",
    "        else:  # Draw\n",
    "            stats['model_win_rates'][model_a]['ties'] += 1\n",
    "            stats['model_win_rates'][model_b]['ties'] += 1\n",
    "            stats['model_win_rates'][model_a]['draws'] += 1\n",
    "            stats['model_win_rates'][model_b]['draws'] += 1\n",
    "            \n",
    "            stats['model_matchups'][model_a][model_b]['ties'] += 1\n",
    "            stats['model_matchups'][model_b][model_a]['ties'] += 1\n",
    "\n",
    "    # Calculate consistency metrics\n",
    "    total_possible_pairs = len(comparison_dict)\n",
    "    inconsistent_pairs = len(stats['inconsistencies'])\n",
    "    stats['consistency_metrics'] = {\n",
    "        'total_pairs': total_possible_pairs,\n",
    "        'inconsistent_pairs': inconsistent_pairs,\n",
    "        'consistency_rate': (total_possible_pairs - inconsistent_pairs) / total_possible_pairs if total_possible_pairs > 0 else 1.0\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "# 3. Find inconsistent judgments\n",
    "def find_inconsistent_judgments(df):\n",
    "    grouped = df.groupby('comparison_key')\n",
    "    inconsistencies = []\n",
    "    for name, group in grouped:\n",
    "        if len(group) == 2:  # Should have both comparison directions\n",
    "            verdict1 = group.iloc[0]['verdict']\n",
    "            verdict2 = group.iloc[1]['verdict']\n",
    "            \n",
    "            # Check if verdicts are not opposites\n",
    "            if not ((verdict1 == '[[A>B]]' and verdict2 == '[[B>A]]') or \n",
    "                   (verdict1 == '[[B>A]]' and verdict2 == '[[A>B]]') or\n",
    "                   (verdict1 == '[[A>>B]]' and verdict2 == '[[B>>A]]') or\n",
    "                   (verdict1 == '[[B>>A]]' and verdict2 == '[[A>>B]]') or\n",
    "                   (verdict1 == '[[A=B]]' and verdict2 == '[[A=B]]')):\n",
    "                inconsistencies.append(group)\n",
    "    \n",
    "    return pd.concat(inconsistencies) if inconsistencies else None\n",
    "\n",
    "# 4. Print significant differences\n",
    "def print_significant_differences(df, n=5):\n",
    "    df['win_magnitude'] = df['verdict'].apply(lambda x: 2 if '>>' in x else 1 if '>' in x else 0)\n",
    "    significant = df[df['win_magnitude'] > 0].nlargest(n, 'win_magnitude')\n",
    "    \n",
    "    for _, row in significant.iterrows():\n",
    "        # Extract core comparison info\n",
    "        print(f\"\\nQuestion ID: {row['id']}\")  # Fixed from 'question_id' to 'id'\n",
    "        print(f\"Models: {row['model_1']} vs {row['model_2']}\")\n",
    "        print(f\"Verdict: {row['verdict']}\")\n",
    "        print(f\"Explanation: {row['explanation']}\")\n",
    "        \n",
    "        # Parse and display conversation content if available\n",
    "        if 'content' in df.columns:\n",
    "            content = row['content']\n",
    "            try:\n",
    "                # Extract question and answers using the specific format markers\n",
    "                question = content.split(\"<|User Prompt|>\\n\")[1].split(\"\\n\\n<|The Start of Assistant A's Answer|>\\n\")[0].strip()\n",
    "                answer_a = content.split(\"<|The Start of Assistant A's Answer|>\\n\")[1].split(\"\\n<|The End of Assistant A's Answer|>\")[0].strip()\n",
    "                answer_b = content.split(\"<|The Start of Assistant B's Answer|>\\n\")[1].split(\"\\n<|The End of Assistant B's Answer|>\")[0].strip()\n",
    "                \n",
    "                print(\"\\n=== Question ===\")\n",
    "                print(question)\n",
    "                print(\"\\n=== Assistant A ===\")\n",
    "                print(answer_a)\n",
    "                print(\"\\n=== Assistant B ===\")\n",
    "                print(answer_b)\n",
    "            except IndexError:\n",
    "                print(\"\\n[Error parsing content format]\")\n",
    "        \n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2991bab3-c60a-4e9b-a0d8-c3c5b7582b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_inconsistency_stats(inconsistencies):\n",
    "    if inconsistencies is None or len(inconsistencies) == 0:\n",
    "        print(\"\\n✅ No inconsistent judgments found!\")\n",
    "        return\n",
    "    \n",
    "    # Prepare stats\n",
    "    total_pairs = len(inconsistencies) // 2\n",
    "    question_ids = inconsistencies['question_id'].unique()\n",
    "    \n",
    "    # Count by verdict combination\n",
    "    verdict_pairs = inconsistencies.groupby('comparison_key')['verdict'].apply(\n",
    "        lambda x: \" vs \".join(sorted(x))\n",
    "    )\n",
    "    verdict_counts = verdict_pairs.value_counts()\n",
    "\n",
    "    question_counts = inconsistencies['question_id'].value_counts()\n",
    "\n",
    "    model_pairs = inconsistencies.apply(\n",
    "        lambda x: f\"{min(x['model_1'], x['model_2'])} vs {max(x['model_1'], x['model_2'])}\", \n",
    "        axis=1)\n",
    "    model_pair_counts = model_pairs.value_counts()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n🔍 Found {total_pairs} inconsistent model pairs across {len(question_ids)} questions\")\n",
    "    print(\"\\n📊 Verdict Combinations:\")\n",
    "    print(tabulate(\n",
    "        [[\"Total\"] + verdict_counts.tolist()],\n",
    "        headers=[\"Verdict Pair\"] + verdict_counts.index.tolist(),\n",
    "        tablefmt=\"rounded_grid\"\n",
    "    ))\n",
    "    \n",
    "    # Print top questions with inconsistencies\n",
    "    print(\"\\n❓ Top Questions with Inconsistencies:\")\n",
    "    print(tabulate(\n",
    "        [(qid, count) for qid, count in question_counts.head(5).items()],\n",
    "        headers=[\"Question ID\", \"Inconsistencies\"],\n",
    "        tablefmt=\"rounded_grid\"\n",
    "    ))\n",
    "\n",
    "    for qid, count in question_counts.head(5).items():\n",
    "        content = inconsistencies[inconsistencies['question_id'] == qid]['content'].iloc[0]\n",
    "        question = content.split(\"<|User Prompt|>\\n\")[1].split(\"\\n\\n<|The Start of Assistant A's Answer|>\\n\")[0].strip()\n",
    "        #answer_a = content.split(\"<|The Start of Assistant A's Answer|>\\n\")[1].split(\"\\n<|The End of Assistant A's Answer|>\")[0].strip()\n",
    "        #answer_b = content.split(\"<|The Start of Assistant B's Answer|>\\n\")[1].split(\"\\n<|The End of Assistant B's Answer|>\")[0].strip()\n",
    "        print(f\"\\n=== Question {qid} ===\")\n",
    "        print(question)\n",
    "    print()\n",
    "    \n",
    "    # Print top model pairs with inconsistencies\n",
    "    print(\"\\n🤖 Top Model Pairs with Inconsistencies:\")\n",
    "    print(tabulate(\n",
    "        [(pair, count) for pair, count in model_pair_counts.head(5).items()],\n",
    "        headers=[\"Model Pair\", \"Inconsistencies\"],\n",
    "        tablefmt=\"rounded_grid\"\n",
    "    ))\n",
    "    \n",
    "    # Print example inconsistencies\n",
    "    print(\"\\n📝 Example Inconsistent Judgments (first 3):\")\n",
    "    examples = inconsistencies.drop_duplicates('comparison_key').head(3)\n",
    "    for _, row in examples.iterrows():\n",
    "        print(f\"\\nQuestion {row['question_id']}: {row['model_1']} vs {row['model_2']}\")\n",
    "        print(f\"• Judgment 1: {row['verdict']} - {row['explanation']}\")\n",
    "        opposite = inconsistencies[\n",
    "            (inconsistencies['comparison_key'] == row['comparison_key']) & \n",
    "            (inconsistencies['verdict'] != row['verdict'])\n",
    "        ]\n",
    "        if not opposite.empty:\n",
    "            print(f\"• Judgment 2: {opposite.iloc[0]['verdict']} - {opposite.iloc[0]['explanation']}\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5abd4364-61cc-4420-8f7b-ff06dfd1b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_report(data):\n",
    "    mirrored_df = create_mirror_comparisons(data)\n",
    "    stats = calculate_statistics(mirrored_df)\n",
    "    stats_consistent = calculate_consistent_statistics(mirrored_df)\n",
    "    inconsistencies = find_inconsistent_judgments(mirrored_df)\n",
    "    \n",
    "    # 1. Print statistics\n",
    "    print(\"=== Overall Statistics ===\")\n",
    "    print(f\"Total comparisons: {stats['total_comparisons']}\")\n",
    "    print(f\"Verdict distribution: {stats['verdict_counts']}\")\n",
    "    \n",
    "    print(\"\\n=== Model Performance ===\")\n",
    "    for model, performance in stats['model_win_rates'].items():\n",
    "        total = performance['wins'] + performance['losses'] + performance['ties']\n",
    "        win_rate = performance['wins'] / total * 100 if total > 0 else 0\n",
    "        print(f\"{model}: {performance['wins']} wins, {performance['losses']} losses, {performance['ties']} ties ({win_rate:.1f}% win rate)\")\n",
    "    print(\"\\n=== Model Permormance [Cleared inconsistencies]\")\n",
    "    for model, data in sorted(stats_consistent['model_win_rates'].items(), \n",
    "                         key=lambda x: x[1]['score'], reverse=True):\n",
    "        print(f\"{model}: {data['score']} pts \"\n",
    "              f\"(Strong: {data['strong_wins']}, Weak: {data['weak_wins']}, \"\n",
    "              f\"Ties: {data['ties']}, Inconsistent: {data['inconsistent_pairs']})\")\n",
    "\n",
    "    # 2. Build final judgment table\n",
    "    print(\"\\n=== Final Judgment Table ===\")\n",
    "    final_judgment = mirrored_df[['question_id', 'model_1', 'model_2', 'verdict', 'explanation']]\n",
    "    print(final_judgment.head())\n",
    "    \n",
    "    # 3. Print significant differences\n",
    "    print(\"\\n=== Top 5 Significant Differences ===\")\n",
    "    print_significant_differences(mirrored_df)\n",
    "\n",
    "    # 4. Inconsistancies\n",
    "    if inconsistencies is not None:\n",
    "        print_inconsistency_stats(inconsistencies)\n",
    "    else:\n",
    "        print(\"\\n✅ No inconsistent judgments found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45758eed-092e-49a1-a5af-d4896f8f279a",
   "metadata": {},
   "source": [
    "### Qwen - GPT markup - Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c304d3e5-61bb-4bc2-bc89-2616b314ec5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions: 4740\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>model_2</th>\n",
       "      <th>model_1</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_qwen-2.5-7b-base_qwen-2.5-7b-int4</td>\n",
       "      <td>0</td>\n",
       "      <td>qwen-2.5-7b-int4</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nEarthworms live underground i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_qwen-2.5-7b-base_qwen-2.5-7b-int4</td>\n",
       "      <td>1</td>\n",
       "      <td>qwen-2.5-7b-int4</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nHow can I temporarily seal a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2_qwen-2.5-7b-base_qwen-2.5-7b-int4</td>\n",
       "      <td>2</td>\n",
       "      <td>qwen-2.5-7b-int4</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nHow to clean earbuds\\n\\n&lt;|The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5_qwen-2.5-7b-base_qwen-2.5-7b-int4</td>\n",
       "      <td>5</td>\n",
       "      <td>qwen-2.5-7b-int4</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nTo hold open a bedroom door w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7_qwen-2.5-7b-base_qwen-2.5-7b-int4</td>\n",
       "      <td>7</td>\n",
       "      <td>qwen-2.5-7b-int4</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nTo add a surprising twist to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id  question_id           model_2  \\\n",
       "0  0_qwen-2.5-7b-base_qwen-2.5-7b-int4            0  qwen-2.5-7b-int4   \n",
       "1  1_qwen-2.5-7b-base_qwen-2.5-7b-int4            1  qwen-2.5-7b-int4   \n",
       "2  2_qwen-2.5-7b-base_qwen-2.5-7b-int4            2  qwen-2.5-7b-int4   \n",
       "3  5_qwen-2.5-7b-base_qwen-2.5-7b-int4            5  qwen-2.5-7b-int4   \n",
       "4  7_qwen-2.5-7b-base_qwen-2.5-7b-int4            7  qwen-2.5-7b-int4   \n",
       "\n",
       "            model_1                                            content  \n",
       "0  qwen-2.5-7b-base  <|User Prompt|>\\nEarthworms live underground i...  \n",
       "1  qwen-2.5-7b-base  <|User Prompt|>\\nHow can I temporarily seal a ...  \n",
       "2  qwen-2.5-7b-base  <|User Prompt|>\\nHow to clean earbuds\\n\\n<|The...  \n",
       "3  qwen-2.5-7b-base  <|User Prompt|>\\nTo hold open a bedroom door w...  \n",
       "4  qwen-2.5-7b-base  <|User Prompt|>\\nTo add a surprising twist to ...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_qwen = pd.read_json(\"data/brain-hard-v0.1/model_judgment/qwen-2.5-7b-family-questions.json\", lines=True)\n",
    "print(f\"Total questions: {questions_qwen.shape[0]}\")\n",
    "questions_qwen = questions_qwen[[\"question_id\", \"old_question_id\", \"model_2\", \"model_1\", \"turns\"]]\n",
    "questions_qwen['content'] = questions_qwen[\"turns\"].map(lambda x: x[1]['content'])\n",
    "questions_qwen.drop(columns = ['turns'], inplace=True)\n",
    "questions_qwen.rename(columns = {'question_id': 'id', 'old_question_id': 'question_id'}, inplace=True)\n",
    "questions_qwen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a5e311a2-62de-4780-bc18-c0404b4d2f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>question_id</th>\n",
       "      <th>model_2</th>\n",
       "      <th>model_1</th>\n",
       "      <th>conv</th>\n",
       "      <th>judge</th>\n",
       "      <th>answer</th>\n",
       "      <th>explanation</th>\n",
       "      <th>verdict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_qwen-2.5-7b-base_qwen-2.5-7b-int4</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nEarthworms live underground i...</td>\n",
       "      <td>0</td>\n",
       "      <td>qwen-2.5-7b-int4</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>[{'role': 'system', 'content': 'Please act as ...</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>{'explanation': 'Both assistants provide accur...</td>\n",
       "      <td>Both assistants provide accurate explanations ...</td>\n",
       "      <td>[[B&gt;A]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_qwen-2.5-7b-base_qwen-2.5-7b-int4</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nHow can I temporarily seal a ...</td>\n",
       "      <td>1</td>\n",
       "      <td>qwen-2.5-7b-int4</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>[{'role': 'system', 'content': 'Please act as ...</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>{'explanation': 'Both assistants provide simil...</td>\n",
       "      <td>Both assistants provide similar methods for te...</td>\n",
       "      <td>[[A&gt;B]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2_qwen-2.5-7b-base_qwen-2.5-7b-int4</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nHow to clean earbuds\\n\\n&lt;|The...</td>\n",
       "      <td>2</td>\n",
       "      <td>qwen-2.5-7b-int4</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>[{'role': 'system', 'content': 'Please act as ...</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>{'explanation': 'Both assistants provide a ste...</td>\n",
       "      <td>Both assistants provide a step-by-step guide o...</td>\n",
       "      <td>[[B&gt;A]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5_qwen-2.5-7b-base_qwen-2.5-7b-int4</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nTo hold open a bedroom door w...</td>\n",
       "      <td>5</td>\n",
       "      <td>qwen-2.5-7b-int4</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>[{'role': 'system', 'content': 'Please act as ...</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>{'explanation': 'Both assistants provide pract...</td>\n",
       "      <td>Both assistants provide practical solutions fo...</td>\n",
       "      <td>[[B&gt;A]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7_qwen-2.5-7b-base_qwen-2.5-7b-int4</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nTo add a surprising twist to ...</td>\n",
       "      <td>7</td>\n",
       "      <td>qwen-2.5-7b-int4</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>[{'role': 'system', 'content': 'Please act as ...</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>{'explanation': 'Both assistants provide creat...</td>\n",
       "      <td>Both assistants provide creative and relevant ...</td>\n",
       "      <td>[[A=B]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id  \\\n",
       "0  0_qwen-2.5-7b-base_qwen-2.5-7b-int4   \n",
       "1  1_qwen-2.5-7b-base_qwen-2.5-7b-int4   \n",
       "2  2_qwen-2.5-7b-base_qwen-2.5-7b-int4   \n",
       "3  5_qwen-2.5-7b-base_qwen-2.5-7b-int4   \n",
       "4  7_qwen-2.5-7b-base_qwen-2.5-7b-int4   \n",
       "\n",
       "                                             content  question_id  \\\n",
       "0  <|User Prompt|>\\nEarthworms live underground i...            0   \n",
       "1  <|User Prompt|>\\nHow can I temporarily seal a ...            1   \n",
       "2  <|User Prompt|>\\nHow to clean earbuds\\n\\n<|The...            2   \n",
       "3  <|User Prompt|>\\nTo hold open a bedroom door w...            5   \n",
       "4  <|User Prompt|>\\nTo add a surprising twist to ...            7   \n",
       "\n",
       "            model_2           model_1  \\\n",
       "0  qwen-2.5-7b-int4  qwen-2.5-7b-base   \n",
       "1  qwen-2.5-7b-int4  qwen-2.5-7b-base   \n",
       "2  qwen-2.5-7b-int4  qwen-2.5-7b-base   \n",
       "3  qwen-2.5-7b-int4  qwen-2.5-7b-base   \n",
       "4  qwen-2.5-7b-int4  qwen-2.5-7b-base   \n",
       "\n",
       "                                                conv   judge  \\\n",
       "0  [{'role': 'system', 'content': 'Please act as ...  gpt-4o   \n",
       "1  [{'role': 'system', 'content': 'Please act as ...  gpt-4o   \n",
       "2  [{'role': 'system', 'content': 'Please act as ...  gpt-4o   \n",
       "3  [{'role': 'system', 'content': 'Please act as ...  gpt-4o   \n",
       "4  [{'role': 'system', 'content': 'Please act as ...  gpt-4o   \n",
       "\n",
       "                                              answer  \\\n",
       "0  {'explanation': 'Both assistants provide accur...   \n",
       "1  {'explanation': 'Both assistants provide simil...   \n",
       "2  {'explanation': 'Both assistants provide a ste...   \n",
       "3  {'explanation': 'Both assistants provide pract...   \n",
       "4  {'explanation': 'Both assistants provide creat...   \n",
       "\n",
       "                                         explanation  verdict  \n",
       "0  Both assistants provide accurate explanations ...  [[B>A]]  \n",
       "1  Both assistants provide similar methods for te...  [[A>B]]  \n",
       "2  Both assistants provide a step-by-step guide o...  [[B>A]]  \n",
       "3  Both assistants provide practical solutions fo...  [[B>A]]  \n",
       "4  Both assistants provide creative and relevant ...  [[A=B]]  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_qwen_data = pd.concat(\n",
    "    [pd.read_json(\"data/brain-hard-v0.1/model_judgment/qwen-7b-markup-gpt-2\"), \n",
    "     pd.read_json(\"data/brain-hard-v0.1/model_judgment/qwen-7b-markup-gpt\")],\n",
    "    axis=0, ignore_index=True\n",
    ")\n",
    "gpt_qwen_data[\"answer\"] = gpt_qwen_data[\"answer\"].map(json.loads)\n",
    "gpt_qwen_data[\"explanation\"] = gpt_qwen_data[\"answer\"].map(lambda x: x[\"explanation\"])\n",
    "gpt_qwen_data[\"verdict\"] = gpt_qwen_data[\"answer\"].map(lambda x: x[\"verdict\"])\n",
    "gpt_qwen_data = gpt_qwen_data.groupby('id').first()\n",
    "\n",
    "gpt_qwen_data = gpt_qwen_data.reset_index()\n",
    "gpt_qwen_data = pd.merge(\n",
    "    left=questions_qwen[['id', 'content']],  \n",
    "    right=gpt_qwen_data,                     \n",
    "    on='id', \n",
    "    how='inner'\n",
    ")\n",
    "gpt_qwen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "49168820-b7e9-4ea4-afe8-6f5d6b01ba4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "verdict\n",
       "[[A>B]]     1523\n",
       "[[A=B]]     1503\n",
       "[[B>A]]     1488\n",
       "[[A>>B]]     140\n",
       "[[B>>A]]      86\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_qwen_data.verdict.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a977f0cb-15f2-4723-bbb0-011088a1a46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Overall Statistics ===\n",
      "Total comparisons: 4740\n",
      "Verdict distribution: {'[[A>B]]': 1523, '[[A=B]]': 1503, '[[B>A]]': 1488, '[[A>>B]]': 140, '[[B>>A]]': 86}\n",
      "\n",
      "=== Model Performance ===\n",
      "qwen-2.5-7b-int4: 980 wins, 839 losses, 551 ties (41.4% win rate)\n",
      "qwen-2.5-7b-base: 595 wins, 826 losses, 949 ties (25.1% win rate)\n",
      "qwen-2.5-7b-int8: 535 wins, 873 losses, 962 ties (22.6% win rate)\n",
      "qwen-2.5-7b-awq: 1127 wins, 699 losses, 544 ties (47.6% win rate)\n",
      "\n",
      "=== Model Permormance [Cleared inconsistencies]\n",
      "qwen-2.5-7b-int4: 299 pts (Strong: 33, Weak: 233, Ties: 1883, Inconsistent: 1332)\n",
      "qwen-2.5-7b-awq: 286 pts (Strong: 35, Weak: 216, Ties: 1916, Inconsistent: 1372)\n",
      "qwen-2.5-7b-base: 177 pts (Strong: 8, Weak: 161, Ties: 1993, Inconsistent: 1044)\n",
      "qwen-2.5-7b-int8: 177 pts (Strong: 10, Weak: 157, Ties: 1982, Inconsistent: 1020)\n",
      "\n",
      "=== Final Judgment Table ===\n",
      "   question_id           model_1           model_2  verdict  \\\n",
      "0            0  qwen-2.5-7b-base  qwen-2.5-7b-int4  [[B>A]]   \n",
      "1            1  qwen-2.5-7b-base  qwen-2.5-7b-int4  [[A>B]]   \n",
      "2            2  qwen-2.5-7b-base  qwen-2.5-7b-int4  [[B>A]]   \n",
      "3            5  qwen-2.5-7b-base  qwen-2.5-7b-int4  [[B>A]]   \n",
      "4            7  qwen-2.5-7b-base  qwen-2.5-7b-int4  [[A=B]]   \n",
      "\n",
      "                                         explanation  \n",
      "0  Both assistants provide accurate explanations ...  \n",
      "1  Both assistants provide similar methods for te...  \n",
      "2  Both assistants provide a step-by-step guide o...  \n",
      "3  Both assistants provide practical solutions fo...  \n",
      "4  Both assistants provide creative and relevant ...  \n",
      "\n",
      "=== Top 5 Significant Differences ===\n",
      "\n",
      "Question ID: 29_qwen-2.5-7b-base_qwen-2.5-7b-int4\n",
      "Models: qwen-2.5-7b-base vs qwen-2.5-7b-int4\n",
      "Verdict: [[B>>A]]\n",
      "Explanation: Assistant A provides incorrect information about using the Trackpad and Magic Keyboard for zooming, which may confuse users. Assistant B offers multiple accurate methods for zooming in on a Mac, including the two-finger pinch and keyboard shortcuts, making it more helpful and relevant.\n",
      "\n",
      "=== Question ===\n",
      "How do you zoom in on a Mac computer?\n",
      "\n",
      "=== Assistant A ===\n",
      "To zoom in on a Mac computer, you can use the Trackpad or Magic Keyboard. For the Trackpad, swipe two fingers up. For a Magic Keyboard, press and hold the Control key while using the Touch ID and then swipe up with one finger.\n",
      "\n",
      "=== Assistant B ===\n",
      "To zoom in on a Mac computer, you can use the following methods:\n",
      "\n",
      "1. **Two-Finger Pinch on Trackpad**: Place two fingers on the trackpad and pinch inwards to zoom in, or spread them apart to zoom out.\n",
      "2. **Command + Plus Sign (+)**: Press and hold the Command key while pressing the plus sign key to zoom in.\n",
      "3. **Command + Minus Sign (-)**: Press and hold the Command key while pressing the minus sign key to zoom out.\n",
      "\n",
      "These methods work in most applications and web browsers on a Mac.\n",
      "--------------------------------------------------\n",
      "\n",
      "Question ID: 64_qwen-2.5-7b-base_qwen-2.5-7b-int4\n",
      "Models: qwen-2.5-7b-base vs qwen-2.5-7b-int4\n",
      "Verdict: [[A>>B]]\n",
      "Explanation: Assistant A correctly explains that a double-double can be achieved by recording 10 or more in any two of the five statistical categories: points, rebounds, assists, steals, and blocks. Assistant B incorrectly specifies that it must be 10 points and 10 assists, which is not accurate as it can be any combination of two categories.\n",
      "\n",
      "=== Question ===\n",
      "To get a double-double in a basketball game,\n",
      "\n",
      "=== Assistant A ===\n",
      "To get a double-double in a basketball game, a player must record at least 10 points and 10 rebounds, or any other combination of 10 or more in two of the following categories: points, rebounds, assists, steals, and blocks.\n",
      "\n",
      "=== Assistant B ===\n",
      "To get a double-double in a basketball game, a player must record at least 10 points and 10 assists, or any other combination of 10 or more in two of the following categories: points, rebounds, assists, steals, and blocks.\n",
      "--------------------------------------------------\n",
      "\n",
      "Question ID: 97_qwen-2.5-7b-base_qwen-2.5-7b-int4\n",
      "Models: qwen-2.5-7b-base vs qwen-2.5-7b-int4\n",
      "Verdict: [[B>>A]]\n",
      "Explanation: Assistant A incorrectly converts the time to 00:00 on February 07, 2023, which is not accurate. Assistant B correctly converts the time to 03:00 on February 08, 2023, taking into account the 3-hour difference from UTC to Moscow time. Therefore, Assistant B provides the correct and more helpful response.\n",
      "\n",
      "=== Question ===\n",
      "Convert to Moscow time zone: \n",
      "February 07, 2023 20:00 UTC\n",
      "\n",
      "=== Assistant A ===\n",
      "February 07, 2023, 00:00 Moscow Time (MSK). Moscow Time is 3 hours ahead of UTC.\n",
      "\n",
      "=== Assistant B ===\n",
      "February 07, 2023, 20:00 UTC is 03:00 (3:00 AM) on February 08, 2023, in Moscow time zone. Moscow is typically 3 hours ahead of UTC during standard time.\n",
      "--------------------------------------------------\n",
      "\n",
      "Question ID: 283_qwen-2.5-7b-base_qwen-2.5-7b-int4\n",
      "Models: qwen-2.5-7b-base vs qwen-2.5-7b-int4\n",
      "Verdict: [[A>>B]]\n",
      "Explanation: Assistant A provides a more accurate and detailed response, mentioning the correct date of the Louisiana Purchase agreement (April 30, 1803) and the formal transfer date (December 20, 1803), as well as the date Louisiana was admitted as a state (April 30, 1812). Assistant B incorrectly states the date of the Louisiana Purchase agreement and lacks additional context about the formal transfer and statehood.\n",
      "\n",
      "=== Question ===\n",
      "when did louisiana became a part of The United States\n",
      "\n",
      "=== Assistant A ===\n",
      "Louisiana became a part of the United States on December 20, 1803, when it was purchased from France in the Louisiana Purchase. However, the territory was admitted as the 18th state on April 30, 1812. The Louisiana Purchase was officially completed on October 20, 1803, when the formal transfer of the territory was finalized.\n",
      "\n",
      "=== Assistant B ===\n",
      "Louisiana became a part of the United States on April 30, 1803, with the signing of the Louisiana Purchase agreement.\n",
      "--------------------------------------------------\n",
      "\n",
      "Question ID: 553_qwen-2.5-7b-base_qwen-2.5-7b-int4\n",
      "Models: qwen-2.5-7b-base vs qwen-2.5-7b-int4\n",
      "Verdict: [[B>>A]]\n",
      "Explanation: Assistant B provides a more accurate and relevant response by detailing the correct steps to pair a new remote with a Fire TV Stick, which involves using the settings on the device itself. Assistant A incorrectly suggests downloading a remote app from the Amazon Appstore, which is not the standard method for adding a new physical remote control to a Fire Stick.\n",
      "\n",
      "=== Question ===\n",
      "Como agregar un nuevo control a mi Fire Stick\n",
      "\n",
      "=== Assistant A ===\n",
      "Para agregar un nuevo control a tu Fire TV Stick, puedes seguir estos pasos:\n",
      "\n",
      "1. **Descargar el control remoto**: Visita la tienda de aplicaciones de Amazon (Appstore) en tu Fire TV Stick.\n",
      "2. **Buscar la aplicación de control remoto**: Busca la aplicación que desees instalar.\n",
      "3. **Instalar la aplicación**: Selecciona la aplicación y sigue las instrucciones para instalarla.\n",
      "4. **Configurar el control remoto**: Algunas aplicaciones te permiten configurar el control remoto para que funcione según tus preferencias.\n",
      "\n",
      "Recuerda que no todos los controles remotos están disponibles en la Appstore de Amazon, y algunos pueden requerir un suscripción o pago adicional.\n",
      "\n",
      "=== Assistant B ===\n",
      "Para agregar un nuevo control a tu Fire TV Stick, puedes seguir estos pasos:\n",
      "\n",
      "1. Abre la aplicación \"Fire TV Settings\" en tu Fire TV Stick.\n",
      "2. Navega hasta \"Controles remotos y dispositivos\".\n",
      "3. Selecciona \"Controles remotos adicionales\".\n",
      "4. Sigue las instrucciones para conectar y configurar el nuevo control remoto.\n",
      "\n",
      "Recuerda que no todos los controles remotos son compatibles con el Fire TV Stick, así que asegúrate de que el que deseas agregar sea compatible.\n",
      "--------------------------------------------------\n",
      "\n",
      "🔍 Found 545 inconsistent model pairs across 253 questions\n",
      "\n",
      "📊 Verdict Combinations:\n",
      "╭────────────────┬──────────────────────┬──────────────────────┬──────────────────────┬──────────────────────┬───────────────────────┬───────────────────────┬───────────────────────┬────────────────────────┬───────────────────────╮\n",
      "│ Verdict Pair   │   [[A>B]] vs [[A>B]] │   [[A=B]] vs [[A>B]] │   [[A=B]] vs [[B>A]] │   [[B>A]] vs [[B>A]] │   [[A>>B]] vs [[B>A]] │   [[A>B]] vs [[B>>A]] │   [[A>>B]] vs [[A>B]] │   [[A>>B]] vs [[A>>B]] │   [[A=B]] vs [[B>>A]] │\n",
      "├────────────────┼──────────────────────┼──────────────────────┼──────────────────────┼──────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼────────────────────────┼───────────────────────┤\n",
      "│ Total          │                  127 │                  121 │                  115 │                   99 │                    53 │                    15 │                    11 │                      3 │                     1 │\n",
      "╰────────────────┴──────────────────────┴──────────────────────┴──────────────────────┴──────────────────────┴───────────────────────┴───────────────────────┴───────────────────────┴────────────────────────┴───────────────────────╯\n",
      "\n",
      "❓ Top Questions with Inconsistencies:\n",
      "╭───────────────┬───────────────────╮\n",
      "│   Question ID │   Inconsistencies │\n",
      "├───────────────┼───────────────────┤\n",
      "│           700 │                12 │\n",
      "├───────────────┼───────────────────┤\n",
      "│           796 │                12 │\n",
      "├───────────────┼───────────────────┤\n",
      "│          1966 │                10 │\n",
      "├───────────────┼───────────────────┤\n",
      "│          1621 │                10 │\n",
      "├───────────────┼───────────────────┤\n",
      "│          1034 │                10 │\n",
      "╰───────────────┴───────────────────╯\n",
      "\n",
      "=== Question 700 ===\n",
      "You are given a paper:\n",
      "Title: Local Contextual Attention with Hierarchical Structure for Dialogue Act Recognition\n",
      "Abstract: Dialogue act recognition is a fundamental task for an intelligent dialogue system. Previous work models the whole dialog to predict dialog acts, which may bring the noise from unrelated sentences. In this work, we design a hierarchical model based on self-attention to capture intra-sentence and inter-sentence information. We revise the attention distribution to focus on the local and contextual semantic information by incorporating the relative position information between utterances. Based on the found that the length of dialog affects the performance, we introduce a new dialog segmentation mechanism to analyze the effect of dialog length and context padding length under online and offline settings. The experiment shows that our method achieves promising performance on two datasets: Switchboard Dialogue Act and DailyDialog with the accuracy of 80.34\\% and 85.81\\% respectively. Visualization of the attention weights shows that our method can learn the context dependency between utterances explicitly.\n",
      "\n",
      "What previous methods is the proposed method compared against?\n",
      "\n",
      "=== Question 796 ===\n",
      "Teacher:In this task, you are given a sentence or phrase in English. You must translate it to Xhosa in a way that is equivalent in terms of meaning and grammatically correct.\n",
      "Teacher: Now, understand the problem? Solve this instance: It has a wire boltrope, served with spunyarn or nettlestuff.\n",
      "Student:\n",
      "\n",
      "=== Question 1966 ===\n",
      "Are Metra Train and MARC Train the same type of train?\n",
      "\n",
      "=== Question 1621 ===\n",
      "¿Que hace 'continue' en este codigo de Javascript? \n",
      "           \"var sum = 0;\n",
      "            for (i =3; i<10; i++)\n",
      "            {\n",
      "                if (i ==7)\n",
      "                {\n",
      "                    continue;\n",
      "                }\n",
      "            \n",
      "                sum += i; \n",
      "                document.write(sum);\n",
      "            }\n",
      "\"\n",
      "\n",
      "=== Question 1034 ===\n",
      "How many households were there in the city in which WPUR is licensed?\n",
      "\n",
      "\n",
      "🤖 Top Model Pairs with Inconsistencies:\n",
      "╭──────────────────────────────────────┬───────────────────╮\n",
      "│ Model Pair                           │   Inconsistencies │\n",
      "├──────────────────────────────────────┼───────────────────┤\n",
      "│ qwen-2.5-7b-int4 vs qwen-2.5-7b-int8 │               218 │\n",
      "├──────────────────────────────────────┼───────────────────┤\n",
      "│ qwen-2.5-7b-base vs qwen-2.5-7b-int4 │               204 │\n",
      "├──────────────────────────────────────┼───────────────────┤\n",
      "│ qwen-2.5-7b-awq vs qwen-2.5-7b-int8  │               196 │\n",
      "├──────────────────────────────────────┼───────────────────┤\n",
      "│ qwen-2.5-7b-awq vs qwen-2.5-7b-int4  │               194 │\n",
      "├──────────────────────────────────────┼───────────────────┤\n",
      "│ qwen-2.5-7b-awq vs qwen-2.5-7b-base  │               190 │\n",
      "╰──────────────────────────────────────┴───────────────────╯\n",
      "\n",
      "📝 Example Inconsistent Judgments (first 3):\n",
      "\n",
      "Question 1027: qwen-2.5-7b-base vs qwen-2.5-7b-awq\n",
      "• Judgment 1: [[A>B]] - Assistant A provides practical and relevant tips, such as using mesh bags and placing items on the bottom rack, which are more effective for preventing lightweight items from flipping over. Assistant B's suggestion to use rubber bands or dish towels is less practical and could interfere with the washing process. Both assistants offer some useful advice, but Assistant A's response is more comprehensive and practical.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question 1027: qwen-2.5-7b-awq vs qwen-2.5-7b-int4\n",
      "• Judgment 1: [[B>A]] - Both assistants provide practical and relevant tips for preventing Tupperware and other light items from flipping over in the dishwasher. Assistant A suggests using dish rack clips and a dishwasher-safe basket, while Assistant B recommends securing items with utensils and using a lower detergent setting. Both responses are helpful and concise, but Assistant B offers a slightly more creative approach by suggesting the use of utensils for stability.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question 1027: qwen-2.5-7b-awq vs qwen-2.5-7b-int8\n",
      "• Judgment 1: [[A>B]] - Assistant A provides practical and straightforward solutions, such as using the top rack and securing items with rubber bands, which are more commonly applicable. Assistant B offers some creative suggestions like using mesh bags and securing with utensils, but the advice to avoid the top shelf contradicts common practice, as lightweight items are typically placed on the top rack to prevent movement. Both responses are helpful, but Assistant A's advice is more aligned with standard dishwasher practices.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_report(gpt_qwen_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1956ac79-e42f-4aec-9c2e-5d421d7e647e",
   "metadata": {},
   "source": [
    "### Qwen - Qwen base markup - Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a2091bf2-2918-45c4-8948-d7c01fef26ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4740, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>model_id</th>\n",
       "      <th>choices</th>\n",
       "      <th>tstamp</th>\n",
       "      <th>answer</th>\n",
       "      <th>explanation</th>\n",
       "      <th>verdict</th>\n",
       "      <th>question_id</th>\n",
       "      <th>model_1</th>\n",
       "      <th>model_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_qwen-2.5-7b-base_qwen-2.5-7b-int4</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nEarthworms live underground i...</td>\n",
       "      <td>Bq4DDnvrN2VMzawMKEGqz6</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>[{'index': 0, 'turns': [{'content': '{\"explana...</td>\n",
       "      <td>1.747083e+09</td>\n",
       "      <td>{'explanation': 'Both answers are accurate and...</td>\n",
       "      <td>Both answers are accurate and relevant, but As...</td>\n",
       "      <td>[[B&gt;A]]</td>\n",
       "      <td>0</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>qwen-2.5-7b-int4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_qwen-2.5-7b-base_qwen-2.5-7b-int4</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nHow can I temporarily seal a ...</td>\n",
       "      <td>CneiFerRwvWMRnB5Y46XFt</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>[{'index': 0, 'turns': [{'content': '{\"explana...</td>\n",
       "      <td>1.747083e+09</td>\n",
       "      <td>{'explanation': 'Assistant A provided a more c...</td>\n",
       "      <td>Assistant A provided a more comprehensive answ...</td>\n",
       "      <td>[[A&gt;B]]</td>\n",
       "      <td>1</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>qwen-2.5-7b-int4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2_qwen-2.5-7b-base_qwen-2.5-7b-int4</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nHow to clean earbuds\\n\\n&lt;|The...</td>\n",
       "      <td>MDidaLMit6SWuz67iyhicM</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>[{'index': 0, 'turns': [{'content': '{\"explana...</td>\n",
       "      <td>1.747083e+09</td>\n",
       "      <td>{'explanation': 'Assistant A provided a concis...</td>\n",
       "      <td>Assistant A provided a concise and accurate se...</td>\n",
       "      <td>[[B&gt;A]]</td>\n",
       "      <td>2</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>qwen-2.5-7b-int4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5_qwen-2.5-7b-base_qwen-2.5-7b-int4</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nTo hold open a bedroom door w...</td>\n",
       "      <td>UcM3PzzLvh424KJoNHsWZP</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>[{'index': 0, 'turns': [{'content': '{\"explana...</td>\n",
       "      <td>1.747083e+09</td>\n",
       "      <td>{'explanation': 'Assistant A provided a clear ...</td>\n",
       "      <td>Assistant A provided a clear and concise solut...</td>\n",
       "      <td>[[B&gt;A]]</td>\n",
       "      <td>5</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>qwen-2.5-7b-int4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7_qwen-2.5-7b-base_qwen-2.5-7b-int4</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nTo add a surprising twist to ...</td>\n",
       "      <td>EW94N5pDaFrKYurQSKJdBJ</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>[{'index': 0, 'turns': [{'content': '{\"explana...</td>\n",
       "      <td>1.747083e+09</td>\n",
       "      <td>{'explanation': 'Both answers are relevant and...</td>\n",
       "      <td>Both answers are relevant and concise, but Ass...</td>\n",
       "      <td>[[B&gt;A]]</td>\n",
       "      <td>7</td>\n",
       "      <td>qwen-2.5-7b-base</td>\n",
       "      <td>qwen-2.5-7b-int4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id  \\\n",
       "0  0_qwen-2.5-7b-base_qwen-2.5-7b-int4   \n",
       "1  1_qwen-2.5-7b-base_qwen-2.5-7b-int4   \n",
       "2  2_qwen-2.5-7b-base_qwen-2.5-7b-int4   \n",
       "3  5_qwen-2.5-7b-base_qwen-2.5-7b-int4   \n",
       "4  7_qwen-2.5-7b-base_qwen-2.5-7b-int4   \n",
       "\n",
       "                                             content               answer_id  \\\n",
       "0  <|User Prompt|>\\nEarthworms live underground i...  Bq4DDnvrN2VMzawMKEGqz6   \n",
       "1  <|User Prompt|>\\nHow can I temporarily seal a ...  CneiFerRwvWMRnB5Y46XFt   \n",
       "2  <|User Prompt|>\\nHow to clean earbuds\\n\\n<|The...  MDidaLMit6SWuz67iyhicM   \n",
       "3  <|User Prompt|>\\nTo hold open a bedroom door w...  UcM3PzzLvh424KJoNHsWZP   \n",
       "4  <|User Prompt|>\\nTo add a surprising twist to ...  EW94N5pDaFrKYurQSKJdBJ   \n",
       "\n",
       "           model_id                                            choices  \\\n",
       "0  qwen-2.5-7b-base  [{'index': 0, 'turns': [{'content': '{\"explana...   \n",
       "1  qwen-2.5-7b-base  [{'index': 0, 'turns': [{'content': '{\"explana...   \n",
       "2  qwen-2.5-7b-base  [{'index': 0, 'turns': [{'content': '{\"explana...   \n",
       "3  qwen-2.5-7b-base  [{'index': 0, 'turns': [{'content': '{\"explana...   \n",
       "4  qwen-2.5-7b-base  [{'index': 0, 'turns': [{'content': '{\"explana...   \n",
       "\n",
       "         tstamp                                             answer  \\\n",
       "0  1.747083e+09  {'explanation': 'Both answers are accurate and...   \n",
       "1  1.747083e+09  {'explanation': 'Assistant A provided a more c...   \n",
       "2  1.747083e+09  {'explanation': 'Assistant A provided a concis...   \n",
       "3  1.747083e+09  {'explanation': 'Assistant A provided a clear ...   \n",
       "4  1.747083e+09  {'explanation': 'Both answers are relevant and...   \n",
       "\n",
       "                                         explanation  verdict question_id  \\\n",
       "0  Both answers are accurate and relevant, but As...  [[B>A]]           0   \n",
       "1  Assistant A provided a more comprehensive answ...  [[A>B]]           1   \n",
       "2  Assistant A provided a concise and accurate se...  [[B>A]]           2   \n",
       "3  Assistant A provided a clear and concise solut...  [[B>A]]           5   \n",
       "4  Both answers are relevant and concise, but Ass...  [[B>A]]           7   \n",
       "\n",
       "            model_1           model_2  \n",
       "0  qwen-2.5-7b-base  qwen-2.5-7b-int4  \n",
       "1  qwen-2.5-7b-base  qwen-2.5-7b-int4  \n",
       "2  qwen-2.5-7b-base  qwen-2.5-7b-int4  \n",
       "3  qwen-2.5-7b-base  qwen-2.5-7b-int4  \n",
       "4  qwen-2.5-7b-base  qwen-2.5-7b-int4  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen_markup_data = pd.read_json(\"data/brain-hard-v0.1/qwen-2.5-7b-family.json\", lines=True)\n",
    "qwen_markup_data[\"answer\"] = qwen_markup_data[\"choices\"].map(lambda x: json.loads(x[0]['turns'][1]['content']))\n",
    "qwen_markup_data[\"explanation\"] = qwen_markup_data[\"answer\"].map(lambda x: x[\"explanation\"])\n",
    "qwen_markup_data[\"verdict\"] = qwen_markup_data[\"answer\"].map(lambda x: x[\"verdict\"])\n",
    "qwen_markup_data[\"id\"] = qwen_markup_data[\"question_id\"].map(lambda x: x.split('_')[0])\n",
    "qwen_markup_data[\"model_1\"] = qwen_markup_data[\"question_id\"].map(lambda x: x.split('_')[1])\n",
    "qwen_markup_data[\"model_2\"] = qwen_markup_data[\"question_id\"].map(lambda x: x.split('_')[2])\n",
    "qwen_markup_data.rename(columns={\"question_id\" : \"id\", \"id\": \"question_id\"}, inplace=True)\n",
    "\n",
    "qwen_markup_data = qwen_markup_data.groupby('id').first()\n",
    "qwen_markup_data = qwen_markup_data.reset_index()\n",
    "qwen_markup_data = pd.merge(\n",
    "    left=questions_qwen[['id', 'content']],  \n",
    "    right=qwen_markup_data,                     \n",
    "    on='id', \n",
    "    how='inner'\n",
    ")\n",
    "print(qwen_markup_data.shape)\n",
    "qwen_markup_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e5b4cfe7-1a89-4fb4-8df1-1130398e8629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Overall Statistics ===\n",
      "Total comparisons: 4740\n",
      "Verdict distribution: {'[[A>B]]': 3660, '[[B>A]]': 807, '[[A>>B]]': 138, '[[A=B]]': 124, '[[B>>A]]': 11}\n",
      "\n",
      "=== Model Performance ===\n",
      "qwen-2.5-7b-int4: 1190 wins, 1140 losses, 40 ties (50.2% win rate)\n",
      "qwen-2.5-7b-base: 1062 wins, 1220 losses, 88 ties (44.8% win rate)\n",
      "qwen-2.5-7b-int8: 1063 wins, 1217 losses, 90 ties (44.9% win rate)\n",
      "qwen-2.5-7b-awq: 1301 wins, 1039 losses, 30 ties (54.9% win rate)\n",
      "\n",
      "=== Model Permormance [Cleared inconsistencies]\n",
      "qwen-2.5-7b-int8: 836 pts (Strong: 27, Weak: 782, Ties: 744, Inconsistent: 654)\n",
      "qwen-2.5-7b-base: 826 pts (Strong: 26, Weak: 774, Ties: 760, Inconsistent: 672)\n",
      "qwen-2.5-7b-int4: 801 pts (Strong: 41, Weak: 719, Ties: 858, Inconsistent: 818)\n",
      "qwen-2.5-7b-awq: 800 pts (Strong: 37, Weak: 726, Ties: 854, Inconsistent: 824)\n",
      "\n",
      "=== Final Judgment Table ===\n",
      "  question_id           model_1           model_2  verdict  \\\n",
      "0           0  qwen-2.5-7b-base  qwen-2.5-7b-int4  [[B>A]]   \n",
      "1           1  qwen-2.5-7b-base  qwen-2.5-7b-int4  [[A>B]]   \n",
      "2           2  qwen-2.5-7b-base  qwen-2.5-7b-int4  [[B>A]]   \n",
      "3           5  qwen-2.5-7b-base  qwen-2.5-7b-int4  [[B>A]]   \n",
      "4           7  qwen-2.5-7b-base  qwen-2.5-7b-int4  [[B>A]]   \n",
      "\n",
      "                                         explanation  \n",
      "0  Both answers are accurate and relevant, but As...  \n",
      "1  Assistant A provided a more comprehensive answ...  \n",
      "2  Assistant A provided a concise and accurate se...  \n",
      "3  Assistant A provided a clear and concise solut...  \n",
      "4  Both answers are relevant and concise, but Ass...  \n",
      "\n",
      "=== Top 5 Significant Differences ===\n",
      "\n",
      "Question ID: 64_qwen-2.5-7b-base_qwen-2.5-7b-int4\n",
      "Models: qwen-2.5-7b-base vs qwen-2.5-7b-int4\n",
      "Verdict: [[A>>B]]\n",
      "Explanation: Assistant A provided the correct definition of a double-double, including rebounds as one of the possible categories. Assistant B mistakenly included assists as one of the required stats for a double-double, which is incorrect.\n",
      "\n",
      "=== Question ===\n",
      "To get a double-double in a basketball game,\n",
      "\n",
      "=== Assistant A ===\n",
      "To get a double-double in a basketball game, a player must record at least 10 points and 10 rebounds, or any other combination of 10 or more in two of the following categories: points, rebounds, assists, steals, and blocks.\n",
      "\n",
      "=== Assistant B ===\n",
      "To get a double-double in a basketball game, a player must record at least 10 points and 10 assists, or any other combination of 10 or more in two of the following categories: points, rebounds, assists, steals, and blocks.\n",
      "--------------------------------------------------\n",
      "\n",
      "Question ID: 162_qwen-2.5-7b-base_qwen-2.5-7b-int4\n",
      "Models: qwen-2.5-7b-base vs qwen-2.5-7b-int4\n",
      "Verdict: [[A>>B]]\n",
      "Explanation: Assistant A provided the correct year and opponent, while Assistant B gave incorrect information about both the year and the opponent. Assistant A's answer is more accurate and relevant.\n",
      "\n",
      "=== Question ===\n",
      "who did the Houston Astros play in the world series last year\n",
      "\n",
      "=== Assistant A ===\n",
      "The Houston Astros played the Washington Nationals in the 2019 World Series.\n",
      "\n",
      "=== Assistant B ===\n",
      "The Houston Astros played against the Atlanta Braves in the 2022 World Series.\n",
      "--------------------------------------------------\n",
      "\n",
      "Question ID: 283_qwen-2.5-7b-base_qwen-2.5-7b-int4\n",
      "Models: qwen-2.5-7b-base vs qwen-2.5-7b-int4\n",
      "Verdict: [[A>>B]]\n",
      "Explanation: Assistant A provided a more accurate and detailed answer, including the correct date of the formal transfer of the territory and the statehood date. Assistant B's answer was less accurate and missing important details.\n",
      "\n",
      "=== Question ===\n",
      "when did louisiana became a part of The United States\n",
      "\n",
      "=== Assistant A ===\n",
      "Louisiana became a part of the United States on December 20, 1803, when it was purchased from France in the Louisiana Purchase. However, the territory was admitted as the 18th state on April 30, 1812. The Louisiana Purchase was officially completed on October 20, 1803, when the formal transfer of the territory was finalized.\n",
      "\n",
      "=== Assistant B ===\n",
      "Louisiana became a part of the United States on April 30, 1803, with the signing of the Louisiana Purchase agreement.\n",
      "--------------------------------------------------\n",
      "\n",
      "Question ID: 539_qwen-2.5-7b-base_qwen-2.5-7b-int4\n",
      "Models: qwen-2.5-7b-base vs qwen-2.5-7b-int4\n",
      "Verdict: [[B>>A]]\n",
      "Explanation: Assistant B provided a more accurate and natural translation of the phrase. Assistant A's translation was a literal and incorrect attempt, while Assistant B's translation accurately conveys the meaning of the English phrase.\n",
      "\n",
      "=== Question ===\n",
      "Translate \"You better call.\" to Czech?\n",
      "\n",
      "=== Assistant A ===\n",
      "\"Lepší se vyspáš.\" \n",
      "\n",
      "Note: This literal translation might not convey the same meaning or urgency as the English phrase. A more appropriate translation would be: \"Lepší zavoláš.\"\n",
      "\n",
      "=== Assistant B ===\n",
      "\"Lepší, že zavoláš.\"\n",
      "--------------------------------------------------\n",
      "\n",
      "Question ID: 553_qwen-2.5-7b-base_qwen-2.5-7b-int4\n",
      "Models: qwen-2.5-7b-base vs qwen-2.5-7b-int4\n",
      "Verdict: [[A>>B]]\n",
      "Explanation: Assistant A provided a detailed and accurate step-by-step guide on how to add a new remote control through the Appstore, which is the correct method. Assistant B's answer, while relevant, is incorrect as the Fire TV Stick does not have a built-in feature to add new remote controls directly from settings. Assistant A's answer is more helpful and accurate.\n",
      "\n",
      "=== Question ===\n",
      "Como agregar un nuevo control a mi Fire Stick\n",
      "\n",
      "=== Assistant A ===\n",
      "Para agregar un nuevo control a tu Fire TV Stick, puedes seguir estos pasos:\n",
      "\n",
      "1. **Descargar el control remoto**: Visita la tienda de aplicaciones de Amazon (Appstore) en tu Fire TV Stick.\n",
      "2. **Buscar la aplicación de control remoto**: Busca la aplicación que desees instalar.\n",
      "3. **Instalar la aplicación**: Selecciona la aplicación y sigue las instrucciones para instalarla.\n",
      "4. **Configurar el control remoto**: Algunas aplicaciones te permiten configurar el control remoto para que funcione según tus preferencias.\n",
      "\n",
      "Recuerda que no todos los controles remotos están disponibles en la Appstore de Amazon, y algunos pueden requerir un suscripción o pago adicional.\n",
      "\n",
      "=== Assistant B ===\n",
      "Para agregar un nuevo control a tu Fire TV Stick, puedes seguir estos pasos:\n",
      "\n",
      "1. Abre la aplicación \"Fire TV Settings\" en tu Fire TV Stick.\n",
      "2. Navega hasta \"Controles remotos y dispositivos\".\n",
      "3. Selecciona \"Controles remotos adicionales\".\n",
      "4. Sigue las instrucciones para conectar y configurar el nuevo control remoto.\n",
      "\n",
      "Recuerda que no todos los controles remotos son compatibles con el Fire TV Stick, así que asegúrate de que el que deseas agregar sea compatible.\n",
      "--------------------------------------------------\n",
      "\n",
      "🔍 Found 1569 inconsistent model pairs across 382 questions\n",
      "\n",
      "📊 Verdict Combinations:\n",
      "╭────────────────┬──────────────────────┬───────────────────────┬──────────────────────┬────────────────────────┬───────────────────────┬──────────────────────┬───────────────────────┬───────────────────────╮\n",
      "│ Verdict Pair   │   [[A>B]] vs [[A>B]] │   [[A>>B]] vs [[A>B]] │   [[B>A]] vs [[B>A]] │   [[A>>B]] vs [[A>>B]] │   [[A>>B]] vs [[B>A]] │   [[A=B]] vs [[A>B]] │   [[A>B]] vs [[B>>A]] │   [[B>>A]] vs [[B>A]] │\n",
      "├────────────────┼──────────────────────┼───────────────────────┼──────────────────────┼────────────────────────┼───────────────────────┼──────────────────────┼───────────────────────┼───────────────────────┤\n",
      "│ Total          │                 1432 │                    56 │                   27 │                     27 │                    19 │                    6 │                     1 │                     1 │\n",
      "╰────────────────┴──────────────────────┴───────────────────────┴──────────────────────┴────────────────────────┴───────────────────────┴──────────────────────┴───────────────────────┴───────────────────────╯\n",
      "\n",
      "❓ Top Questions with Inconsistencies:\n",
      "╭───────────────┬───────────────────╮\n",
      "│   Question ID │   Inconsistencies │\n",
      "├───────────────┼───────────────────┤\n",
      "│          1929 │                12 │\n",
      "├───────────────┼───────────────────┤\n",
      "│          1948 │                12 │\n",
      "├───────────────┼───────────────────┤\n",
      "│          1864 │                12 │\n",
      "├───────────────┼───────────────────┤\n",
      "│          1876 │                12 │\n",
      "├───────────────┼───────────────────┤\n",
      "│            18 │                12 │\n",
      "╰───────────────┴───────────────────╯\n",
      "\n",
      "=== Question 1929 ===\n",
      "How can I stop a run in pantyhose from getting bigger?\n",
      "\n",
      "=== Question 1948 ===\n",
      "Given the following question, let's solve step-by-step. Is the following sentence factually correct?\n",
      "\"Judas Iscariot founded the company Delta Airlines.\"\n",
      "Options:\n",
      "- yes\n",
      "- no\n",
      "\n",
      "=== Question 1864 ===\n",
      "Instructions: Classify the given tweet into the three categories: (1) 'Hate Speech', (2) 'Offensive' and (3) 'Neither'. 'Hate Speech' is kind of a threating statement or sometimes include call for violence while 'offensive' statement just offensds someone. 'Neither' is when it doesn't fall into Hate Speech or Offensive category.\n",
      "Input: They slut for me they kill for me they steal for me and of course it'll be yo cash and I'll murder that bitch send the body back to yo ass\n",
      "Output:\n",
      "\n",
      "=== Question 1876 ===\n",
      "How to decorate for the Holidays.\n",
      "\n",
      "=== Question 18 ===\n",
      "Give car tires traction.\n",
      "\n",
      "\n",
      "🤖 Top Model Pairs with Inconsistencies:\n",
      "╭──────────────────────────────────────┬───────────────────╮\n",
      "│ Model Pair                           │   Inconsistencies │\n",
      "├──────────────────────────────────────┼───────────────────┤\n",
      "│ qwen-2.5-7b-base vs qwen-2.5-7b-int8 │               628 │\n",
      "├──────────────────────────────────────┼───────────────────┤\n",
      "│ qwen-2.5-7b-awq vs qwen-2.5-7b-int4  │               526 │\n",
      "├──────────────────────────────────────┼───────────────────┤\n",
      "│ qwen-2.5-7b-int4 vs qwen-2.5-7b-int8 │               502 │\n",
      "├──────────────────────────────────────┼───────────────────┤\n",
      "│ qwen-2.5-7b-awq vs qwen-2.5-7b-int8  │               498 │\n",
      "├──────────────────────────────────────┼───────────────────┤\n",
      "│ qwen-2.5-7b-awq vs qwen-2.5-7b-base  │               496 │\n",
      "╰──────────────────────────────────────┴───────────────────╯\n",
      "\n",
      "📝 Example Inconsistent Judgments (first 3):\n",
      "\n",
      "Question 0: qwen-2.5-7b-base vs qwen-2.5-7b-awq\n",
      "• Judgment 1: [[A>B]] - Assistant A provided a more comprehensive and accurate answer, including multiple benefits such as improving soil structure, increasing nutrient availability, and enhancing water infiltration. Assistant B's answer was concise but less detailed and slightly less accurate in explaining the full range of benefits.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question 0: qwen-2.5-7b-awq vs qwen-2.5-7b-int4\n",
      "• Judgment 1: [[A>B]] - Assistant B provided a more comprehensive and detailed answer, including additional relevant information about earthworm castings and soil compaction. However, Assistant A's answer was still accurate and concise, making it slightly better.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question 0: qwen-2.5-7b-awq vs qwen-2.5-7b-int8\n",
      "• Judgment 1: [[A>B]] - Both answers are accurate and relevant, but Assistant A's answer is slightly more concise and directly addresses the question without unnecessary details. Assistant B's answer, while comprehensive, includes some additional information that is not directly asked for in the prompt.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_report(qwen_markup_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d14444c0-1c70-447a-baea-39bc2bfa28bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|User Prompt|>\\nEarthworms live underground in the soil. As they move through the soil, they create tunnels. The tunnels help improve the soil. Plants grow better in soil that has earthworms. Which statement explains how earthworm tunnels help plants?\\n\\n<|The Start of Assistant A\\'s Answer|>\\nEarthworm tunnels help plants by improving soil aeration, allowing for better water and nutrient absorption, and increasing soil porosity, which supports healthier root growth and overall plant health.\\n<|The End of Assistant A\\'s Answer|>\\n\\n<|The Start of Assistant B\\'s Answer|>\\nEarthworm tunnels help plants in several ways, primarily by improving soil structure and aeration. Here\\'s a statement that explains how earthworm tunnels contribute to plant growth:\\n\\n\"Earthworm tunnels increase soil aeration and create pathways for water and nutrients to reach plant roots more easily, which in turn supports healthier plant growth.\"\\n\\nThis statement highlights the key benefits of earthworm tunnels, such as improved aeration and better access to water and nutrients, which are crucial for plant health and growth.\\n<|The End of Assistant B\\'s Answer|>'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen_markup_data['content'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4756d4a-f606-4694-b0bb-40bddf49d150",
   "metadata": {},
   "source": [
    "### Falcon GPT Markup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "81916eaf-5037-4467-9290-e2163c31317e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions: 4740\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>model_2</th>\n",
       "      <th>model_1</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_falcon-7b-int8_falcon-7b-base</td>\n",
       "      <td>0</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "      <td>falcon-7b-int8</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nEarthworms live underground i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_falcon-7b-int8_falcon-7b-base</td>\n",
       "      <td>1</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "      <td>falcon-7b-int8</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nHow can I temporarily seal a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2_falcon-7b-int8_falcon-7b-base</td>\n",
       "      <td>2</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "      <td>falcon-7b-int8</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nHow to clean earbuds\\n\\n&lt;|The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5_falcon-7b-int8_falcon-7b-base</td>\n",
       "      <td>5</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "      <td>falcon-7b-int8</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nTo hold open a bedroom door w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7_falcon-7b-int8_falcon-7b-base</td>\n",
       "      <td>7</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "      <td>falcon-7b-int8</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nTo add a surprising twist to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id  question_id         model_2  \\\n",
       "0  0_falcon-7b-int8_falcon-7b-base            0  falcon-7b-base   \n",
       "1  1_falcon-7b-int8_falcon-7b-base            1  falcon-7b-base   \n",
       "2  2_falcon-7b-int8_falcon-7b-base            2  falcon-7b-base   \n",
       "3  5_falcon-7b-int8_falcon-7b-base            5  falcon-7b-base   \n",
       "4  7_falcon-7b-int8_falcon-7b-base            7  falcon-7b-base   \n",
       "\n",
       "          model_1                                            content  \n",
       "0  falcon-7b-int8  <|User Prompt|>\\nEarthworms live underground i...  \n",
       "1  falcon-7b-int8  <|User Prompt|>\\nHow can I temporarily seal a ...  \n",
       "2  falcon-7b-int8  <|User Prompt|>\\nHow to clean earbuds\\n\\n<|The...  \n",
       "3  falcon-7b-int8  <|User Prompt|>\\nTo hold open a bedroom door w...  \n",
       "4  falcon-7b-int8  <|User Prompt|>\\nTo add a surprising twist to ...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_falcon = pd.read_json(\"data/brain-hard-v0.1/model_judgment/falcon-7b-family-questions.json\", lines=True)\n",
    "print(f\"Total questions: {questions_falcon.shape[0]}\")\n",
    "questions_falcon = questions_falcon[[\"question_id\", \"old_question_id\", \"model_2\", \"model_1\", \"turns\"]]\n",
    "questions_falcon['content'] = questions_falcon[\"turns\"].map(lambda x: x[1]['content'])\n",
    "questions_falcon.drop(columns = ['turns'], inplace=True)\n",
    "questions_falcon.rename(columns = {'question_id': 'id', 'old_question_id': 'question_id'}, inplace=True)\n",
    "questions_falcon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "99ba36bb-da01-4435-be2e-4aadf10c4d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>question_id</th>\n",
       "      <th>model_2</th>\n",
       "      <th>model_1</th>\n",
       "      <th>conv</th>\n",
       "      <th>judge</th>\n",
       "      <th>answer</th>\n",
       "      <th>explanation</th>\n",
       "      <th>verdict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_falcon-7b-int8_falcon-7b-base</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nEarthworms live underground i...</td>\n",
       "      <td>0</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "      <td>falcon-7b-int8</td>\n",
       "      <td>[{'role': 'system', 'content': 'Please act as ...</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>{'explanation': 'Both Assistant A and Assistan...</td>\n",
       "      <td>Both Assistant A and Assistant B provided iden...</td>\n",
       "      <td>[[A=B]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_falcon-7b-int8_falcon-7b-base</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nHow can I temporarily seal a ...</td>\n",
       "      <td>1</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "      <td>falcon-7b-int8</td>\n",
       "      <td>[{'role': 'system', 'content': 'Please act as ...</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>{'explanation': 'Both assistants provide a ste...</td>\n",
       "      <td>Both assistants provide a step-by-step guide o...</td>\n",
       "      <td>[[A&gt;B]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2_falcon-7b-int8_falcon-7b-base</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nHow to clean earbuds\\n\\n&lt;|The...</td>\n",
       "      <td>2</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "      <td>falcon-7b-int8</td>\n",
       "      <td>[{'role': 'system', 'content': 'Please act as ...</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>{'explanation': 'Both assistants provide a ste...</td>\n",
       "      <td>Both assistants provide a step-by-step guide o...</td>\n",
       "      <td>[[B&gt;A]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5_falcon-7b-int8_falcon-7b-base</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nTo hold open a bedroom door w...</td>\n",
       "      <td>5</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "      <td>falcon-7b-int8</td>\n",
       "      <td>[{'role': 'system', 'content': 'Please act as ...</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>{'explanation': 'Both assistants provided iden...</td>\n",
       "      <td>Both assistants provided identical responses, ...</td>\n",
       "      <td>[[A=B]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7_falcon-7b-int8_falcon-7b-base</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nTo add a surprising twist to ...</td>\n",
       "      <td>7</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "      <td>falcon-7b-int8</td>\n",
       "      <td>[{'role': 'system', 'content': 'Please act as ...</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>{'explanation': 'Both assistants provide the s...</td>\n",
       "      <td>Both assistants provide the same suggestion of...</td>\n",
       "      <td>[[A=B]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id  \\\n",
       "0  0_falcon-7b-int8_falcon-7b-base   \n",
       "1  1_falcon-7b-int8_falcon-7b-base   \n",
       "2  2_falcon-7b-int8_falcon-7b-base   \n",
       "3  5_falcon-7b-int8_falcon-7b-base   \n",
       "4  7_falcon-7b-int8_falcon-7b-base   \n",
       "\n",
       "                                             content  question_id  \\\n",
       "0  <|User Prompt|>\\nEarthworms live underground i...            0   \n",
       "1  <|User Prompt|>\\nHow can I temporarily seal a ...            1   \n",
       "2  <|User Prompt|>\\nHow to clean earbuds\\n\\n<|The...            2   \n",
       "3  <|User Prompt|>\\nTo hold open a bedroom door w...            5   \n",
       "4  <|User Prompt|>\\nTo add a surprising twist to ...            7   \n",
       "\n",
       "          model_2         model_1  \\\n",
       "0  falcon-7b-base  falcon-7b-int8   \n",
       "1  falcon-7b-base  falcon-7b-int8   \n",
       "2  falcon-7b-base  falcon-7b-int8   \n",
       "3  falcon-7b-base  falcon-7b-int8   \n",
       "4  falcon-7b-base  falcon-7b-int8   \n",
       "\n",
       "                                                conv   judge  \\\n",
       "0  [{'role': 'system', 'content': 'Please act as ...  gpt-4o   \n",
       "1  [{'role': 'system', 'content': 'Please act as ...  gpt-4o   \n",
       "2  [{'role': 'system', 'content': 'Please act as ...  gpt-4o   \n",
       "3  [{'role': 'system', 'content': 'Please act as ...  gpt-4o   \n",
       "4  [{'role': 'system', 'content': 'Please act as ...  gpt-4o   \n",
       "\n",
       "                                              answer  \\\n",
       "0  {'explanation': 'Both Assistant A and Assistan...   \n",
       "1  {'explanation': 'Both assistants provide a ste...   \n",
       "2  {'explanation': 'Both assistants provide a ste...   \n",
       "3  {'explanation': 'Both assistants provided iden...   \n",
       "4  {'explanation': 'Both assistants provide the s...   \n",
       "\n",
       "                                         explanation  verdict  \n",
       "0  Both Assistant A and Assistant B provided iden...  [[A=B]]  \n",
       "1  Both assistants provide a step-by-step guide o...  [[A>B]]  \n",
       "2  Both assistants provide a step-by-step guide o...  [[B>A]]  \n",
       "3  Both assistants provided identical responses, ...  [[A=B]]  \n",
       "4  Both assistants provide the same suggestion of...  [[A=B]]  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_falcon_data = pd.read_json(\"data/brain-hard-v0.1/model_judgment/falcon-7b-markup-gpt\")\n",
    "gpt_falcon_data[\"answer\"] = gpt_falcon_data[\"answer\"].map(json.loads)\n",
    "gpt_falcon_data[\"explanation\"] = gpt_falcon_data[\"answer\"].map(lambda x: x[\"explanation\"])\n",
    "gpt_falcon_data[\"verdict\"] = gpt_falcon_data[\"answer\"].map(lambda x: x[\"verdict\"])\n",
    "gpt_falcon_data = gpt_falcon_data.groupby('id').first()\n",
    "\n",
    "gpt_falcon_data = gpt_falcon_data.reset_index()\n",
    "gpt_falcon_data = pd.merge(\n",
    "    left=questions_falcon[['id', 'content']],  \n",
    "    right=gpt_falcon_data,                     \n",
    "    on='id', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "gpt_falcon_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2b03c25d-5b42-4f3a-acfc-02e6e461db6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Overall Statistics ===\n",
      "Total comparisons: 4740\n",
      "Verdict distribution: {'[[A=B]]': 1594, '[[A>B]]': 1464, '[[B>A]]': 1387, '[[A>>B]]': 183, '[[B>>A]]': 112}\n",
      "\n",
      "=== Model Performance ===\n",
      "falcon-7b-int8: 711 wins, 677 losses, 982 ties (30.0% win rate)\n",
      "falcon-7b-base: 721 wins, 653 losses, 996 ties (30.4% win rate)\n",
      "falcon-7b-int4: 763 wins, 1046 losses, 561 ties (32.2% win rate)\n",
      "falcon-7b-awq: 951 wins, 770 losses, 649 ties (40.1% win rate)\n",
      "\n",
      "=== Model Permormance [Cleared inconsistencies]\n",
      "falcon-7b-awq: 300 pts (Strong: 41, Weak: 218, Ties: 1875, Inconsistent: 1226)\n",
      "falcon-7b-int4: 298 pts (Strong: 33, Weak: 232, Ties: 1833, Inconsistent: 1272)\n",
      "falcon-7b-base: 225 pts (Strong: 22, Weak: 181, Ties: 1964, Inconsistent: 968)\n",
      "falcon-7b-int8: 194 pts (Strong: 17, Weak: 160, Ties: 2000, Inconsistent: 1018)\n",
      "\n",
      "=== Final Judgment Table ===\n",
      "   question_id         model_1         model_2  verdict  \\\n",
      "0            0  falcon-7b-int8  falcon-7b-base  [[A=B]]   \n",
      "1            1  falcon-7b-int8  falcon-7b-base  [[A>B]]   \n",
      "2            2  falcon-7b-int8  falcon-7b-base  [[B>A]]   \n",
      "3            5  falcon-7b-int8  falcon-7b-base  [[A=B]]   \n",
      "4            7  falcon-7b-int8  falcon-7b-base  [[A=B]]   \n",
      "\n",
      "                                         explanation  \n",
      "0  Both Assistant A and Assistant B provided iden...  \n",
      "1  Both assistants provide a step-by-step guide o...  \n",
      "2  Both assistants provide a step-by-step guide o...  \n",
      "3  Both assistants provided identical responses, ...  \n",
      "4  Both assistants provide the same suggestion of...  \n",
      "\n",
      "=== Top 5 Significant Differences ===\n",
      "\n",
      "Question ID: 211_falcon-7b-int8_falcon-7b-base\n",
      "Models: falcon-7b-int8 vs falcon-7b-base\n",
      "Verdict: [[A>>B]]\n",
      "Explanation: Assistant A provides a correct method for making a soft-boiled egg, including the important step of using an ice bath to stop the cooking process, which is more effective than just running cold water as suggested by Assistant B. Assistant B incorrectly instructs to crack the egg into a bowl before boiling, which is not typical for making soft-boiled eggs. Therefore, Assistant A's response is more accurate and helpful.\n",
      "\n",
      "=== Question ===\n",
      "Make a soft boiled egg ( a three minute egg)\n",
      "\n",
      "=== Assistant A ===\n",
      "To make a soft-boiled egg (3 minutes), follow these steps:\n",
      "\n",
      "1. Fill a medium saucepan with water and bring it to a rolling boil.\n",
      "2. Carefully lower the eggs into the boiling water using a slotted spoon or tongs.\n",
      "3. Set a timer for 3 minutes.\n",
      "4. Once the timer goes off, immediately drain the hot water from the saucepan and transfer the eggs to a bowl of ice water to stop the cooking process.\n",
      "5. Let the eggs sit in the ice water for at least 5 minutes before peeling.\n",
      "\n",
      "Enjoy your soft-boiled egg!\n",
      "\n",
      "=== Assistant B ===\n",
      "To make a soft-boiled egg (3 minutes), follow these steps:\n",
      "\n",
      "1. Fill a medium saucepan with water and bring it to a rolling boil.\n",
      "2. Gently crack an egg into a small bowl or cup.\n",
      "3. Using a slotted spoon, carefully lower the egg into the boiling water.\n",
      "4. Set a timer for 3 minutes.\n",
      "5. Once the timer goes off, immediately remove the egg from the water using the slotted spoon.\n",
      "6. Run cold water over the egg to stop the cooking process.\n",
      "7. Peel the egg and enjoy!\n",
      "--------------------------------------------------\n",
      "\n",
      "Question ID: 1140_falcon-7b-int8_falcon-7b-base\n",
      "Models: falcon-7b-int8 vs falcon-7b-base\n",
      "Verdict: [[A>>B]]\n",
      "Explanation: Assistant A correctly identifies that the actual playing time in an American football game is around 11 to 15 minutes, which is a widely accepted estimate. Assistant B incorrectly states that the playing time is around 110 minutes, which is inaccurate as it exceeds the total duration of the game including downtime. Therefore, Assistant A provides a more accurate and relevant response to the user's question.\n",
      "\n",
      "=== Question ===\n",
      "how long does an American football match REALLY last, if you substract all the downtime?\n",
      "\n",
      "=== Assistant A ===\n",
      "An American football match, including all play, lasts approximately 3 hours and 12 minutes. However, this includes time for stoppages such as timeouts, halftime, and injuries. If you subtract all the downtime, the actual playing time is around 13 minutes and 20 seconds.\n",
      "\n",
      "=== Assistant B ===\n",
      "An American football match, including all play, lasts approximately 3 hours and 12 minutes. However, this includes halftime, timeouts, and other stoppages. If you subtract all the downtime, the actual playing time is around 110 minutes (1 hour and 50 minutes).\n",
      "--------------------------------------------------\n",
      "\n",
      "Question ID: 1220_falcon-7b-int8_falcon-7b-base\n",
      "Models: falcon-7b-int8 vs falcon-7b-base\n",
      "Verdict: [[B>>A]]\n",
      "Explanation: Assistant B provides a comprehensive list of strategies to calm nerves before a speech, covering various techniques such as practice, deep breathing, and positive self-talk, making it more helpful and detailed than Assistant A's brief response. Assistant A's answer is concise but lacks the depth and variety of methods that B offers, which are beneficial for the user seeking advice on calming nerves.\n",
      "\n",
      "=== Question ===\n",
      "Are you nervous about giving a speech or doing something?  How can you calm yourself?\n",
      "\n",
      "=== Assistant A ===\n",
      "Yes, I can be nervous about giving a speech or performing in front of others. To calm myself, I practice deep breathing exercises, visualize success, and focus on the message I want to convey rather than my anxiety.\n",
      "\n",
      "=== Assistant B ===\n",
      "Yes, it's common to feel nervous about giving a speech or performing in front of others. Here are some ways to help calm yourself:\n",
      "\n",
      "1. Practice: Rehearse your speech or performance multiple times to build confidence and familiarity with the material.\n",
      "\n",
      "2. Deep breathing: Take slow, deep breaths to help reduce anxiety and promote relaxation.\n",
      "\n",
      "3. Visualization: Imagine yourself successfully completing the task, which can help boost your confidence.\n",
      "\n",
      "4. Positive self-talk: Replace negative thoughts with positive affirmations to maintain a confident mindset.\n",
      "\n",
      "5. Progressive muscle relaxation: Tense and then relax different muscle groups to release physical tension.\n",
      "\n",
      "6. Focus on the message: Concentrate on the content of your speech or performance rather than your nerves.\n",
      "\n",
      "7. Arrive early: Give yourself plenty of time to get settled and familiarize yourself with the environment.\n",
      "\n",
      "8. Connect with the audience: Engage with your listeners and remember that they want you to succeed.\n",
      "\n",
      "9. Seek support: Talk to friends, family, or a mentor about your nerves and gain their encouragement.\n",
      "\n",
      "10. Remember it's okay to make mistakes: Nobody is perfect, and a few slip-ups won't ruin the entire experience.\n",
      "\n",
      "By incorporating these strategies into your preparation and mindset, you can help manage your nerves and deliver a successful speech or performance.\n",
      "--------------------------------------------------\n",
      "\n",
      "Question ID: 167_falcon-7b-int4_falcon-7b-awq\n",
      "Models: falcon-7b-int4 vs falcon-7b-awq\n",
      "Verdict: [[A>>B]]\n",
      "Explanation: Assistant A correctly identifies that the sales of ice cream are not directly affected by the breakdown of the sunglasses truck, as the correlation is due to external factors like weather, not the availability of sunglasses. Assistant B incorrectly suggests that ice cream sales would decrease due to the correlation, misunderstanding the nature of correlation versus causation.\n",
      "\n",
      "=== Question ===\n",
      "Il y a une forte corrélation entre les ventes de lunettes de soleil et les ventes de glaces. Pendant les journées chaudes et ensoleillées, les deux se vendent beaucoup plus ; et pendant les journées plus froides et plus nuageuses, les deux se vendent beaucoup moins. Un jour, le camion de lunettes de soleil tombe en panne sur l'autoroute avant d'arriver au magasin, et les ventes de lunettes de soleil tombent à zéro. Qu'arrive-t-il aux ventes de crème glacée ?\n",
      "\n",
      "=== Assistant A ===\n",
      "Les ventes de crème glacée ne sont pas directement affectées par la panne du camion de lunettes de soleil. Les ventes de crème glacée sont généralement influencées par la température extérieure et les tendances saisonnières, plutôt que par les ventes de lunettes de soleil.\n",
      "\n",
      "=== Assistant B ===\n",
      "Les ventes de crème glacée devraient également diminuer, car il y a une forte corrélation entre les deux produits. Les jours plus chauds et ensoleillés favorisent à la fois les achats de lunettes de soleil et de crème glacée, tandis que les jours plus froids et nuageux entraînent une baisse des ventes pour les deux produits. Cependant, il est important de noter que cette corrélation peut varier en fonction de nombreux autres facteurs, tels que la saison, les tendances de consommation, et d'autres événements ou promotions spécifiques.\n",
      "--------------------------------------------------\n",
      "\n",
      "Question ID: 211_falcon-7b-int4_falcon-7b-awq\n",
      "Models: falcon-7b-int4 vs falcon-7b-awq\n",
      "Verdict: [[A>>B]]\n",
      "Explanation: Assistant A provides a correct method for making a soft-boiled egg by boiling the eggs in their shells, which is the standard approach. Assistant B incorrectly suggests cracking the eggs into the boiling water, which is not a typical method for making soft-boiled eggs and would result in poached eggs instead. Therefore, Assistant A's response is more accurate and relevant to the prompt.\n",
      "\n",
      "=== Question ===\n",
      "Make a soft boiled egg ( a three minute egg)\n",
      "\n",
      "=== Assistant A ===\n",
      "To make a soft-boiled egg (a three-minute egg), follow these steps:\n",
      "\n",
      "1. Fill a saucepan with water and bring it to a rolling boil.\n",
      "2. Carefully lower the eggs into the boiling water using a slotted spoon or tongs.\n",
      "3. Start the timer as soon as the eggs are in the water.\n",
      "4. Cook the eggs for exactly 3 minutes.\n",
      "5. Immediately transfer the eggs to an ice bath (a bowl filled with cold water and ice) to stop the cooking process.\n",
      "6. Let the eggs cool for about 10 minutes before peeling them.\n",
      "\n",
      "Enjoy your soft-boiled egg!\n",
      "\n",
      "=== Assistant B ===\n",
      "To make a soft-boiled egg (also known as a \"three-minute egg\"), follow these steps:\n",
      "\n",
      "1. Fill a medium-sized pot with water and bring it to a rolling boil.\n",
      "2. Carefully crack one or two eggs into the pot, ensuring they are fully submerged in the water.\n",
      "3. Start a timer for 3 minutes.\n",
      "4. Once the time is up, immediately remove the eggs from the pot using a slotted spoon.\n",
      "5. Place the eggs in a bowl of ice water to stop the cooking process and make them easier to peel.\n",
      "6. After about 5 minutes, the eggs should be cool enough to handle. Gently tap the egg on a hard surface to create a small crack, then peel the shell off.\n",
      "\n",
      "Your soft-boiled egg is now ready to eat. Enjoy!\n",
      "--------------------------------------------------\n",
      "\n",
      "🔍 Found 594 inconsistent model pairs across 277 questions\n",
      "\n",
      "📊 Verdict Combinations:\n",
      "╭────────────────┬──────────────────────┬──────────────────────┬──────────────────────┬───────────────────────┬──────────────────────┬───────────────────────┬───────────────────────┬────────────────────────┬────────────────────────┬───────────────────────┬───────────────────────┬───────────────────────╮\n",
      "│ Verdict Pair   │   [[A=B]] vs [[A>B]] │   [[A>B]] vs [[A>B]] │   [[A=B]] vs [[B>A]] │   [[A>>B]] vs [[B>A]] │   [[B>A]] vs [[B>A]] │   [[A>B]] vs [[B>>A]] │   [[A>>B]] vs [[A>B]] │   [[A>>B]] vs [[A>>B]] │   [[B>>A]] vs [[B>>A]] │   [[B>>A]] vs [[B>A]] │   [[A=B]] vs [[A>>B]] │   [[A=B]] vs [[B>>A]] │\n",
      "├────────────────┼──────────────────────┼──────────────────────┼──────────────────────┼───────────────────────┼──────────────────────┼───────────────────────┼───────────────────────┼────────────────────────┼────────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┤\n",
      "│ Total          │                  149 │                  133 │                  133 │                    75 │                   74 │                    13 │                     6 │                      5 │                      3 │                     1 │                     1 │                     1 │\n",
      "╰────────────────┴──────────────────────┴──────────────────────┴──────────────────────┴───────────────────────┴──────────────────────┴───────────────────────┴───────────────────────┴────────────────────────┴────────────────────────┴───────────────────────┴───────────────────────┴───────────────────────╯\n",
      "\n",
      "❓ Top Questions with Inconsistencies:\n",
      "╭───────────────┬───────────────────╮\n",
      "│   Question ID │   Inconsistencies │\n",
      "├───────────────┼───────────────────┤\n",
      "│          1462 │                12 │\n",
      "├───────────────┼───────────────────┤\n",
      "│          1742 │                12 │\n",
      "├───────────────┼───────────────────┤\n",
      "│          1433 │                10 │\n",
      "├───────────────┼───────────────────┤\n",
      "│          1845 │                10 │\n",
      "├───────────────┼───────────────────┤\n",
      "│           722 │                10 │\n",
      "╰───────────────┴───────────────────╯\n",
      "\n",
      "=== Question 1462 ===\n",
      "Was ist der Unterschied zwischen Backstahl und Pizzastein? Was davon sollte ich für Brötchen verwenden?\n",
      "\n",
      "=== Question 1742 ===\n",
      "How to remove the seems from jeans and turn into a skirt?\n",
      "\n",
      "=== Question 1433 ===\n",
      "What determines how long the Moon takes to complete one cycle of phases?\n",
      "\n",
      "=== Question 1845 ===\n",
      "To convert cheese into liquid state, you can\n",
      "\n",
      "=== Question 722 ===\n",
      "Create back rest for sink repair.\n",
      "\n",
      "\n",
      "🤖 Top Model Pairs with Inconsistencies:\n",
      "╭──────────────────────────────────┬───────────────────╮\n",
      "│ Model Pair                       │   Inconsistencies │\n",
      "├──────────────────────────────────┼───────────────────┤\n",
      "│ falcon-7b-awq vs falcon-7b-int4  │               254 │\n",
      "├──────────────────────────────────┼───────────────────┤\n",
      "│ falcon-7b-base vs falcon-7b-int4 │               234 │\n",
      "├──────────────────────────────────┼───────────────────┤\n",
      "│ falcon-7b-int4 vs falcon-7b-int8 │               210 │\n",
      "├──────────────────────────────────┼───────────────────┤\n",
      "│ falcon-7b-awq vs falcon-7b-base  │               208 │\n",
      "├──────────────────────────────────┼───────────────────┤\n",
      "│ falcon-7b-awq vs falcon-7b-int8  │               186 │\n",
      "╰──────────────────────────────────┴───────────────────╯\n",
      "\n",
      "📝 Example Inconsistent Judgments (first 3):\n",
      "\n",
      "Question 0: falcon-7b-awq vs falcon-7b-base\n",
      "• Judgment 1: [[A>B]] - Both assistants provide similar explanations on how earthworm tunnels benefit plants by improving soil aeration and water infiltration, which enhances root growth and nutrient availability. Assistant A's response is slightly more detailed by mentioning the improvement of soil structure, which adds a bit more depth to the explanation.\n",
      "• Judgment 2: [[A=B]] - Both assistants provide accurate and relevant explanations of how earthworm tunnels benefit plants by improving soil aeration and water infiltration, which enhances root growth and nutrient availability. The responses are concise and cover the essential points without unnecessary information. There is no significant difference in the quality of the answers provided by both assistants.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question 0: falcon-7b-int4 vs falcon-7b-awq\n",
      "• Judgment 1: [[A=B]] - Both assistants provide accurate explanations of how earthworm tunnels benefit plants by improving soil aeration, water infiltration, and nutrient availability, which enhance root growth and plant health. The responses are concise and relevant, with no significant differences in quality or content.\n",
      "• Judgment 2: [[A>B]] - Both assistants provide accurate explanations of how earthworm tunnels benefit plants by improving soil aeration and water infiltration, which enhances root growth and nutrient availability. However, Assistant A's response is slightly more detailed, mentioning oxygen penetration and nutrient absorption, which adds depth to the explanation.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question 0: falcon-7b-int4 vs falcon-7b-int8\n",
      "• Judgment 1: [[A>B]] - Both assistants provide accurate explanations of how earthworm tunnels benefit plant growth by improving soil aeration and water infiltration, which enhances root growth and nutrient availability. However, Assistant A's response is slightly more comprehensive by mentioning soil structure improvement, which is an important aspect of how earthworm tunnels benefit plants.\n",
      "• Judgment 2: [[A=B]] - Both assistants provide accurate explanations of how earthworm tunnels benefit plant growth by improving soil aeration and water infiltration, which enhances root growth and nutrient absorption. The responses are similar in content and clarity, with no significant differences in quality or accuracy.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_report(gpt_falcon_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619113cb-ed3d-4f2b-8b1d-0f91bcd32404",
   "metadata": {},
   "source": [
    "### Falcon family Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "19166e40-2e6d-4730-aa1e-f095a2a31da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions: 4740\n",
      "(4740, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>model_id</th>\n",
       "      <th>choices</th>\n",
       "      <th>tstamp</th>\n",
       "      <th>answer</th>\n",
       "      <th>explanation</th>\n",
       "      <th>verdict</th>\n",
       "      <th>question_id</th>\n",
       "      <th>model_1</th>\n",
       "      <th>model_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_falcon-7b-int8_falcon-7b-base</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nEarthworms live underground i...</td>\n",
       "      <td>9kJqWDpzRTeDconecDF4dv</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "      <td>[{'index': 0, 'turns': [{'content': '{\"explana...</td>\n",
       "      <td>1.747129e+09</td>\n",
       "      <td>{'explanation': 'Both Assistant A and Assistan...</td>\n",
       "      <td>Both Assistant A and Assistant B provided iden...</td>\n",
       "      <td>[[A=B]]</td>\n",
       "      <td>0</td>\n",
       "      <td>falcon-7b-int8</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_falcon-7b-int8_falcon-7b-base</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nHow can I temporarily seal a ...</td>\n",
       "      <td>9Dhx6RrhS7nPm2GPte5Zft</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "      <td>[{'index': 0, 'turns': [{'content': '{\"explana...</td>\n",
       "      <td>1.747129e+09</td>\n",
       "      <td>{'explanation': 'Assistant A provided a more c...</td>\n",
       "      <td>Assistant A provided a more comprehensive and ...</td>\n",
       "      <td>[[A&gt;&gt;B]]</td>\n",
       "      <td>1</td>\n",
       "      <td>falcon-7b-int8</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2_falcon-7b-int8_falcon-7b-base</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nHow to clean earbuds\\n\\n&lt;|The...</td>\n",
       "      <td>6QMMcwUk85ovLbquUAyjMG</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "      <td>[{'index': 0, 'turns': [{'content': '{\"explana...</td>\n",
       "      <td>1.747129e+09</td>\n",
       "      <td>{'explanation': 'Assistant A provided a more c...</td>\n",
       "      <td>Assistant A provided a more comprehensive guid...</td>\n",
       "      <td>[[A&gt;&gt;B]]</td>\n",
       "      <td>2</td>\n",
       "      <td>falcon-7b-int8</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5_falcon-7b-int8_falcon-7b-base</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nTo hold open a bedroom door w...</td>\n",
       "      <td>C7dSWyeK4rLFtZp9HKtpiS</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "      <td>[{'index': 0, 'turns': [{'content': '{\"explana...</td>\n",
       "      <td>1.747129e+09</td>\n",
       "      <td>{'explanation': 'Both assistants provided iden...</td>\n",
       "      <td>Both assistants provided identical and accurat...</td>\n",
       "      <td>[[A=B]]</td>\n",
       "      <td>5</td>\n",
       "      <td>falcon-7b-int8</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7_falcon-7b-int8_falcon-7b-base</td>\n",
       "      <td>&lt;|User Prompt|&gt;\\nTo add a surprising twist to ...</td>\n",
       "      <td>jwDqWjtqdoF9izNUAoc4z6</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "      <td>[{'index': 0, 'turns': [{'content': '{\"explana...</td>\n",
       "      <td>1.747129e+09</td>\n",
       "      <td>{'explanation': 'Both assistants provided simi...</td>\n",
       "      <td>Both assistants provided similar and accurate ...</td>\n",
       "      <td>[[A&gt;B]]</td>\n",
       "      <td>7</td>\n",
       "      <td>falcon-7b-int8</td>\n",
       "      <td>falcon-7b-base</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id  \\\n",
       "0  0_falcon-7b-int8_falcon-7b-base   \n",
       "1  1_falcon-7b-int8_falcon-7b-base   \n",
       "2  2_falcon-7b-int8_falcon-7b-base   \n",
       "3  5_falcon-7b-int8_falcon-7b-base   \n",
       "4  7_falcon-7b-int8_falcon-7b-base   \n",
       "\n",
       "                                             content               answer_id  \\\n",
       "0  <|User Prompt|>\\nEarthworms live underground i...  9kJqWDpzRTeDconecDF4dv   \n",
       "1  <|User Prompt|>\\nHow can I temporarily seal a ...  9Dhx6RrhS7nPm2GPte5Zft   \n",
       "2  <|User Prompt|>\\nHow to clean earbuds\\n\\n<|The...  6QMMcwUk85ovLbquUAyjMG   \n",
       "3  <|User Prompt|>\\nTo hold open a bedroom door w...  C7dSWyeK4rLFtZp9HKtpiS   \n",
       "4  <|User Prompt|>\\nTo add a surprising twist to ...  jwDqWjtqdoF9izNUAoc4z6   \n",
       "\n",
       "         model_id                                            choices  \\\n",
       "0  falcon-7b-base  [{'index': 0, 'turns': [{'content': '{\"explana...   \n",
       "1  falcon-7b-base  [{'index': 0, 'turns': [{'content': '{\"explana...   \n",
       "2  falcon-7b-base  [{'index': 0, 'turns': [{'content': '{\"explana...   \n",
       "3  falcon-7b-base  [{'index': 0, 'turns': [{'content': '{\"explana...   \n",
       "4  falcon-7b-base  [{'index': 0, 'turns': [{'content': '{\"explana...   \n",
       "\n",
       "         tstamp                                             answer  \\\n",
       "0  1.747129e+09  {'explanation': 'Both Assistant A and Assistan...   \n",
       "1  1.747129e+09  {'explanation': 'Assistant A provided a more c...   \n",
       "2  1.747129e+09  {'explanation': 'Assistant A provided a more c...   \n",
       "3  1.747129e+09  {'explanation': 'Both assistants provided iden...   \n",
       "4  1.747129e+09  {'explanation': 'Both assistants provided simi...   \n",
       "\n",
       "                                         explanation   verdict question_id  \\\n",
       "0  Both Assistant A and Assistant B provided iden...   [[A=B]]           0   \n",
       "1  Assistant A provided a more comprehensive and ...  [[A>>B]]           1   \n",
       "2  Assistant A provided a more comprehensive guid...  [[A>>B]]           2   \n",
       "3  Both assistants provided identical and accurat...   [[A=B]]           5   \n",
       "4  Both assistants provided similar and accurate ...   [[A>B]]           7   \n",
       "\n",
       "          model_1         model_2  \n",
       "0  falcon-7b-int8  falcon-7b-base  \n",
       "1  falcon-7b-int8  falcon-7b-base  \n",
       "2  falcon-7b-int8  falcon-7b-base  \n",
       "3  falcon-7b-int8  falcon-7b-base  \n",
       "4  falcon-7b-int8  falcon-7b-base  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_falcon = pd.read_json(\"data/brain-hard-v0.1/model_judgment/falcon-7b-family-questions.json\", lines=True)\n",
    "print(f\"Total questions: {questions_falcon.shape[0]}\")\n",
    "questions_falcon = questions_falcon[[\"question_id\", \"old_question_id\", \"model_2\", \"model_1\", \"turns\"]]\n",
    "questions_falcon['content'] = questions_falcon[\"turns\"].map(lambda x: x[1]['content'])\n",
    "questions_falcon.drop(columns = ['turns'], inplace=True)\n",
    "questions_falcon.rename(columns = {'question_id': 'id', 'old_question_id': 'question_id'}, inplace=True)\n",
    "\n",
    "falcon_markup_data = pd.read_json(\"data/brain-hard-v0.1/falcon-7b-family.json\", lines=True)\n",
    "falcon_markup_data[\"answer\"] = falcon_markup_data[\"choices\"].map(lambda x: json.loads(x[0]['turns'][1]['content']))\n",
    "falcon_markup_data[\"explanation\"] = falcon_markup_data[\"answer\"].map(lambda x: x[\"explanation\"])\n",
    "falcon_markup_data[\"verdict\"] = falcon_markup_data[\"answer\"].map(lambda x: x[\"verdict\"])\n",
    "falcon_markup_data[\"id\"] = falcon_markup_data[\"question_id\"].map(lambda x: x.split('_')[0])\n",
    "falcon_markup_data[\"model_1\"] = falcon_markup_data[\"question_id\"].map(lambda x: x.split('_')[1])\n",
    "falcon_markup_data[\"model_2\"] = falcon_markup_data[\"question_id\"].map(lambda x: x.split('_')[2])\n",
    "falcon_markup_data.rename(columns={\"question_id\" : \"id\", \"id\": \"question_id\"}, inplace=True)\n",
    "falcon_markup_data = falcon_markup_data.groupby('id').first()\n",
    "falcon_markup_data = falcon_markup_data.reset_index()\n",
    "\n",
    "falcon_markup_data = pd.merge(\n",
    "    left=questions_falcon[['id', 'content']],  \n",
    "    right=falcon_markup_data,                     \n",
    "    on='id', \n",
    "    how='inner'\n",
    ")\n",
    "print(falcon_markup_data.shape)\n",
    "falcon_markup_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cdb5537d-5d21-453c-8cec-e64c2d94303e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Overall Statistics ===\n",
      "Total comparisons: 4740\n",
      "Verdict distribution: {'[[A>B]]': 2347, '[[A>>B]]': 1285, '[[A=B]]': 943, '[[B>A]]': 157, '[[B>>A]]': 8}\n",
      "\n",
      "=== Model Performance ===\n",
      "falcon-7b-int8: 853 wins, 874 losses, 643 ties (36.0% win rate)\n",
      "falcon-7b-base: 874 wins, 854 losses, 642 ties (36.9% win rate)\n",
      "falcon-7b-int4: 1036 wins, 1060 losses, 274 ties (43.7% win rate)\n",
      "falcon-7b-awq: 1034 wins, 1009 losses, 327 ties (43.6% win rate)\n",
      "\n",
      "=== Model Permormance [Cleared inconsistencies]\n",
      "falcon-7b-awq: 1319 pts (Strong: 363, Weak: 593, Ties: 451, Inconsistent: 124)\n",
      "falcon-7b-int4: 1311 pts (Strong: 329, Weak: 653, Ties: 386, Inconsistent: 112)\n",
      "falcon-7b-base: 1129 pts (Strong: 293, Weak: 543, Ties: 730, Inconsistent: 88)\n",
      "falcon-7b-int8: 1107 pts (Strong: 292, Weak: 523, Ties: 735, Inconsistent: 92)\n",
      "\n",
      "=== Final Judgment Table ===\n",
      "  question_id         model_1         model_2   verdict  \\\n",
      "0           0  falcon-7b-int8  falcon-7b-base   [[A=B]]   \n",
      "1           1  falcon-7b-int8  falcon-7b-base  [[A>>B]]   \n",
      "2           2  falcon-7b-int8  falcon-7b-base  [[A>>B]]   \n",
      "3           5  falcon-7b-int8  falcon-7b-base   [[A=B]]   \n",
      "4           7  falcon-7b-int8  falcon-7b-base   [[A>B]]   \n",
      "\n",
      "                                         explanation  \n",
      "0  Both Assistant A and Assistant B provided iden...  \n",
      "1  Assistant A provided a more comprehensive and ...  \n",
      "2  Assistant A provided a more comprehensive guid...  \n",
      "3  Both assistants provided identical and accurat...  \n",
      "4  Both assistants provided similar and accurate ...  \n",
      "\n",
      "=== Top 5 Significant Differences ===\n",
      "\n",
      "Question ID: 1_falcon-7b-int8_falcon-7b-base\n",
      "Models: falcon-7b-int8 vs falcon-7b-base\n",
      "Verdict: [[A>>B]]\n",
      "Explanation: Assistant A provided a more comprehensive and practical guide, including steps for reinforcing with a pane of glass and checking for leaks. Assistant B's answer was also correct but slightly less detailed and lacked the reinforcement step. Assistant A's answer was more helpful and relevant, making it the better choice.\n",
      "\n",
      "=== Question ===\n",
      "How can I temporarily seal a broken window?\n",
      "\n",
      "=== Assistant A ===\n",
      "To temporarily seal a broken window, you can follow these steps:\n",
      "\n",
      "1. Clean the broken area: Remove any glass shards or debris from the window frame and the broken area.\n",
      "\n",
      "2. Use a temporary sealant: Apply a temporary window sealant or a strong adhesive tape around the broken area. Make sure to cover the entire perimeter of the break.\n",
      "\n",
      "3. Reinforce with a pane of glass: If possible, place a pane of glass over the broken area to provide additional support and prevent further damage.\n",
      "\n",
      "4. Secure the glass: Use strong adhesive tape or a temporary window film to hold the glass in place.\n",
      "\n",
      "5. Check for leaks: Inspect the sealed area for any gaps or leaks. If necessary, add more sealant or tape to ensure a proper seal.\n",
      "\n",
      "6. Schedule a professional repair: Once the temporary seal is in place, make sure to schedule a professional window repair service to fix the broken window permanently.\n",
      "\n",
      "Remember, this is a temporary solution and should not be relied upon for long-term use. Always prioritize safety and seek professional help for a permanent repair.\n",
      "\n",
      "=== Assistant B ===\n",
      "To temporarily seal a broken window, you can follow these steps:\n",
      "\n",
      "1. Clean the broken area: Remove any glass shards or debris from the window frame and the broken area.\n",
      "\n",
      "2. Use a temporary sealant: You can use a temporary window sealant or a silicone-based caulk to seal the broken area. Make sure to choose a product that is suitable for windows and can withstand the weather conditions.\n",
      "\n",
      "3. Apply the sealant: Using a caulk gun or a small spatula, apply the sealant to the broken area, filling in the gaps and smoothing it out. Make sure to cover the entire broken area and create a smooth, even surface.\n",
      "\n",
      "4. Clean up excess sealant: Use a damp cloth or a putty knife to remove any excess sealant that may have oozed out of the window frame.\n",
      "\n",
      "5. Allow the sealant to dry: Follow the manufacturer's instructions for drying time, which usually ranges from a few hours to overnight.\n",
      "\n",
      "6. Add additional support (optional): If the broken window is in a high-traffic area or prone to strong winds, you may want to add additional support, such as a temporary window pane or a window film, to prevent further damage.\n",
      "\n",
      "Remember, this is a temporary solution, and you should repair or replace the broken window as soon as possible.\n",
      "--------------------------------------------------\n",
      "\n",
      "Question ID: 2_falcon-7b-int8_falcon-7b-base\n",
      "Models: falcon-7b-int8 vs falcon-7b-base\n",
      "Verdict: [[A>>B]]\n",
      "Explanation: Assistant A provided a more comprehensive guide, including steps for cleaning stubborn wax buildup and emphasizing the importance of avoiding harsh chemicals. Assistant B's response was slightly less detailed and did not cover all aspects of cleaning, such as the use of baby oil for wax removal. Assistant A's answer was more thorough and helpful.\n",
      "\n",
      "=== Question ===\n",
      "How to clean earbuds\n",
      "\n",
      "=== Assistant A ===\n",
      "To clean earbuds, follow these steps:\n",
      "\n",
      "1. Use a soft, dry cloth or brush to gently remove any visible dirt or debris from the earbuds.\n",
      "2. Mix a small amount of mild dish soap with warm water.\n",
      "3. Dip a cotton swab or soft cloth into the soapy water and gently clean the earbuds, focusing on the tips and any visible areas with dirt or wax buildup.\n",
      "4. Rinse the cotton swab or cloth and gently wipe away any remaining soap residue.\n",
      "5. Dry the earbuds with a clean, dry cloth.\n",
      "6. For stubborn wax buildup, you can use a specialized earwax removal tool or a mixture of baby oil and warm water. Apply the mixture to the tip of the earbud and let it sit for a few minutes before gently wiping it away with a cotton swab.\n",
      "7. Avoid using harsh chemicals, abrasive materials, or high heat, as they can damage the earbuds.\n",
      "\n",
      "Regular cleaning will help maintain the sound quality and hygiene of your earbuds.\n",
      "\n",
      "=== Assistant B ===\n",
      "To clean earbuds, follow these steps:\n",
      "\n",
      "1. Use a soft, dry cloth or brush to gently remove any visible dirt or debris from the earbuds.\n",
      "2. Mix a small amount of mild dish soap with warm water.\n",
      "3. Dip a cotton swab or soft cloth into the soapy water and gently clean the exterior of the earbuds, being careful not to push water into the openings.\n",
      "4. For the interior, use a dry cotton swab to carefully clean the tips and any visible buildup.\n",
      "5. Rinse the cloth or cotton swab with water and gently wipe away any remaining soap.\n",
      "6. Dry the earbuds with a clean, dry cloth.\n",
      "7. Allow the earbuds to air dry completely before using them again.\n",
      "--------------------------------------------------\n",
      "\n",
      "Question ID: 29_falcon-7b-int8_falcon-7b-base\n",
      "Models: falcon-7b-int8 vs falcon-7b-base\n",
      "Verdict: [[A>>B]]\n",
      "Explanation: Assistant A provided a more comprehensive list of methods, including the Touch Bar option, which Assistant B did not mention. Assistant A's answer was also more concise and directly addressed the user's question. Assistant B's answer included the mouse wheel method, which is not typically used on a Mac. Therefore, Assistant A is significantly better in this case.\n",
      "\n",
      "=== Question ===\n",
      "How do you zoom in on a Mac computer?\n",
      "\n",
      "=== Assistant A ===\n",
      "To zoom in on a Mac computer, you can use the following methods:\n",
      "\n",
      "1. Press Command (⌘) + Option (⌥) + Plus sign (+) keys simultaneously.\n",
      "2. Use the mouse: Click the \"Zoom\" button in the menu bar, then select \"Zoom In.\"\n",
      "3. Use the keyboard shortcut: Press Control (⌃) + Option (⌥) + Plus sign (+) keys simultaneously.\n",
      "4. Use the Trackpad: Two-finger pinch gesture on the trackpad.\n",
      "5. Use the Touch Bar (if available): Click the \"+\" icon on the Touch Bar.\n",
      "\n",
      "These methods will allow you to zoom in on your Mac screen.\n",
      "\n",
      "=== Assistant B ===\n",
      "To zoom in on a Mac computer, you can use the following methods:\n",
      "\n",
      "1. Two-finger pinch gesture on the trackpad: Place two fingers on the trackpad and pinch them together to zoom in.\n",
      "2. Keyboard shortcut: Press Command (⌘) + Option (⌥) + Plus sign (+) to zoom in.\n",
      "3. Mouse wheel: Scroll the mouse wheel upwards to zoom in.\n",
      "4. Accessibility settings: Go to System Preferences > Accessibility > Zoom and enable the Zoom feature. You can then use the keyboard shortcut Command (⌘) + Option (⌥) + Equals sign (=) to zoom in.\n",
      "\n",
      "Choose the method that suits you best for zooming in on your Mac computer.\n",
      "--------------------------------------------------\n",
      "\n",
      "Question ID: 46_falcon-7b-int8_falcon-7b-base\n",
      "Models: falcon-7b-int8 vs falcon-7b-base\n",
      "Verdict: [[A>>B]]\n",
      "Explanation: Both assistants provided accurate information about NASA's plans to send humans to Mars and the Moon. Assistant A included additional information about other celestial bodies, which adds value to the response. Assistant B's answer was slightly more concise. Overall, Assistant A's answer is more comprehensive and creative, making it significantly better.\n",
      "\n",
      "=== Question ===\n",
      "To which planet do National Aeronautics and Space Administration (NASA) scientists hope to send humans next?\n",
      "\n",
      "=== Assistant A ===\n",
      "As of my last update in 2023, NASA scientists have expressed interest in sending humans to Mars as the next major destination. However, there are also plans for missions to other celestial bodies, such as the Moon through the Artemis program. It's important to check the latest updates from NASA for the most current information on their exploration plans.\n",
      "\n",
      "=== Assistant B ===\n",
      "As of my last update in 2023, NASA scientists have expressed interest in sending humans to Mars as the next major destination. However, they are also exploring the possibility of returning to the Moon through their Artemis program, which aims to establish sustainable lunar exploration.\n",
      "--------------------------------------------------\n",
      "\n",
      "Question ID: 61_falcon-7b-int8_falcon-7b-base\n",
      "Models: falcon-7b-int8 vs falcon-7b-base\n",
      "Verdict: [[A>>B]]\n",
      "Explanation: Assistant A's instructions are more straightforward and practical, providing clear steps without unnecessary complexity. Assistant B's method involves drilling holes, which adds an extra step and potential safety risk. Assistant A's approach is also more concise and focuses on the essential steps. Therefore, Assistant A's answer is significantly better.\n",
      "\n",
      "=== Question ===\n",
      "To make old glass bottles into an instrument,\n",
      "\n",
      "=== Assistant A ===\n",
      "One way to transform old glass bottles into an instrument is by creating a glass bottle maraca. Here's how you can do it:\n",
      "\n",
      "1. Gather materials: You'll need a clean, empty glass bottle (make sure it's thoroughly washed and dried), small beads or dried beans, a hammer, and a piece of cloth or a plastic bag.\n",
      "\n",
      "2. Prepare the bottle: Remove any labels from the bottle and clean it thoroughly. Make sure there are no sharp edges or cracks.\n",
      "\n",
      "3. Fill the bottle: Add a small amount of beads or dried beans to the bottle. The amount will depend on the size of the bottle and the desired sound.\n",
      "\n",
      "4. Seal the bottle: Place the cloth or plastic bag over the opening of the bottle and gently tap it with the hammer to create a makeshift seal. Make sure the opening is airtight to prevent the beads or beans from spilling out.\n",
      "\n",
      "5. Test the instrument: Hold the bottle and shake it gently to test the sound. Adjust the amount of beads or beans if needed to achieve the desired sound.\n",
      "\n",
      "6. Decorate (optional): You can decorate the outside of the bottle with paint, stickers, or other embellishments to make it more visually appealing.\n",
      "\n",
      "Now you have a unique glass bottle maraca that you can use for various musical purposes!\n",
      "\n",
      "=== Assistant B ===\n",
      "One way to transform old glass bottles into an instrument is by creating a glass bottle maraca. Here's how you can do it:\n",
      "\n",
      "1. Gather materials: You'll need a clean, empty glass bottle (make sure it's thoroughly washed and dried), small beads or dried beans, a drill with a small drill bit, and a cork or rubber band.\n",
      "\n",
      "2. Prepare the bottle: Remove any labels from the bottle and clean it thoroughly. Make sure the bottle is completely dry before proceeding.\n",
      "\n",
      "3. Drill holes: Using the drill, carefully create small holes around the neck of the bottle. The number of holes and their spacing will depend on the desired sound and the size of the beads or beans you'll be using.\n",
      "\n",
      "4. Add beads or beans: Fill the bottle with small beads or dried beans. The more beads or beans you add, the louder the instrument will be.\n",
      "\n",
      "5. Seal the bottle: Insert a cork or rubber band over the neck of the bottle to keep the beads or beans inside. Make sure it's secure but still allows the beads or beans to move freely.\n",
      "\n",
      "6. Test and adjust: Shake the bottle gently to test the sound. If you're not satisfied with the sound, you can add or remove beads or beans, or adjust the spacing of the holes.\n",
      "\n",
      "Now you have a unique glass bottle maraca that you can use in various musical performances or simply for fun. Enjoy experimenting with different sounds and arrangements!\n",
      "--------------------------------------------------\n",
      "\n",
      "🔍 Found 1885 inconsistent model pairs across 371 questions\n",
      "\n",
      "📊 Verdict Combinations:\n",
      "╭────────────────┬──────────────────────┬───────────────────────┬────────────────────────┬──────────────────────┬───────────────────────┬───────────────────────┬──────────────────────┬──────────────────────╮\n",
      "│ Verdict Pair   │   [[A>B]] vs [[A>B]] │   [[A>>B]] vs [[A>B]] │   [[A>>B]] vs [[A>>B]] │   [[A=B]] vs [[A>B]] │   [[A>>B]] vs [[B>A]] │   [[A=B]] vs [[A>>B]] │   [[A=B]] vs [[B>A]] │   [[B>A]] vs [[B>A]] │\n",
      "├────────────────┼──────────────────────┼───────────────────────┼────────────────────────┼──────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┼──────────────────────┤\n",
      "│ Total          │                  766 │                   546 │                    336 │                  173 │                    54 │                     5 │                    3 │                    2 │\n",
      "╰────────────────┴──────────────────────┴───────────────────────┴────────────────────────┴──────────────────────┴───────────────────────┴───────────────────────┴──────────────────────┴──────────────────────╯\n",
      "\n",
      "❓ Top Questions with Inconsistencies:\n",
      "╭───────────────┬───────────────────╮\n",
      "│   Question ID │   Inconsistencies │\n",
      "├───────────────┼───────────────────┤\n",
      "│           999 │                12 │\n",
      "├───────────────┼───────────────────┤\n",
      "│           164 │                12 │\n",
      "├───────────────┼───────────────────┤\n",
      "│          1670 │                12 │\n",
      "├───────────────┼───────────────────┤\n",
      "│          1675 │                12 │\n",
      "├───────────────┼───────────────────┤\n",
      "│          1685 │                12 │\n",
      "╰───────────────┴───────────────────╯\n",
      "\n",
      "=== Question 999 ===\n",
      "Me ofereça por favor dez sugestões de nome para meu canal no YouTube sobre como criar peixes de oceano em aquários domésticos.\n",
      "\n",
      "=== Question 164 ===\n",
      "how to learn spanish without classes?\n",
      "\n",
      "=== Question 1670 ===\n",
      "The longitude of a location determines which time zone it is in. Different time zones are the result of\n",
      "\n",
      "=== Question 1675 ===\n",
      "how do you edit an order at a drive thru?\n",
      "\n",
      "=== Question 1685 ===\n",
      "What is the best size for a napkin holder?\n",
      "\n",
      "\n",
      "🤖 Top Model Pairs with Inconsistencies:\n",
      "╭──────────────────────────────────┬───────────────────╮\n",
      "│ Model Pair                       │   Inconsistencies │\n",
      "├──────────────────────────────────┼───────────────────┤\n",
      "│ falcon-7b-awq vs falcon-7b-int4  │               694 │\n",
      "├──────────────────────────────────┼───────────────────┤\n",
      "│ falcon-7b-base vs falcon-7b-int4 │               694 │\n",
      "├──────────────────────────────────┼───────────────────┤\n",
      "│ falcon-7b-int4 vs falcon-7b-int8 │               686 │\n",
      "├──────────────────────────────────┼───────────────────┤\n",
      "│ falcon-7b-awq vs falcon-7b-base  │               662 │\n",
      "├──────────────────────────────────┼───────────────────┤\n",
      "│ falcon-7b-awq vs falcon-7b-int8  │               660 │\n",
      "╰──────────────────────────────────┴───────────────────╯\n",
      "\n",
      "📝 Example Inconsistent Judgments (first 3):\n",
      "\n",
      "Question 0: falcon-7b-awq vs falcon-7b-base\n",
      "• Judgment 1: [[A>B]] - Both Assistant A and Assistant B provide accurate and relevant explanations, but Assistant A's response is slightly more concise and clear. Assistant B's answer is also correct but includes slightly more words, making it less concise. Therefore, Assistant A is slightly better in this case.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question 0: falcon-7b-int4 vs falcon-7b-awq\n",
      "• Judgment 1: [[A>B]] - Both Assistant A and Assistant B provide accurate and relevant explanations, but Assistant A's response is slightly more concise and clear. Assistant B's answer, while correct, includes slightly more detail than necessary. Therefore, Assistant A is slightly better in this case.\n",
      "• Judgment 2: [[A=B]] - Both Assistant A and Assistant B provide accurate and relevant explanations for how earthworm tunnels benefit plants. Assistant A's response is slightly more concise, but Assistant B's answer is also clear and to the point. Both assistants effectively address the prompt's question.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question 0: falcon-7b-int8 vs falcon-7b-awq\n",
      "• Judgment 1: [[A>B]] - Both Assistant A and Assistant B provide accurate and relevant explanations, but Assistant A's response is slightly more concise and clear. Assistant B's answer, while correct, includes slightly more detail than necessary. Therefore, Assistant A is slightly better in this case.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_report(falcon_markup_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b062ae-ed26-4c6c-8d27-28235642e2bc",
   "metadata": {},
   "source": [
    "### Second round: making baseline questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ab5f3e70-d4c9-4a75-8fde-a98214b38c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyze_dataset.ipynb\t\t    final_dataset_filtered-v2.json\n",
      "building_first_stage_dataset.ipynb  final_dataset.json\n",
      "final_dataser_filtered-v1.json\t    final_dataset_processed.json\n"
     ]
    }
   ],
   "source": [
    "!ls datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "dc9ba673-84f3-4513-8d80-c137eb3a5763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>source</th>\n",
       "      <th>meta_info</th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>default_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>multiple_choice</td>\n",
       "      <td>allenai/ai2_arc</td>\n",
       "      <td>{'id_in_dataset': 'Mercury_7135345'}</td>\n",
       "      <td>6.751745e+12</td>\n",
       "      <td>Two processes are involved in the formation of...</td>\n",
       "      <td>wind erosion then deposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>knowledge_based</td>\n",
       "      <td>khaimaitien/qa-expert-multi-hop-qa-V1.0</td>\n",
       "      <td>{'tag': 'long_attributes-train.json', 'src': '...</td>\n",
       "      <td>1.321561e+28</td>\n",
       "      <td>Is the Colegio Nacional de Buenos Aires locate...</td>\n",
       "      <td>Yes, the Colegio Nacional de Buenos Aires is l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>multiple_choice</td>\n",
       "      <td>allenai/ai2_arc</td>\n",
       "      <td>{'id_in_dataset': 'OHAT_2009_5_9'}</td>\n",
       "      <td>4.041745e+12</td>\n",
       "      <td>Earthworms live underground in the soil. As th...</td>\n",
       "      <td>Earthworm tunnels loosen the soil so plant roo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>physical_reasoning</td>\n",
       "      <td>ybisk/piqa</td>\n",
       "      <td>None</td>\n",
       "      <td>8.572158e+28</td>\n",
       "      <td>How can I temporarily seal a broken window?</td>\n",
       "      <td>Tape a sheet of heavy plastic over the hole.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>knowledge_based</td>\n",
       "      <td>khaimaitien/qa-expert-multi-hop-qa-V1.0</td>\n",
       "      <td>{'tag': 'entities-single_neg_valid.json', 'src...</td>\n",
       "      <td>-8.343583e+27</td>\n",
       "      <td>What position does Jungkook hold in his group ...</td>\n",
       "      <td>Based on the provided knowledge, it is unclear...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tag                                   source  \\\n",
       "0     multiple_choice                          allenai/ai2_arc   \n",
       "1     knowledge_based  khaimaitien/qa-expert-multi-hop-qa-V1.0   \n",
       "2     multiple_choice                          allenai/ai2_arc   \n",
       "3  physical_reasoning                               ybisk/piqa   \n",
       "4     knowledge_based  khaimaitien/qa-expert-multi-hop-qa-V1.0   \n",
       "\n",
       "                                           meta_info            id  \\\n",
       "0               {'id_in_dataset': 'Mercury_7135345'}  6.751745e+12   \n",
       "1  {'tag': 'long_attributes-train.json', 'src': '...  1.321561e+28   \n",
       "2                 {'id_in_dataset': 'OHAT_2009_5_9'}  4.041745e+12   \n",
       "3                                               None  8.572158e+28   \n",
       "4  {'tag': 'entities-single_neg_valid.json', 'src... -8.343583e+27   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Two processes are involved in the formation of...   \n",
       "1  Is the Colegio Nacional de Buenos Aires locate...   \n",
       "2  Earthworms live underground in the soil. As th...   \n",
       "3        How can I temporarily seal a broken window?   \n",
       "4  What position does Jungkook hold in his group ...   \n",
       "\n",
       "                                      default_answer  \n",
       "0                       wind erosion then deposition  \n",
       "1  Yes, the Colegio Nacional de Buenos Aires is l...  \n",
       "2  Earthworm tunnels loosen the soil so plant roo...  \n",
       "3       Tape a sheet of heavy plastic over the hole.  \n",
       "4  Based on the provided knowledge, it is unclear...  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import orjson\n",
    "\n",
    "with open(\"./datasets/final_dataset.json\", \"rb\") as f:\n",
    "    final_dataset = pd.DataFrame(orjson.loads(f.read()))\n",
    "final_dataset[\"id\"] = (final_dataset[\"question_id\"].astype(str) + final_dataset[\"tstamp\"].astype(str)).astype(float).round(3)\n",
    "final_dataset[\"prompt\"] = final_dataset[\"conversation_a\"].apply(lambda x: x[0]['content'].strip())\n",
    "final_dataset[\"default_answer\"] = final_dataset[\"conversation_a\"].apply(lambda x: np.nan if len(x) == 1 else x[1]['content'])\n",
    "final_dataset.drop(columns=[\"question_id\", \"tstamp\", \"conversation_a\"], inplace=True)\n",
    "final_dataset.head()\n",
    "\n",
    "answer_df = pd.DataFrame()\n",
    "for key in list(processed_df[\"answer\"][0].keys()):\n",
    "    answer_df[key] = processed_df[\"answer\"].apply(lambda answer: answer.get(key, np.nan))\n",
    "for key in ['tag', 'source', 'meta_info', 'prompt', 'default_answer']:\n",
    "    answer_df[key] = final_dataset[key]\n",
    "answer_df[\"model_answer\"] = processed_df[\"answer\"].apply(lambda answer: answer.get(\"answer\", np.nan))\n",
    "answer_df[\"row-id\"] = processed_df[\"row-id\"]\n",
    "print(f\"Final table has {answer_df.shape[0]} rows\")\n",
    "answer_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e517eab8-a01a-43be-a3c1-b8c036b666eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_falcon = questions_falcon[[\"question_id\", \"old_question_id\", \"model_2\", \"model_1\", \"turns\"]]\n",
    "questions_falcon['content'] = questions_falcon[\"turns\"].map(lambda x: x[1]['content'])\n",
    "questions_falcon.drop(columns = ['turns'], inplace=True)\n",
    "questions_falcon.rename(columns = {'question_id': 'id', 'old_question_id': 'question_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6ea23c70-98e6-4ecb-ac29-7a1ab65ef73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system_prompt: \"### Task: Act as an impartial judge to evaluate the quality of an\\\n",
      "  \\ AI assistant's response. You will be provided a baseline answer for the given\\\n",
      "  \\ user prompt for clarity along with the aspects to mainly consider.\\n\\n### Steps:\\n\\\n",
      "  1. **General Conclusion** (1-2 sentences):\\n   - Highlight major strengths/weaknesses/missed\\\n",
      "  \\ parts compared to the baseline.\\n   - Note if the answer is entirely wrong, partially\\\n",
      "  \\ correct, or fully satisfies the prompt.\\n\\n2. **Aspect Ratings** (Per the user-provided\\\n",
      "  \\ list):  \\n   - **Accuracy** → *True* (no errors), *Partial* (minor inaccuracies),\\\n",
      "  \\ *False* (major errors).  \\n   - **Completeness** → *True* (covers all key points),\\\n",
      "  \\ *Partial* (misses some), *False* (omits critical details).  \\n   - **Creativity/Novelty**\\\n",
      "  \\ → *True* (original insights), *Partial* (some added value), *False* (generic/repetitive).\\\n",
      "  \\  \\n   - **Helpfulness** → *True* (fully addresses the prompt), *Partial* (partially\\\n",
      "  \\ useful), *False* (irrelevant).  \\n\\n3. **Verdict (1-5):**  \\n   - **1** → Gibberish/wholly\\\n",
      "  \\ incorrect.  \\n   - **2** → Major dissatisfaction (wrong/core aspects missed).\\\n",
      "  \\  \\n   - **3** → Partial satisfaction (mixed quality).  \\n   - **4** → Minor flaws\\\n",
      "  \\ (near-complete alignment).  \\n   - **5** → Excellent (fully meets prompt, no flaws).\\\n",
      "  \\  \\n\\n#### **Output Format:**\\n{\\n  \\\"conclusion\\\": \\\"Summary of evaluation (e.g.,\\\n",
      "  \\ 'The assistant matches the baseline perfectly but lacks creative elaboration.').\\\"\\\n",
      "  ,\\n  \\\"aspects\\\": {\\n    \\\"accuracy\\\": \\\"True/Partial/False\\\",\\n    \\\"completeness\\\"\\\n",
      "  : \\\"True/Partial/False\\\",\\n    \\\"creativity\\\": \\\"True/Partial/False\\\",\\n    // ...\\\n",
      "  \\ other specified aspects  \\n  },\\n  \\\"verdict\\\": 1-5  \\n}\\n\\n### **Example Evaluation**\\n\\\n",
      "  <|User Prompt|>\\nEarthworms live underground in the soil. As they move through the\\\n",
      "  \\ soil, they create tunnels. The tunnels help improve the soil. Plants grow better\\\n",
      "  \\ in soil that has earthworms. Which statement explains how earthworm tunnels help\\\n",
      "  \\ plants?\\n\\n<|The Start of Baseline Answer|>\\nEarthworm tunnels improve soil structure,\\\n",
      "  \\ allowing better water infiltration and aeration, which in turn promotes healthier\\\n",
      "  \\ root growth and nutrient availability for plants, ultimately leading to better\\\n",
      "  \\ plant growth.\\n<|The End of Baseline Answer|>\\n\\n<|The Start of Assistant Answer|>\\n\\\n",
      "  Earthworm tunnels help plants in several ways, primarily by improving soil structure\\\n",
      "  \\ and aeration. Here's a statement that explains how earthworm tunnels contribute\\\n",
      "  \\ to plant growth:\\n\\n\\\"Earthworm tunnels increase soil aeration and create pathways\\\n",
      "  \\ for water and nutrients to reach plant roots more easily, which in turn supports\\\n",
      "  \\ healthier plant growth.\\\"\\n\\nThis statement highlights the key benefits of earthworm\\\n",
      "  \\ tunnels, such as improved aeration and better access to water and nutrients, which\\\n",
      "  \\ are crucial for plant health and growth.\\n<|The End of Assistant Answer|>\\n\\n\\\n",
      "  Key aspects: accuracy, completeness\\n\\n{\\n  \\\"conclusion\\\": \\\"The assistant's answer\\\n",
      "  \\ covers all key points accurately but is less concise than the baseline and has\\\n",
      "  \\ an awkward structure with unnecessary framing.\\\",\\n  \\\"aspects\\\": {\\n    \\\"accuracy\\\"\\\n",
      "  : \\\"True\\\",\\n    \\\"completeness\\\": \\\"True\\\"\\n  },\\n  \\\"verdict\\\": 4\\n}\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"### Task: Act as an impartial judge to evaluate the quality of an AI assistant's response. You will be provided a baseline answer for the given user prompt for clarity along with the aspects to mainly consider.\n",
    "\n",
    "### Steps:\n",
    "1. **General Conclusion** (1-2 sentences):\n",
    "   - Highlight major strengths/weaknesses/missed parts compared to the baseline.\n",
    "   - Note if the answer is entirely wrong, partially correct, or fully satisfies the prompt.\n",
    "\n",
    "2. **Aspect Ratings** (Per the user-provided list):  \n",
    "   - **Accuracy** → *True* (no errors), *Partial* (minor inaccuracies), *False* (major errors).  \n",
    "   - **Completeness** → *True* (covers all key points), *Partial* (misses some), *False* (omits critical details).  \n",
    "   - **Creativity/Novelty** → *True* (original insights), *Partial* (some added value), *False* (generic/repetitive).  \n",
    "   - **Helpfulness** → *True* (fully addresses the prompt), *Partial* (partially useful), *False* (irrelevant).  \n",
    "\n",
    "3. **Verdict (1-5):**  \n",
    "   - **1** → Gibberish/wholly incorrect.  \n",
    "   - **2** → Major dissatisfaction (wrong/core aspects missed).  \n",
    "   - **3** → Partial satisfaction (mixed quality).  \n",
    "   - **4** → Minor flaws (near-complete alignment).  \n",
    "   - **5** → Excellent (fully meets prompt, no flaws).  \n",
    "\n",
    "#### **Output Format:**\n",
    "{\n",
    "  \"conclusion\": \"Summary of evaluation (e.g., 'The assistant matches the baseline perfectly but lacks creative elaboration.').\",\n",
    "  \"aspects\": {\n",
    "    \"accuracy\": \"True/Partial/False\",\n",
    "    \"completeness\": \"True/Partial/False\",\n",
    "    \"creativity\": \"True/Partial/False\",\n",
    "    // ... other specified aspects  \n",
    "  },\n",
    "  \"verdict\": 1-5  \n",
    "}\n",
    "\n",
    "### **Example Evaluation**\n",
    "<|User Prompt|>\n",
    "Earthworms live underground in the soil. As they move through the soil, they create tunnels. The tunnels help improve the soil. Plants grow better in soil that has earthworms. Which statement explains how earthworm tunnels help plants?\n",
    "\n",
    "<|The Start of Baseline Answer|>\n",
    "Earthworm tunnels improve soil structure, allowing better water infiltration and aeration, which in turn promotes healthier root growth and nutrient availability for plants, ultimately leading to better plant growth.\n",
    "<|The End of Baseline Answer|>\n",
    "\n",
    "<|The Start of Assistant Answer|>\n",
    "Earthworm tunnels help plants in several ways, primarily by improving soil structure and aeration. Here's a statement that explains how earthworm tunnels contribute to plant growth:\\n\\n\"Earthworm tunnels increase soil aeration and create pathways for water and nutrients to reach plant roots more easily, which in turn supports healthier plant growth.\"\\n\\nThis statement highlights the key benefits of earthworm tunnels, such as improved aeration and better access to water and nutrients, which are crucial for plant health and growth.\n",
    "<|The End of Assistant Answer|>\n",
    "\n",
    "Key aspects: accuracy, completeness\n",
    "\n",
    "{\n",
    "  \"conclusion\": \"The assistant's answer covers all key points accurately but is less concise than the baseline and has an awkward structure with unnecessary framing.\",\n",
    "  \"aspects\": {\n",
    "    \"accuracy\": \"True\",\n",
    "    \"completeness\": \"True\"\n",
    "  },\n",
    "  \"verdict\": 4\n",
    "}\"\"\"\n",
    "\n",
    "import yaml\n",
    "\n",
    "yaml_config = {\n",
    "    \"system_prompt\": prompt.strip()  # Removes leading/trailing whitespace\n",
    "}\n",
    "\n",
    "with open(\"eval_prompt.yaml\", \"w\") as f:\n",
    "    yaml.dump(yaml_config, f, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "with open(\"eval_prompt.yaml\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d7caba2c-23cf-4943-a752-3b2eed870a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Task: Act as an impartial judge to evaluate the quality of an AI assistant's response. You will be provided a baseline answer for the given user prompt for clarity along with the aspects to mainly consider.\n",
      "\n",
      "### Steps:\n",
      "1. **General Conclusion** (1-2 sentences):\n",
      "   - Highlight major strengths/weaknesses/missed parts compared to the baseline.\n",
      "   - Note if the answer is entirely wrong, partially correct, or fully satisfies the prompt.\n",
      "\n",
      "2. **Aspect Ratings** (Per the user-provided list):  \n",
      "   - **Accuracy** → *True* (no errors), *Partial* (minor inaccuracies), *False* (major errors).  \n",
      "   - **Completeness** → *True* (covers all key points), *Partial* (misses some), *False* (omits critical details).  \n",
      "   - **Creativity/Novelty** → *True* (original insights), *Partial* (some added value), *False* (generic/repetitive).  \n",
      "   - **Helpfulness** → *True* (fully addresses the prompt), *Partial* (partially useful), *False* (irrelevant).  \n",
      "\n",
      "3. **Verdict (1-5):**  \n",
      "   - **1** → Gibberish/wholly incorrect.  \n",
      "   - **2** → Major dissatisfaction (wrong/core aspects missed).  \n",
      "   - **3** → Partial satisfaction (mixed quality).  \n",
      "   - **4** → Minor flaws (near-complete alignment).  \n",
      "   - **5** → Excellent (fully meets prompt, no flaws).  \n",
      "\n",
      "#### **Output Format:**\n",
      "{\n",
      "  \"conclusion\": \"Summary of evaluation (e.g., 'The assistant matches the baseline perfectly but lacks creative elaboration.').\",\n",
      "  \"aspects\": {\n",
      "    \"accuracy\": \"True/Partial/False\",\n",
      "    \"completeness\": \"True/Partial/False\",\n",
      "    \"creativity\": \"True/Partial/False\",\n",
      "    // ... other specified aspects  \n",
      "  },\n",
      "  \"verdict\": 1-5  \n",
      "}\n",
      "\n",
      "### **Example Evaluation**\n",
      "<|User Prompt|>\n",
      "Earthworms live underground in the soil. As they move through the soil, they create tunnels. The tunnels help improve the soil. Plants grow better in soil that has earthworms. Which statement explains how earthworm tunnels help plants?\n",
      "\n",
      "<|The Start of Baseline Answer|>\n",
      "Earthworm tunnels improve soil structure, allowing better water infiltration and aeration, which in turn promotes healthier root growth and nutrient availability for plants, ultimately leading to better plant growth.\n",
      "<|The End of Baseline Answer|>\n",
      "\n",
      "<|The Start of Assistant Answer|>\n",
      "Earthworm tunnels help plants in several ways, primarily by improving soil structure and aeration. Here's a statement that explains how earthworm tunnels contribute to plant growth:\n",
      "\n",
      "\"Earthworm tunnels increase soil aeration and create pathways for water and nutrients to reach plant roots more easily, which in turn supports healthier plant growth.\"\n",
      "\n",
      "This statement highlights the key benefits of earthworm tunnels, such as improved aeration and better access to water and nutrients, which are crucial for plant health and growth.\n",
      "<|The End of Assistant Answer|>\n",
      "\n",
      "Key aspects: accuracy, completeness\n",
      "\n",
      "{\n",
      "  \"conclusion\": \"The assistant's answer covers all key points accurately but is less concise than the baseline and has an awkward structure with unnecessary framing.\",\n",
      "  \"aspects\": {\n",
      "    \"accuracy\": \"True\",\n",
      "    \"completeness\": \"True\"\n",
      "  },\n",
      "  \"verdict\": 4\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(x[0][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e25e9af-8101-48fc-b12d-1652a34a9a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
